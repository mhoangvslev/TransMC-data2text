{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class e2eDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file)\n",
    "        self.columns = self.dataset.columns\n",
    "        self.conditions = self.dataset[self.columns[0]]\n",
    "        self.sentences = self.dataset[self.columns[1]]\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.typ_list = {}\n",
    "        for k in range(len(self.conditions)):\n",
    "            cond_set = self.conditions[k].split(',')\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] in self.typ_list.keys():\n",
    "                    self.typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "                else:            \n",
    "                    self.typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conditions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cond = self.conditions[idx]\n",
    "        cond_set = cond.split(',')\n",
    "        condition_string = ''\n",
    "        for m in range(len(cond_set)):\n",
    "            cond_set[m] = cond_set[m].strip()\n",
    "            pos = cond_set[m].index('[')\n",
    "            \n",
    "            condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        \n",
    "        sen = self.sentences[idx]\n",
    "        input_string = condition_string + '<START>' + sen\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(input_string, add_special_tokens=True))\n",
    "        \n",
    "        label_string = sen + ' <|endoftext|>'\n",
    "        label_ids = torch.tensor(self.tokenizer.encode(label_string, add_special_tokens=True))\n",
    "\n",
    "        return input_ids, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 34\n",
      "eatType 3\n",
      "priceRange 6\n",
      "customer rating 6\n",
      "near 19\n",
      "food 7\n",
      "area 2\n",
      "familyFriendly 2\n",
      "8 79\n",
      "['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('dataset/trainset.csv')\n",
    "columns = dataset.columns\n",
    "conditions = dataset[columns[0]]\n",
    "\n",
    "typ_list = {}\n",
    "for k in range(len(conditions)):\n",
    "    cond_set = conditions[k].split(',')\n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        if cond_set[m][:pos] in typ_list.keys():\n",
    "            typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "        else:            \n",
    "            typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "            \n",
    "condition_token = []\n",
    "v_num = 0\n",
    "for k, v in typ_list.items():\n",
    "    v_num += len(v)\n",
    "    condition_token.append('<'+k+'>')\n",
    "    print(k, len(v))\n",
    "print(len(typ_list.keys()), v_num)\n",
    "print(condition_token)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0403 08:00:56.982329 140153994127168 file_utils.py:41] PyTorch version 1.2.0 available.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 13:44:49.296769 139684483344192 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0401 13:44:49.297794 139684483344192 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0401 13:44:49.369932 139684483344192 tokenization_utils.py:663] Adding <START> to the vocabulary\n",
      "I0401 13:44:49.371214 139684483344192 tokenization_utils.py:741] Assigning <START> to the bos_token key of the tokenizer\n",
      "I0401 13:44:49.371765 139684483344192 tokenization_utils.py:663] Adding <name> to the vocabulary\n",
      "I0401 13:44:49.372470 139684483344192 tokenization_utils.py:663] Adding <eatType> to the vocabulary\n",
      "I0401 13:44:49.373089 139684483344192 tokenization_utils.py:663] Adding <priceRange> to the vocabulary\n",
      "I0401 13:44:49.373710 139684483344192 tokenization_utils.py:663] Adding <customer rating> to the vocabulary\n",
      "I0401 13:44:49.374315 139684483344192 tokenization_utils.py:663] Adding <near> to the vocabulary\n",
      "I0401 13:44:49.374893 139684483344192 tokenization_utils.py:663] Adding <food> to the vocabulary\n",
      "I0401 13:44:49.375478 139684483344192 tokenization_utils.py:663] Adding <area> to the vocabulary\n",
      "I0401 13:44:49.376050 139684483344192 tokenization_utils.py:663] Adding <familyFriendly> to the vocabulary\n",
      "I0401 13:44:49.376616 139684483344192 tokenization_utils.py:741] Assigning ['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>'] to the additional_special_tokens key of the tokenizer\n",
      "I0401 13:44:50.158687 139684483344192 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/ds_user1/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
      "I0401 13:44:50.161347 139684483344192 configuration_utils.py:311] Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0401 13:44:50.935984 139684483344192 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[50258,   464, 21314, 50259, 12984, 50260,  3549,   621,  4248,  1270,\n",
       "          50261,    20,   503,   286,   642, 50262,    34,  1878,  2634,  1215,\n",
       "            380,  1512, 50257,   464, 21314,  2240,  1474, 42151,  1215,   380,\n",
       "           1512,   468,   257,   642,  3491,  7955,    13,   220, 29431,   923,\n",
       "            379,  4248,  1270,    13]]),\n",
       " tensor([[  464, 21314,  2240,  1474, 42151,  1215,   380,  1512,   468,   257,\n",
       "            642,  3491,  7955,    13,   220, 29431,   923,   379,  4248,  1270,\n",
       "             13, 50256]]),\n",
       " '<name>The Vaults <eatType>pub <priceRange>more than £30 <customer rating>5 out of 5 <near>Café Adriatic <START>The Vaults pub near Café Adriatic has a 5 star rating.  Prices start at £30.',\n",
       " 'The Vaults pub near Café Adriatic has a 5 star rating.  Prices start at £30. <|endoftext|>')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.SPECIAL_TOKENS_ATTRIBUTES: ['bos_token', 'eos_token', 'unk_token', 'sep_token', 'pad_token', 'cls_token', 'mask_token', 'additional_special_tokens']\n",
    "model_class, tokenizer_class, pretrained_weights = (GPT2Model, GPT2Tokenizer, 'gpt2')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "special_tokens = {'bos_token': '<START>', 'additional_special_tokens': condition_token}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "e2e_dataset = e2eDataset(csv_file='dataset/trainset.csv', tokenizer=tokenizer)\n",
    "e2e_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> 50257\n",
      "['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>']\n",
      "All special tokens: ['<eatType>', '<familyFriendly>', '<|endoftext|>', '<name>', '<priceRange>', '<food>', '<near>', '<customer rating>', '<area>', '<START>']\n",
      "All special ids: [50259, 50265, 50256, 50258, 50260, 50263, 50262, 50261, 50264, 50257]\n",
      "50266\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "print(tokenizer.additional_special_tokens)\n",
    "print('All special tokens:', tokenizer.all_special_tokens)\n",
    "print('All special ids:', tokenizer.all_special_ids)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "matrix_D = nn.Linear(768, 50266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 768]) torch.Size([49, 50266]) torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    output_vector = model(sample_batched[0].squeeze(0).squeeze(0))[0]\n",
    "    voacb_logit = matrix_D(output_vector)\n",
    "    label_idx = sample_batched[1].squeeze(0).squeeze(0)\n",
    "    print(output_vector.shape, voacb_logit.shape, label_idx.shape)\n",
    "    if i_batch == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50258,    57,  6457,    72, 50259,  1073,  5853,  6128, 50260, 47189,\n",
       "         50261,    18,   503,   286,   642, 50264,   380,   690,   485, 50265,\n",
       "          8505, 50257,    57,  6457,    72,   318,   257,  3988,    12, 13120,\n",
       "          6891,  6128,   351, 10768,  4536,   287,   262, 18180,   485,  1989,\n",
       "           351,   257,  1115,   503,   286,  1936,  6491,  7955,    13]),\n",
       " tensor([   57,  6457,    72,   318,   257,  3988,    12, 13120,  6891,  6128,\n",
       "           351, 10768,  4536,   287,   262, 18180,   485,  1989,   351,   257,\n",
       "          1115,   503,   286,  1936,  6491,  7955,    13, 50256]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batched[0].squeeze(0).squeeze(0), label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('<name>Zizzi <eatType>coffee shop <priceRange>moderate <customer rating>3 out of 5 <area>riverside <familyFriendly>yes <START>Zizzi is a kids-friendly coffee shop with moderate prices in the riverside area with a three out of five customer rating.',),\n",
       " ('Zizzi is a kids-friendly coffee shop with moderate prices in the riverside area with a three out of five customer rating. <|endoftext|>',))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batched[2], sample_batched[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50266])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voacb_logit[-5:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode('apple'), tokenizer.encode(' apple')\n",
    "# tokenizer.decode([20920, 318,   257, 7090, 19744,  7072,  1474,   383,   347,  3979])\n",
    "# tokenizer.convert_ids_to_tokens(17180), tokenizer.convert_tokens_to_ids('apple'), tokenizer.convert_tokens_to_string(['my','Ġhere','<START>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0403 08:01:22.214917 140153994127168 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/69f8d734111f39eaa51a85907bfdc81a7ef42242d638ffab6f77df305402b2b2.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0403 08:01:22.215691 140153994127168 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/38d28acc17953e356348dca948e152c653c0ccf5058a552eea30168e27f02046.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0403 08:01:22.295722 140153994127168 tokenization_utils.py:663] Adding <START> to the vocabulary\n",
      "I0403 08:01:22.297630 140153994127168 tokenization_utils.py:741] Assigning <START> to the bos_token key of the tokenizer\n",
      "I0403 08:01:23.081983 140153994127168 filelock.py:274] Lock 140150406728056 acquired on /home/ds_user1/.cache/torch/transformers/c8f887cdfff4327916f4b7ed06a379c0add42bd9c66e1fe3b4a5a8525a4b2678.e4da68877e47676a9ceb9fb82e7c751246f61bc204d806ebd36e86911e825095.lock\n",
      "I0403 08:01:23.083734 140153994127168 file_utils.py:479] https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-config.json not found in cache or force_download set to True, downloading to /home/ds_user1/.cache/torch/transformers/tmpom13lo30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a9adadd4ce48f39d2b17ed601aa76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=577, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0403 08:01:23.933815 140153994127168 file_utils.py:489] storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-config.json in cache at /home/ds_user1/.cache/torch/transformers/c8f887cdfff4327916f4b7ed06a379c0add42bd9c66e1fe3b4a5a8525a4b2678.e4da68877e47676a9ceb9fb82e7c751246f61bc204d806ebd36e86911e825095\n",
      "I0403 08:01:23.934561 140153994127168 file_utils.py:492] creating metadata file for /home/ds_user1/.cache/torch/transformers/c8f887cdfff4327916f4b7ed06a379c0add42bd9c66e1fe3b4a5a8525a4b2678.e4da68877e47676a9ceb9fb82e7c751246f61bc204d806ebd36e86911e825095\n",
      "I0403 08:01:23.935866 140153994127168 filelock.py:318] Lock 140150406728056 released on /home/ds_user1/.cache/torch/transformers/c8f887cdfff4327916f4b7ed06a379c0add42bd9c66e1fe3b4a5a8525a4b2678.e4da68877e47676a9ceb9fb82e7c751246f61bc204d806ebd36e86911e825095.lock\n",
      "I0403 08:01:23.936877 140153994127168 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c8f887cdfff4327916f4b7ed06a379c0add42bd9c66e1fe3b4a5a8525a4b2678.e4da68877e47676a9ceb9fb82e7c751246f61bc204d806ebd36e86911e825095\n",
      "I0403 08:01:23.937670 140153994127168 configuration_utils.py:311] Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0403 08:01:24.709419 140153994127168 filelock.py:274] Lock 140150406727384 acquired on /home/ds_user1/.cache/torch/transformers/bcc61dff8b1b03d0fd33a1eb1dc4db00875cae33296848155c6882d4bab03db4.999a50942f8e31ea6fa89ec2580cb38fa40e3db5aa46102d0406bcfa77d9142d.lock\n",
      "I0403 08:01:24.710845 140153994127168 file_utils.py:479] https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin not found in cache or force_download set to True, downloading to /home/ds_user1/.cache/torch/transformers/tmpbock9fgp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783bc9a49e1547f588c517b9bb842e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=3247202234, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (GPT2Model, GPT2Tokenizer, 'gpt2-large')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "model.save_pretrained('./gpt_model/large_model/')  # save\n",
    "tokenizer.save_pretrained('./gpt_model/large_model/')  # save\n",
    "    \n",
    "# special_tokens = {'bos_token': '<START>'}\n",
    "# tokenizer.add_special_tokens(special_tokens)\n",
    "# model = model_class.from_pretrained(pretrained_weights)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

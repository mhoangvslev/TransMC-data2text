{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "my_model.load_state_dict(torch.load('./gen_model/base/2/model'))\n",
    "print('ok') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class e2eDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file)\n",
    "        self.columns = self.dataset.columns\n",
    "        self.conditions = self.dataset[self.columns[0]]\n",
    "        self.sentences = self.dataset[self.columns[1]]\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.typ_list = {}\n",
    "        for k in range(len(self.conditions)):\n",
    "            cond_set = self.conditions[k].split(',')\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] in self.typ_list.keys():\n",
    "                    self.typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "                else:            \n",
    "                    self.typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conditions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cond = self.conditions[idx]\n",
    "        cond_set = cond.split(',')\n",
    "        condition_string = ''\n",
    "        for m in range(len(cond_set)):\n",
    "            cond_set[m] = cond_set[m].strip()\n",
    "            pos = cond_set[m].index('[')\n",
    "            \n",
    "            condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        \n",
    "        sen = self.sentences[idx]\n",
    "        input_string = condition_string + '<START>'\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(input_string, add_special_tokens=True))\n",
    "        \n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        return input_ids, sen, condition_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2e_dataset_test = e2eDataset(csv_file='dataset/testset_w_refs.csv', tokenizer=my_model.tokenizer)\n",
    "# e2e_dataset_test2 = e2eDataset(csv_file='dataset/testset.csv', tokenizer=my_model.tokenizer)\n",
    "# e2e_dataset_train = e2eDataset(csv_file='dataset/trainset.csv', tokenizer=my_model.tokenizer)\n",
    "e2e_dataset = e2eDataset(csv_file='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=False, num_workers=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(e2e_dataset_train), len(e2e_dataset), len(e2e_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in e2e_dataset_train.typ_list.items():\n",
    "    print(k, len(v))\n",
    "e2e_dataset_train.typ_list['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = my_model.tokenizer.additional_special_tokens_ids\n",
    "x = [50258, 50259, 50263, 50260, 50261, 50264, 50265, 50262]\n",
    "print(x)\n",
    "print(my_model.tokenizer.all_special_tokens)\n",
    "for i in range(len(x)):\n",
    "    print(my_model.tokenizer.decode([x[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "same_condition = []\n",
    "ref_sentences = []\n",
    "input_ids_list = []\n",
    "pre_condition_string = ''\n",
    "start = 0\n",
    "for i_batch, sample_batched in tqdm(enumerate(dataloader)):\n",
    "    sen = sample_batched[1][0]\n",
    "#     print(i_batch, sen)\n",
    "    condition_string = sample_batched[2]  \n",
    "    input_ids = sample_batched[0].squeeze(0).cuda()\n",
    "        \n",
    "    if start == 0 or condition_string == pre_condition_string:      \n",
    "        if start == 0:\n",
    "            input_ids_list.append(input_ids)\n",
    "        same_condition.append(sen)        \n",
    "        pre_condition_string = condition_string\n",
    "        start += 1\n",
    "    else:   \n",
    "        input_ids_list.append(input_ids)\n",
    "        ref_sentences.append(same_condition)\n",
    "        pre_condition_string = condition_string\n",
    "        same_condition = [sen]\n",
    "        start += 1\n",
    "\n",
    "#     if i_batch == 30:\n",
    "#         break            \n",
    "ref_sentences.append(same_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_sentences), len(input_ids_list)\n",
    "# input_ids_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "max_len = 50\n",
    "start_time = time.time()\n",
    "bleu_score = 0\n",
    "bleu_1 = 0\n",
    "\n",
    "f_dev = open('./predictions/base/f_dev_2.txt', 'w')\n",
    "f_pred = open('./predictions/base/f_pred_2.txt', 'w')\n",
    "\n",
    "for k in range(len(ref_sentences)):\n",
    "    input_ids = input_ids_list[k]\n",
    "    input_len = len(input_ids)\n",
    "\n",
    "    ori_tokens = []\n",
    "    for m in range(len(ref_sentences[k])):\n",
    "        f_dev.write(ref_sentences[k][m]+'\\n')\n",
    "        ori_tokens.append(word_tokenize(ref_sentences[k][m]))\n",
    "    if k < len(ref_sentences)-1:\n",
    "        f_dev.write('\\n')\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "        pred_idx = model_out.argmax(1)[-1]        \n",
    "        if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "            break            \n",
    "        input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "    \n",
    "    out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "    f_pred.write(out_sen+'\\n')\n",
    "    \n",
    "#     print(ref_sentences[k])\n",
    "#     print(out_sen)    \n",
    "    \n",
    "    out_tokens = word_tokenize(out_sen)\n",
    "    \n",
    "    bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "    bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    bleu_1 += bleu_1_score\n",
    "\n",
    "    bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))    \n",
    "    \n",
    "#     print(\"time: {}\".format(time.time()-start_time))\n",
    "#     print('')\n",
    "f_dev.close()\n",
    "f_pred.close()\n",
    "    \n",
    "print(\"BLEU score: {}\".format(bleu_score/len(ref_sentences)*100))\n",
    "print(\"BLEU1 score: {}\".format(bleu_1/len(ref_sentences)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU score: 71.18825887456514\n",
    "BLEU1 score: 88.76109911863252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./measure_scores.py ../predictions/base/f_dev_6.txt ../predictions/base/f_pred_6.txt\n",
    "./measure_scores.py ../predictions/base2/f_dev_2.txt ../predictions/base2/f_pred_2.txt\n",
    "./measure_scores.py ../predictions/large/f_dev_5.txt ../predictions/large/f_pred_5.txt\n",
    "./measure_scores.py ../predictions/f_dev_2.txt ../predictions/large2/f_pred_2.txt\n",
    "\n",
    "### Large\n",
    "#### my_output_1\n",
    "BLEU: 0.6826\n",
    "NIST: 8.4243\n",
    "METEOR: 0.4557\n",
    "ROUGE_L: 0.7032\n",
    "CIDEr: 2.1234\n",
    "\n",
    "#### my_output_2\n",
    "BLEU: 0.7228\n",
    "NIST: 8.5241\n",
    "METEOR: 0.4851\n",
    "ROUGE_L: 0.7461\n",
    "CIDEr: 2.4645\n",
    "\n",
    "#### my_output_3\n",
    "BLEU: 0.7035\n",
    "NIST: 8.5937\n",
    "METEOR: 0.4700\n",
    "ROUGE_L: 0.7252\n",
    "CIDEr: 2.3310\n",
    "\n",
    "#### my_output_4\n",
    "BLEU: 0.6738\n",
    "NIST: 8.4018\n",
    "METEOR: 0.4576\n",
    "ROUGE_L: 0.7075\n",
    "CIDEr: 2.2172\n",
    "\n",
    "#### my_output_5\n",
    "BLEU: 0.6927\n",
    "NIST: 8.4429\n",
    "METEOR: 0.4662\n",
    "ROUGE_L: 0.7180\n",
    "CIDEr: 2.2729\n",
    "\n",
    "### base\n",
    "#### my_output_2\n",
    "BLEU: 0.6812\n",
    "NIST: 8.5491\n",
    "METEOR: 0.4442\n",
    "ROUGE_L: 0.7036\n",
    "CIDEr: 2.1261\n",
    "\n",
    "#### my_output_6\n",
    "BLEU: 0.6655\n",
    "NIST: 8.4830\n",
    "METEOR: 0.4475\n",
    "ROUGE_L: 0.6992\n",
    "CIDEr: 2.1077\n",
    "    \n",
    "#### my_output_final\n",
    "BLEU: 0.6529\n",
    "NIST: 8.3116\n",
    "METEOR: 0.4430\n",
    "ROUGE_L: 0.6842\n",
    "CIDEr: 1.9766\n",
    "\n",
    "### Pragmatically Informative Text Generation\n",
    "BLEU 68.60\n",
    "NIST 8.73\n",
    "METEOR 45.25\n",
    "R-L 70.82\n",
    "CIDEr 2.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 00:51:08.586743 139954174723904 file_utils.py:41] PyTorch version 1.2.0 available.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package punkt to /home/ds_user1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "I0506 00:51:12.681335 139954174723904 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0506 00:51:12.682127 139954174723904 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 00:51:12.866730 139954174723904 tokenization_utils.py:663] Adding <START> to the vocabulary\n",
      "I0506 00:51:12.868072 139954174723904 tokenization_utils.py:741] Assigning <START> to the bos_token key of the tokenizer\n",
      "I0506 00:51:12.868580 139954174723904 tokenization_utils.py:663] Adding <name> to the vocabulary\n",
      "I0506 00:51:12.869179 139954174723904 tokenization_utils.py:663] Adding <eatType> to the vocabulary\n",
      "I0506 00:51:12.869720 139954174723904 tokenization_utils.py:663] Adding <priceRange> to the vocabulary\n",
      "I0506 00:51:12.870337 139954174723904 tokenization_utils.py:663] Adding <customer rating> to the vocabulary\n",
      "I0506 00:51:12.870880 139954174723904 tokenization_utils.py:663] Adding <near> to the vocabulary\n",
      "I0506 00:51:12.871432 139954174723904 tokenization_utils.py:663] Adding <food> to the vocabulary\n",
      "I0506 00:51:12.871983 139954174723904 tokenization_utils.py:663] Adding <area> to the vocabulary\n",
      "I0506 00:51:12.873968 139954174723904 tokenization_utils.py:663] Adding <familyFriendly> to the vocabulary\n",
      "I0506 00:51:12.874541 139954174723904 tokenization_utils.py:741] Assigning ['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>'] to the additional_special_tokens key of the tokenizer\n",
      "I0506 00:51:13.651663 139954174723904 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/ds_user1/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5\n",
      "I0506 00:51:13.652786 139954174723904 configuration_utils.py:311] Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0506 00:51:14.434033 139954174723904 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4693it [00:02, 1630.54it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th model\n",
      "BLEU score: 67.011002844619\n",
      "BLEU1 score: 82.1452596341907\n",
      "ok\n",
      "2 th model\n",
      "BLEU score: 74.80444465381424\n",
      "BLEU1 score: 89.29666997331987\n",
      "ok\n",
      "3 th model\n",
      "BLEU score: 72.97053596564822\n",
      "BLEU1 score: 87.38864333690032\n",
      "ok\n",
      "4 th model\n",
      "BLEU score: 72.98439792676945\n",
      "BLEU1 score: 88.62888868036747\n",
      "ok\n",
      "5 th model\n",
      "BLEU score: 74.13844074346744\n",
      "BLEU1 score: 89.21020861290603\n",
      "ok\n",
      "6 th model\n",
      "BLEU score: 74.42687041332428\n",
      "BLEU1 score: 89.68754978933163\n",
      "ok\n",
      "7 th model\n",
      "BLEU score: 74.26234161453121\n",
      "BLEU1 score: 88.89235991750782\n",
      "ok\n",
      "8 th model\n",
      "BLEU score: 73.61824480493395\n",
      "BLEU1 score: 88.92714291178227\n"
     ]
    }
   ],
   "source": [
    "# from model_large import *\n",
    "from model import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "max_len = 70\n",
    "\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "\n",
    "for i in range(1, 9):\n",
    "    model_name = './gen_model/base2_sample_30/'+str(i)+'/model'\n",
    "    my_model.load_state_dict(torch.load(model_name))\n",
    "    print('ok') \n",
    "    if i == 1:\n",
    "#         e2e_dataset = e2eDataset(csv_file='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "        e2e_dataset = e2eDataset(csv_file='dataset/testset_w_refs.csv', tokenizer=my_model.tokenizer)\n",
    "        dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        same_condition = []\n",
    "        ref_sentences = []\n",
    "        input_ids_list = []\n",
    "        pre_condition_string = ''\n",
    "        start = 0\n",
    "        for i_batch, sample_batched in tqdm(enumerate(dataloader)):\n",
    "            sen = sample_batched[1][0]\n",
    "            condition_string = sample_batched[2]  \n",
    "            input_ids = sample_batched[0].squeeze(0).cuda()\n",
    "\n",
    "            if start == 0 or condition_string == pre_condition_string:      \n",
    "                if start == 0:\n",
    "                    input_ids_list.append(input_ids)\n",
    "                same_condition.append(sen)        \n",
    "                pre_condition_string = condition_string\n",
    "                start += 1\n",
    "            else:   \n",
    "                input_ids_list.append(input_ids)\n",
    "                ref_sentences.append(same_condition)\n",
    "                pre_condition_string = condition_string\n",
    "                same_condition = [sen]\n",
    "                start += 1\n",
    "        ref_sentences.append(same_condition)    \n",
    "\n",
    "    bleu_score = 0\n",
    "    bleu_1 = 0\n",
    "\n",
    "#     f_dev = open('./predictions/testset/large2/f_dev_'+str(i)+'.txt', 'w')\n",
    "#     f_pred = open('./predictions/devset/base4/f_pred_'+str(i)+'.txt', 'w')\n",
    "    f_pred = open('./predictions/joosung2/testset/base2_sample30/f_pred_'+str(i)+'.txt', 'w')\n",
    "\n",
    "    for k in range(len(ref_sentences)):\n",
    "        input_ids = input_ids_list[k]\n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        ori_tokens = []\n",
    "        for m in range(len(ref_sentences[k])):\n",
    "#             f_dev.write(ref_sentences[k][m]+'\\n')\n",
    "            ori_tokens.append(word_tokenize(ref_sentences[k][m]))\n",
    "#         if k < len(ref_sentences)-1:\n",
    "#             f_dev.write('\\n')\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "            pred_idx = model_out.argmax(1)[-1]        \n",
    "            if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "                break            \n",
    "            input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "\n",
    "        out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "        f_pred.write(out_sen+'\\n')\n",
    "\n",
    "        out_tokens = word_tokenize(out_sen)\n",
    "\n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "        bleu_1 += bleu_1_score\n",
    "\n",
    "        bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))    \n",
    "\n",
    "#     f_dev.close()\n",
    "    f_pred.close()\n",
    "    \n",
    "    print(i, \"th model\")\n",
    "    print(\"BLEU score: {}\".format(bleu_score/len(ref_sentences)*100))\n",
    "    print(\"BLEU1 score: {}\".format(bleu_1/len(ref_sentences)*100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bfe329cacbe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me2e_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me2eDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dataset/testset_w_refs.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=False, num_workers=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0me2e_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "e2e_dataset = e2eDataset(csv_file='dataset/testset_w_refs.csv', tokenizer=my_model.tokenizer)\n",
    "# dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "e2e_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 02:25:27.922973 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:25:27.923840 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:25:29.478046 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0506 02:25:29.478856 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 02:25:30.319428 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:25:30.320361 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:25:31.106602 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c897c338340f48caba99df4545a1bb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164d5f92e4004174ba60a609449cfb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 27.33 seconds, 171.71 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 02:26:16.694179 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:26:16.695167 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:26:18.249252 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0506 02:26:18.250080 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 02:26:19.077619 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:26:19.078688 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:26:19.853295 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b9e4265598418cb788bf58f9199291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0421ed079da43e99b556a37390ea3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 27.71 seconds, 169.35 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 02:26:56.707400 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:26:56.708481 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:26:58.288155 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0506 02:26:58.288866 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 02:26:59.169553 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:26:59.170468 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:26:59.945120 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432686478f474e31a29a228b47cdf1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6994b99e632147b7ac1e015e408a6002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 28.17 seconds, 166.59 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 02:27:37.320190 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:27:37.321148 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:27:38.949697 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0506 02:27:38.950402 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 02:27:39.789372 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:27:39.790431 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:27:40.567417 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dec15558b6a466b97fe24b59de0e8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402a4948ee954514b4f44836135f4928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 28.08 seconds, 167.10 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 02:28:17.290150 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:28:17.291114 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:28:18.890244 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0506 02:28:18.890929 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 02:28:19.722401 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:28:19.723473 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:28:20.535593 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c47aa9f0ae74eeb8cc80620b1947fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=81.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc081cdbc2f41d2a8addd5ba37d309d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 28.85 seconds, 162.66 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 02:28:58.396482 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:28:58.397446 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:29:00.032734 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0506 02:29:00.033463 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 02:29:00.857808 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:29:00.858852 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:29:01.687607 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3865eaf77a104441931f3779fa677468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbe1a810943414cb4f1dac5eb88cde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 28.54 seconds, 164.41 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 02:29:39.391092 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:29:39.392071 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:29:40.928152 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0506 02:29:40.928829 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 02:29:41.765873 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:29:41.766922 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:29:42.531049 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e014ed0f5e68475fb3f9806a4b20b0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=81.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb751221fda4056a93ba24bfcae41c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 28.56 seconds, 164.34 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 02:30:20.333524 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:30:20.334524 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:30:21.939299 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0506 02:30:21.940119 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 02:30:22.790251 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:30:22.791215 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:30:23.654959 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5f749e95e743f48762f10c84e3822b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f796491bfac34d7ca74d937b2ca5cd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 28.88 seconds, 162.51 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 02:31:01.515883 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:31:01.516969 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:31:03.053394 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0506 02:31:03.054218 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 02:31:03.886138 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0506 02:31:03.887118 140215530821440 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0506 02:31:04.777847 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2621a0b23bb4f2eaa5aeddf4629ac3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88adaf38a602403faa1c6d4ef95d00be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 28.80 seconds, 162.94 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "### BERT score with human reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_files = \"/project/work/E2E/predictions/joosung2/testset/f_dev.txt\"\n",
    "\n",
    "human_open = open(human_files, \"r\")\n",
    "human_dataset = human_open.readlines()\n",
    "human_open.close()\n",
    "\n",
    "human_references = []\n",
    "\n",
    "temp_reference = []\n",
    "for i in range(len(human_dataset)):\n",
    "    if human_dataset[i] == '\\n':\n",
    "        human_references.append(temp_reference)\n",
    "        temp_reference = []\n",
    "    else:\n",
    "        temp_reference.append(human_dataset[i].strip())\n",
    "human_references.append(temp_reference)\n",
    "human_compare = []\n",
    "for i in range(len(human_references)):\n",
    "    for k in range(len(human_references[i])):\n",
    "        human_compare.append(human_references[i][k])\n",
    "\n",
    "# output_path = \"/project/work/E2E/predictions/joosung2/testset/base1_sample30/*\"\n",
    "output_path = \"/project/work/E2E/predictions/final/*\"\n",
    "pred_files = glob.glob(output_path)\n",
    "# pred_files = [\"/project/work/E2E/predictions/final/sampling30_1.txt\"]\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(pred_files)):    \n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        repeat_num = len(human_references[k])\n",
    "        for _ in range(repeat_num):\n",
    "            cands.append(out_sen)\n",
    "\n",
    "    P, R, F1 = score(cands, human_compare, lang='en', verbose=True)\n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    \n",
    "    score_list.append(BERT_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/project/work/E2E/predictions/final/final_model.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling10_3.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling30_1.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling10_1.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling50.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling30_3.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling30_2.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling10_2.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling30_4.txt'],\n",
       " [0.9420184408096827,\n",
       "  0.939660089205964,\n",
       "  0.9404677509270877,\n",
       "  0.9385546076889615,\n",
       "  0.9420467598235559,\n",
       "  0.9393550385271541,\n",
       "  0.9410023027687674,\n",
       "  0.9391040466193982,\n",
       "  0.9402433829776976])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_files, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compared System / BERT score with human reference\n",
    "import csv\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_files = \"/project/work/E2E/predictions/testset/f_dev.txt\"\n",
    "\n",
    "human_open = open(human_files, \"r\")\n",
    "human_dataset = human_open.readlines()\n",
    "human_open.close()\n",
    "\n",
    "human_references = []\n",
    "\n",
    "temp_reference = []\n",
    "for i in range(len(human_dataset)):\n",
    "    if human_dataset[i] == '\\n':\n",
    "        human_references.append(temp_reference)\n",
    "        temp_reference = []\n",
    "    else:\n",
    "        temp_reference.append(human_dataset[i].strip())\n",
    "human_references.append(temp_reference)\n",
    "human_compare = []\n",
    "for i in range(len(human_references)):\n",
    "    for k in range(len(human_references[i])):\n",
    "        human_compare.append(human_references[i][k])\n",
    "\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/primary_txt/*\"\n",
    "pred_files = glob.glob(output_path)\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(pred_files)):    \n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        repeat_num = len(human_references[k])\n",
    "        for _ in range(repeat_num):\n",
    "            cands.append(out_sen)\n",
    "\n",
    "    P, R, F1 = score(cands, human_compare, lang='en', verbose=True)\n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    \n",
    "    score_list.append(BERT_score)  \n",
    "print(pred_files, score_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred_files)):\n",
    "    print(pred_files[i], score_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/slug.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 547362.73 tokens per second.\n",
      "PTBTokenizer tokenized 17228 tokens at 180441.17 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.445\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.677\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.262\n",
      "Creating temp directory  /tmp/e2e-eval-wbdld4sm\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 07:45:40\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-wbdld4sm/mteval_ref.sgm -s /tmp/e2e-eval-wbdld4sm/mteval_src.sgm -t /tmp/e2e-eval-wbdld4sm/mteval_sys.sgm -f /tmp/e2e-eval-wbdld4sm/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.6130  BLEU score = 0.6619 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.6389   1.4164   0.7348   0.4644   0.3587   0.1825   0.1016   0.0638   0.0434  \"tst\"\n",
      "\n",
      " BLEU:  0.9347   0.7661   0.5983   0.4527   0.3233   0.2350   0.1717   0.1239   0.0852  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.6389   7.0552   7.7900   8.2544   8.6130   8.7955   8.8971   8.9609   9.0043  \"tst\"\n",
      "\n",
      " BLEU:  0.9323   0.8440   0.7519   0.6619   0.5732   0.4938   0.4245   0.3638   0.3095  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 07:45:53\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6619\n",
      "NIST: 8.6130\n",
      "METEOR: 0.4454\n",
      "ROUGE_L: 0.6772\n",
      "CIDEr: 2.2615\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/slug-alt.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 545173.68 tokens per second.\n",
      "PTBTokenizer tokenized 17664 tokens at 160850.99 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.437\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.599\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.102\n",
      "Creating temp directory  /tmp/e2e-eval-u73c6a3v\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 07:46:10\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-u73c6a3v/mteval_ref.sgm -s /tmp/e2e-eval-u73c6a3v/mteval_src.sgm -t /tmp/e2e-eval-u73c6a3v/mteval_sys.sgm -f /tmp/e2e-eval-u73c6a3v/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.3954  BLEU score = 0.6035 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.6013   1.3976   0.7122   0.3993   0.2850   0.1340   0.0697   0.0338   0.0194  \"tst\"\n",
      "\n",
      " BLEU:  0.9200   0.7146   0.5312   0.3814   0.2511   0.1666   0.1117   0.0711   0.0393  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.6013   6.9989   7.7111   8.1103   8.3954   8.5293   8.5990   8.6328   8.6522  \"tst\"\n",
      "\n",
      " BLEU:  0.9191   0.8100   0.7035   0.6035   0.5063   0.4206   0.3480   0.2853   0.2289  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 07:46:23\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6035\n",
      "NIST: 8.3954\n",
      "METEOR: 0.4369\n",
      "ROUGE_L: 0.5991\n",
      "CIDEr: 2.1019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_1.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_2.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_3.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_4.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_5.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_6.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_7.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_8.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_9.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_10.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base1_sample10_6.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base2_sample10_9.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base4_devtest_4.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/2base1_sample50_7.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/slug.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/slug-alt.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tsv to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/primary/*\"\n",
    "comapred_files = glob.glob(output_path)\n",
    "\n",
    "for i in range(len(comapred_files)):\n",
    "    dataset = pd.read_csv(comapred_files[i], delimiter='\\t', header=None)\n",
    "    \n",
    "    name = comapred_files[i].split('/')[-1].split('.')[0]\n",
    "    txt_files = \"/project/work/E2E/compared_system/system_outputs/primary_txt/\"+name+\".txt\"\n",
    "    f = open(txt_files, \"w\")\n",
    "    gen_sentences = dataset[1]\n",
    "    \n",
    "    for k in range(1, len(gen_sentences)):\n",
    "        f.write(gen_sentences[k]+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gen_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero-shot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0507 00:59:26.720654 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0507 00:59:26.721606 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0507 00:59:26.798727 140215530821440 tokenization_utils.py:663] Adding <START> to the vocabulary\n",
      "I0507 00:59:26.799590 140215530821440 tokenization_utils.py:741] Assigning <START> to the bos_token key of the tokenizer\n",
      "I0507 00:59:26.800053 140215530821440 tokenization_utils.py:663] Adding <name> to the vocabulary\n",
      "I0507 00:59:26.800630 140215530821440 tokenization_utils.py:663] Adding <eatType> to the vocabulary\n",
      "I0507 00:59:26.801137 140215530821440 tokenization_utils.py:663] Adding <priceRange> to the vocabulary\n",
      "I0507 00:59:26.801638 140215530821440 tokenization_utils.py:663] Adding <customer rating> to the vocabulary\n",
      "I0507 00:59:26.802139 140215530821440 tokenization_utils.py:663] Adding <near> to the vocabulary\n",
      "I0507 00:59:26.802641 140215530821440 tokenization_utils.py:663] Adding <food> to the vocabulary\n",
      "I0507 00:59:26.803125 140215530821440 tokenization_utils.py:663] Adding <area> to the vocabulary\n",
      "I0507 00:59:26.803602 140215530821440 tokenization_utils.py:663] Adding <familyFriendly> to the vocabulary\n",
      "I0507 00:59:26.804103 140215530821440 tokenization_utils.py:741] Assigning ['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>'] to the additional_special_tokens key of the tokenizer\n",
      "I0507 00:59:27.610918 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/ds_user1/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5\n",
      "I0507 00:59:27.611916 140215530821440 configuration_utils.py:311] Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0507 00:59:28.384023 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import *\n",
    "\n",
    "max_len = 70\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "model_name = './gen_model/base_devtrain_4/4/model'\n",
    "my_model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<name>', '<eatType>', '<food>', '<customer rating>', '<area>', '<familyFriendly>', '<near>']\n",
      "['The Waterman', 'pub', 'Italian', 'high', 'city centre', 'yes', 'All Bar One']\n",
      "The Waterman is a highly rated, family-friendly pub in the city centre near All Bar One.\n",
      "<NAME> is a <CUSTOMER_RATING>ly rated, family-friendly <EATTYPE> in the <area> near <NEAR>.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "csv_file='dataset/testset_w_refs.csv'\n",
    "dataset = pd.read_csv(csv_file)\n",
    "columns = dataset.columns\n",
    "conditions = dataset[columns[0]]\n",
    "        \n",
    "typ_list = {}\n",
    "for k in range(len(conditions)):\n",
    "    cond_set = conditions[k].split(',')\n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        if cond_set[m][:pos] in typ_list.keys():\n",
    "            typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "        else:            \n",
    "            typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}   \n",
    "\n",
    "def sample_batch(tokenizer, cond_name, cond_set):\n",
    "    condition_string = ''\n",
    "    for m in range(len(cond_set)):\n",
    "        condition_string += cond_name[m] + cond_set[m] + ' '\n",
    "\n",
    "    input_string = condition_string + '<START>'\n",
    "    input_ids = torch.tensor(tokenizer.encode(input_string, add_special_tokens=True))\n",
    "\n",
    "    input_len = len(input_ids)\n",
    "\n",
    "    return input_ids, condition_string\n",
    "\n",
    "# <name> <eatType> <food> <priceRange> <customer rating> <area> <familyFriendly> <near>\n",
    "\n",
    "name = '<NAME>' # None\n",
    "eatType = '<EATTYPE>' # None\n",
    "food = '<FOOD>' # None \n",
    "priceRange = None # '<priceRange>'\n",
    "customer_rating = '<CUSTOMER_RATING>' # None \n",
    "area = '<area>' # None\n",
    "familyFriendly = 'yes' # None\n",
    "near = '<NEAR>' # None\n",
    "\n",
    "cond_list = []\n",
    "conditions = []\n",
    "if name is not None:\n",
    "    placeholder_name = random.choice(list(typ_list['name']))\n",
    "    cond_list.append('<name>')\n",
    "    conditions.append(placeholder_name)\n",
    "if eatType is not None:\n",
    "    placeholder_eatType = random.choice(list(typ_list['eatType']))\n",
    "    cond_list.append('<eatType>')\n",
    "    conditions.append(placeholder_eatType)\n",
    "if food is not None:\n",
    "    placeholder_food = random.choice(list(typ_list['food']))\n",
    "    cond_list.append('<food>')\n",
    "    conditions.append(placeholder_food)\n",
    "if priceRange is not None:\n",
    "    placeholder_priceRange = random.choice(list(typ_list['priceRange']))\n",
    "    cond_list.append('<priceRange>')\n",
    "    conditions.append(placeholder_priceRange)    \n",
    "if customer_rating is not None:\n",
    "    placeholder_customer_rating = random.choice(list(typ_list['customer rating']))\n",
    "    cond_list.append('<customer rating>')\n",
    "    conditions.append(placeholder_customer_rating)        \n",
    "if area is not None:\n",
    "    placeholder_area = random.choice(list(typ_list['area']))\n",
    "    cond_list.append('<area>')\n",
    "    conditions.append(placeholder_area)    \n",
    "if familyFriendly is not None:\n",
    "    cond_list.append('<familyFriendly>')\n",
    "    conditions.append(familyFriendly)            \n",
    "if near is not None:\n",
    "    placeholder_near = random.choice(list(typ_list['near']))\n",
    "    cond_list.append('<near>')\n",
    "    conditions.append(placeholder_near)        \n",
    "\n",
    "\n",
    "# del cond_name[2]\n",
    "# del conditions[2]\n",
    "\n",
    "sample = sample_batch(my_model.tokenizer, cond_list, conditions)\n",
    "\n",
    "input_ids = sample[0].cuda()\n",
    "condition_string = sample[1]  \n",
    "input_len = len(input_ids)\n",
    "\n",
    "max_len = 70\n",
    "for _ in range(max_len):\n",
    "    model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "    pred_idx = model_out.argmax(1)[-1]        \n",
    "    if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "        break            \n",
    "    input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "\n",
    "out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "print(cond_list)\n",
    "print(conditions)\n",
    "print(out_sen)\n",
    "\n",
    "if name is not None:\n",
    "    out_sen = out_sen.replace(placeholder_name, name)\n",
    "if eatType is not None:\n",
    "    out_sen = out_sen.replace(placeholder_eatType, eatType)\n",
    "if food is not None:\n",
    "    out_sen = out_sen.replace(placeholder_food, food)\n",
    "if priceRange is not None:\n",
    "    out_sen = out_sen.replace(placeholder_priceRange, priceRange)\n",
    "if customer_rating is not None:\n",
    "    out_sen = out_sen.replace(placeholder_customer_rating, customer_rating)\n",
    "if area is not None:\n",
    "    out_sen = out_sen.replace(placeholder_area, area)  \n",
    "if near is not None:\n",
    "    out_sen = out_sen.replace(placeholder_near, near)\n",
    "print(out_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delexicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class e2eDataset(Dataset):\n",
    "    def __init__(self, csv_file1, csv_file2, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "        \"\"\"\n",
    "        self.dataset1 = pd.read_csv(csv_file1)\n",
    "        self.dataset2 = pd.read_csv(csv_file2)\n",
    "        \n",
    "        self.columns1 = self.dataset1.columns\n",
    "        self.columns2 = self.dataset2.columns\n",
    "        \n",
    "        self.conditions = list(self.dataset1[self.columns1[0]]) + list(self.dataset2[self.columns2[0]])\n",
    "        self.sentences = list(self.dataset1[self.columns1[1]]) + list(self.dataset2[self.columns2[1]])\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.typ_list = {}\n",
    "        for k in range(len(self.conditions)):\n",
    "            cond_set = self.conditions[k].split(',')\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] in self.typ_list.keys():\n",
    "                    self.typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "                else:            \n",
    "                    self.typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conditions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sen = self.sentences[idx]\n",
    "        \n",
    "        cond = self.conditions[idx]\n",
    "        cond_set = cond.split(',')        \n",
    "        condition_string = ''\n",
    "        \n",
    "        \n",
    "        p = random.random()\n",
    "\n",
    "        if p > 0.3: # 70%\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "\n",
    "                condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        else: # p <= 0.3 / 30%\n",
    "            nochange_list = ['priceRange', 'customer rating', 'familyFriendly']\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                \n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] not in nochange_list:\n",
    "                    placeholder = '<' + cond_set[m][:pos] + '>'\n",
    "                    condition_string += placeholder + ' '\n",
    "                    sen = sen.replace(cond_set[m][pos+1:-1], placeholder)        \n",
    "                else:\n",
    "                    condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        \n",
    "        input_string = condition_string + '<START>' + sen\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(input_string, add_special_tokens=True))\n",
    "        \n",
    "        label_string = sen + ' <|endoftext|>'\n",
    "        label_ids = torch.tensor(self.tokenizer.encode(label_string, add_special_tokens=True))\n",
    "\n",
    "        return input_string, input_ids, label_ids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_dataset = e2eDataset(csv_file1='dataset/trainset.csv', csv_file2='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "print(e2e_dataset[200][0])\n",
    "# print(e2e_dataset.typ_list.keys())\n",
    "# print(e2e_dataset.typ_list)\n",
    "# priceRange, customer rating, familyFriendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dev = open('f_dev.txt', 'r')\n",
    "f_pred = open('f_pred.txt', 'r')\n",
    "f_dev_dataset = f_dev.readlines()\n",
    "f_pred_dataset = f_pred.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f_dev_dataset), len(f_pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[x for x in f_dev_dataset if x != '\\n']\n",
    "data2=[x for x in f_pred_dataset if x != '\\n']\n",
    "len(data), len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4672+546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dev = open('./e2e-metrics/example-inputs/devel-conc.txt', 'r')\n",
    "f_pred = open('./e2e-metrics/example-inputs/baseline-output.txt', 'r')\n",
    "f_dev_dataset = f_dev.readlines()\n",
    "f_pred_dataset = f_pred.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f_dev_dataset), len(f_pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[x for x in f_dev_dataset if x != '\\n']\n",
    "data2=[x for x in f_pred_dataset if x != '\\n']\n",
    "len(data), len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "p=random.random()\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouput 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "csv_file=\"/project/work/E2E/dataset/testset_w_refs.csv\"\n",
    "dataset = pd.read_csv(csv_file)\n",
    "columns = dataset.columns\n",
    "conditions = dataset[columns[0]]\n",
    "refs = dataset[columns[1]]\n",
    "\n",
    "ref_conditions = []\n",
    "ref_sentences = []\n",
    "pre_condition = ''\n",
    "start = 0\n",
    "for k in range(len(refs)):\n",
    "    sen = refs[k]    \n",
    "        \n",
    "    if start == 0 or conditions[k] == pre_condition:      \n",
    "        if start == 0:\n",
    "            ref_sentences.append(sen)\n",
    "            ref_conditions.append(conditions[k])\n",
    "        pre_condition = conditions[k]\n",
    "        start += 1\n",
    "    else:\n",
    "        ref_sentences.append(sen)\n",
    "        ref_conditions.append(conditions[k])\n",
    "        pre_condition = conditions[k]\n",
    "        start += 1        \n",
    "\n",
    "\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/compared/\"\n",
    "pred_files = [output_path+'our_model.txt',output_path+'slug.txt',output_path+'tgen.txt', output_path+'tuda.txt']\n",
    "\n",
    "sentence_list = []\n",
    "file_name = ['our_model', 'slug', 'tgen', 'tuda', 'references']\n",
    "for i in range(len(pred_files)):\n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    sentence_list.append(cands)  \n",
    "sentence_list.append(ref_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(output_path + 'total.txt', 'w')\n",
    "\n",
    "print(file_name)\n",
    "\n",
    "for ind in range(630):\n",
    "#     if sentence_list[0][ind] != sentence_list[-1][ind]:\n",
    "    f.write(str(ind) + '\\t' + ref_conditions[ind] + '\\n')\n",
    "    print(ind, ref_conditions[ind])\n",
    "    for k in range(len(file_name)):\n",
    "        print(sentence_list[k][ind])\n",
    "        f.write(sentence_list[k][ind] + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zizzi is a pub near The Sorrento. Close to The Sorrento you will be able to find a pub called Zizzi\n"
     ]
    }
   ],
   "source": [
    "if sentence_list[0][ind] != sentence_list[-1][ind]:\n",
    "    print(sentence_list[0][ind], sentence_list[-1][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including Slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our_model', 'slug', 'tgen', 'tuda', 'references']\n",
      "[0.9512169312169305, 0.9839606953892663, 0.855699168556305, 0.964172335600906, 0.864580498866211]\n"
     ]
    }
   ],
   "source": [
    "print(file_name)\n",
    "\n",
    "include_percent = [0 for _ in range(len(file_name))]\n",
    "for ind in range(630):\n",
    "    cond_dict = {}    \n",
    "    cond_set = ref_conditions[ind].split(',')\n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        cond_dict[cond_set[m][:pos]] = (cond_set[m][pos+1:-1])    \n",
    "    \n",
    "    for m in range(len(file_name)):\n",
    "        num = len(cond_dict)\n",
    "        sentence_temp = sentence_list[m][ind]\n",
    "        count_temp  = 0\n",
    "        \n",
    "        for k, v in cond_dict.items():\n",
    "            if k != 'familyFriendly':\n",
    "                if v in sentence_temp:\n",
    "                    count_temp += 1\n",
    "            else:\n",
    "                num -= 1\n",
    "                \n",
    "        include_percent[m] += count_temp/num\n",
    "                    \n",
    "include_final = [x/630 for x in include_percent]\n",
    "print(include_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our_model', 'slug', 'tgen', 'tuda', 'references']\n",
      "[0.9668253968253964, 0.9839606953892663, 0.9823507180650033, 0.9844293272864691, 0.8871957671957653]\n"
     ]
    }
   ],
   "source": [
    "print(file_name)\n",
    "\n",
    "include_percent = [0 for _ in range(len(file_name))]\n",
    "for ind in range(630):\n",
    "    cond_dict = {}    \n",
    "    cond_set = ref_conditions[ind].split(',')\n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        cond_dict[cond_set[m][:pos]] = (cond_set[m][pos+1:-1])    \n",
    "    \n",
    "    for m in range(len(file_name)):\n",
    "        num = len(cond_dict)\n",
    "        sentence_temp = sentence_list[m][ind]\n",
    "        count_temp  = 0\n",
    "        \n",
    "        for k, v in cond_dict.items():\n",
    "            if k != 'familyFriendly':\n",
    "                if v.lower() in sentence_temp.strip().lower():\n",
    "                    count_temp += 1\n",
    "#                 else:\n",
    "#                     print(v, '##', sentence_temp)\n",
    "            else:\n",
    "                num -= 1\n",
    "                \n",
    "        include_percent[m] += count_temp/num\n",
    "                    \n",
    "include_final = [x/630 for x in include_percent]\n",
    "print(include_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x='English'\n",
    "x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

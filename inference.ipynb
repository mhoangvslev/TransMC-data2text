{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "my_model.load_state_dict(torch.load('./gen_model/base/2/model'))\n",
    "print('ok') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class e2eDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file)\n",
    "        self.columns = self.dataset.columns\n",
    "        self.conditions = self.dataset[self.columns[0]]\n",
    "        self.sentences = self.dataset[self.columns[1]]\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.typ_list = {}\n",
    "        for k in range(len(self.conditions)):\n",
    "            cond_set = self.conditions[k].split(',')\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] in self.typ_list.keys():\n",
    "                    self.typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "                else:            \n",
    "                    self.typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conditions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cond = self.conditions[idx]\n",
    "        cond_set = cond.split(',')\n",
    "        condition_string = ''\n",
    "        for m in range(len(cond_set)):\n",
    "            cond_set[m] = cond_set[m].strip()\n",
    "            pos = cond_set[m].index('[')\n",
    "            \n",
    "            condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        \n",
    "        sen = self.sentences[idx]\n",
    "        input_string = condition_string + '<START>'\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(input_string, add_special_tokens=True))\n",
    "        \n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        return input_ids, sen, condition_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2e_dataset_test = e2eDataset(csv_file='dataset/testset_w_refs.csv', tokenizer=my_model.tokenizer)\n",
    "# e2e_dataset_test2 = e2eDataset(csv_file='dataset/testset.csv', tokenizer=my_model.tokenizer)\n",
    "# e2e_dataset_train = e2eDataset(csv_file='dataset/trainset.csv', tokenizer=my_model.tokenizer)\n",
    "e2e_dataset = e2eDataset(csv_file='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=False, num_workers=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(e2e_dataset_train), len(e2e_dataset), len(e2e_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in e2e_dataset_train.typ_list.items():\n",
    "    print(k, len(v))\n",
    "e2e_dataset_train.typ_list['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = my_model.tokenizer.additional_special_tokens_ids\n",
    "x = [50258, 50259, 50263, 50260, 50261, 50264, 50265, 50262]\n",
    "print(x)\n",
    "print(my_model.tokenizer.all_special_tokens)\n",
    "for i in range(len(x)):\n",
    "    print(my_model.tokenizer.decode([x[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "same_condition = []\n",
    "ref_sentences = []\n",
    "input_ids_list = []\n",
    "pre_condition_string = ''\n",
    "start = 0\n",
    "for i_batch, sample_batched in tqdm(enumerate(dataloader)):\n",
    "    sen = sample_batched[1][0]\n",
    "#     print(i_batch, sen)\n",
    "    condition_string = sample_batched[2]  \n",
    "    input_ids = sample_batched[0].squeeze(0).cuda()\n",
    "        \n",
    "    if start == 0 or condition_string == pre_condition_string:      \n",
    "        if start == 0:\n",
    "            input_ids_list.append(input_ids)\n",
    "        same_condition.append(sen)        \n",
    "        pre_condition_string = condition_string\n",
    "        start += 1\n",
    "    else:   \n",
    "        input_ids_list.append(input_ids)\n",
    "        ref_sentences.append(same_condition)\n",
    "        pre_condition_string = condition_string\n",
    "        same_condition = [sen]\n",
    "        start += 1\n",
    "\n",
    "#     if i_batch == 30:\n",
    "#         break            \n",
    "ref_sentences.append(same_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_sentences), len(input_ids_list)\n",
    "# input_ids_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "max_len = 50\n",
    "start_time = time.time()\n",
    "bleu_score = 0\n",
    "bleu_1 = 0\n",
    "\n",
    "f_dev = open('./predictions/base/f_dev_2.txt', 'w')\n",
    "f_pred = open('./predictions/base/f_pred_2.txt', 'w')\n",
    "\n",
    "for k in range(len(ref_sentences)):\n",
    "    input_ids = input_ids_list[k]\n",
    "    input_len = len(input_ids)\n",
    "\n",
    "    ori_tokens = []\n",
    "    for m in range(len(ref_sentences[k])):\n",
    "        f_dev.write(ref_sentences[k][m]+'\\n')\n",
    "        ori_tokens.append(word_tokenize(ref_sentences[k][m]))\n",
    "    if k < len(ref_sentences)-1:\n",
    "        f_dev.write('\\n')\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "        pred_idx = model_out.argmax(1)[-1]        \n",
    "        if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "            break            \n",
    "        input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "    \n",
    "    out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "    f_pred.write(out_sen+'\\n')\n",
    "    \n",
    "#     print(ref_sentences[k])\n",
    "#     print(out_sen)    \n",
    "    \n",
    "    out_tokens = word_tokenize(out_sen)\n",
    "    \n",
    "    bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "    bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    bleu_1 += bleu_1_score\n",
    "\n",
    "    bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))    \n",
    "    \n",
    "#     print(\"time: {}\".format(time.time()-start_time))\n",
    "#     print('')\n",
    "f_dev.close()\n",
    "f_pred.close()\n",
    "    \n",
    "print(\"BLEU score: {}\".format(bleu_score/len(ref_sentences)*100))\n",
    "print(\"BLEU1 score: {}\".format(bleu_1/len(ref_sentences)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU score: 71.18825887456514\n",
    "BLEU1 score: 88.76109911863252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./measure_scores.py ../predictions/base/f_dev_6.txt ../predictions/base/f_pred_6.txt\n",
    "./measure_scores.py ../predictions/base2/f_dev_2.txt ../predictions/base2/f_pred_2.txt\n",
    "./measure_scores.py ../predictions/large/f_dev_5.txt ../predictions/large/f_pred_5.txt\n",
    "./measure_scores.py ../predictions/f_dev_2.txt ../predictions/large2/f_pred_2.txt\n",
    "\n",
    "### Large\n",
    "#### my_output_1\n",
    "BLEU: 0.6826\n",
    "NIST: 8.4243\n",
    "METEOR: 0.4557\n",
    "ROUGE_L: 0.7032\n",
    "CIDEr: 2.1234\n",
    "\n",
    "#### my_output_2\n",
    "BLEU: 0.7228\n",
    "NIST: 8.5241\n",
    "METEOR: 0.4851\n",
    "ROUGE_L: 0.7461\n",
    "CIDEr: 2.4645\n",
    "\n",
    "#### my_output_3\n",
    "BLEU: 0.7035\n",
    "NIST: 8.5937\n",
    "METEOR: 0.4700\n",
    "ROUGE_L: 0.7252\n",
    "CIDEr: 2.3310\n",
    "\n",
    "#### my_output_4\n",
    "BLEU: 0.6738\n",
    "NIST: 8.4018\n",
    "METEOR: 0.4576\n",
    "ROUGE_L: 0.7075\n",
    "CIDEr: 2.2172\n",
    "\n",
    "#### my_output_5\n",
    "BLEU: 0.6927\n",
    "NIST: 8.4429\n",
    "METEOR: 0.4662\n",
    "ROUGE_L: 0.7180\n",
    "CIDEr: 2.2729\n",
    "\n",
    "### base\n",
    "#### my_output_2\n",
    "BLEU: 0.6812\n",
    "NIST: 8.5491\n",
    "METEOR: 0.4442\n",
    "ROUGE_L: 0.7036\n",
    "CIDEr: 2.1261\n",
    "\n",
    "#### my_output_6\n",
    "BLEU: 0.6655\n",
    "NIST: 8.4830\n",
    "METEOR: 0.4475\n",
    "ROUGE_L: 0.6992\n",
    "CIDEr: 2.1077\n",
    "    \n",
    "#### my_output_final\n",
    "BLEU: 0.6529\n",
    "NIST: 8.3116\n",
    "METEOR: 0.4430\n",
    "ROUGE_L: 0.6842\n",
    "CIDEr: 1.9766\n",
    "\n",
    "### Pragmatically Informative Text Generation\n",
    "BLEU 68.60\n",
    "NIST 8.73\n",
    "METEOR 45.25\n",
    "R-L 70.82\n",
    "CIDEr 2.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 110] Connection\n",
      "[nltk_data]     timed out>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4693it [00:05, 794.55it/s]\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c84b575289e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#             f_dev.write(ref_sentences[k][m]+'\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mori_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;31m#         if k < len(ref_sentences)-1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#             f_dev.write('\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# from model_large import *\n",
    "from model import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "max_len = 70\n",
    "\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "\n",
    "for i in range(1, 9):\n",
    "    model_name = './gen_model/random_init/try_1/'+str(i)+'/model'\n",
    "    my_model.load_state_dict(torch.load(model_name))\n",
    "    print('ok') \n",
    "    if i == 1:\n",
    "#         e2e_dataset = e2eDataset(csv_file='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "        e2e_dataset = e2eDataset(csv_file='dataset/testset_w_refs.csv', tokenizer=my_model.tokenizer)\n",
    "        dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        same_condition = []\n",
    "        ref_sentences = []\n",
    "        input_ids_list = []\n",
    "        pre_condition_string = ''\n",
    "        start = 0\n",
    "        for i_batch, sample_batched in tqdm(enumerate(dataloader)):\n",
    "            sen = sample_batched[1][0]\n",
    "            condition_string = sample_batched[2]  \n",
    "            input_ids = sample_batched[0].squeeze(0).cuda()\n",
    "\n",
    "            if start == 0 or condition_string == pre_condition_string:      \n",
    "                if start == 0:\n",
    "                    input_ids_list.append(input_ids)\n",
    "                same_condition.append(sen)        \n",
    "                pre_condition_string = condition_string\n",
    "                start += 1\n",
    "            else:   \n",
    "                input_ids_list.append(input_ids)\n",
    "                ref_sentences.append(same_condition)\n",
    "                pre_condition_string = condition_string\n",
    "                same_condition = [sen]\n",
    "                start += 1\n",
    "        ref_sentences.append(same_condition)    \n",
    "\n",
    "    bleu_score = 0\n",
    "    bleu_1 = 0\n",
    "\n",
    "#     f_dev = open('./predictions/testset/large2/f_dev_'+str(i)+'.txt', 'w')\n",
    "#     f_pred = open('./predictions/devset/base4/f_pred_'+str(i)+'.txt', 'w')\n",
    "#     f_pred = open('./predictions/joosung2/testset/base2_sample30/f_pred_'+str(i)+'.txt', 'w')\n",
    "    f_pred = open('./predictions/fix/try_1/f_pred_'+str(i)+'.txt', 'w')\n",
    "\n",
    "    for k in range(len(ref_sentences)):\n",
    "        input_ids = input_ids_list[k]\n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        ori_tokens = []\n",
    "        for m in range(len(ref_sentences[k])):\n",
    "#             f_dev.write(ref_sentences[k][m]+'\\n')\n",
    "            ori_tokens.append(word_tokenize(ref_sentences[k][m]))\n",
    "#         if k < len(ref_sentences)-1:\n",
    "#             f_dev.write('\\n')\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "            pred_idx = model_out.argmax(1)[-1]        \n",
    "            if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "                break            \n",
    "            input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "\n",
    "        out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "        f_pred.write(out_sen+'\\n')\n",
    "\n",
    "        out_tokens = word_tokenize(out_sen)\n",
    "\n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "        bleu_1 += bleu_1_score\n",
    "\n",
    "        bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))    \n",
    "\n",
    "#     f_dev.close()\n",
    "    f_pred.close()\n",
    "    \n",
    "    print(i, \"th model\")\n",
    "    print(\"BLEU score: {}\".format(bleu_score/len(ref_sentences)*100))\n",
    "    print(\"BLEU1 score: {}\".format(bleu_1/len(ref_sentences)*100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 110] Connection\n",
      "[nltk_data]     timed out>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0512 04:16:40.331747 139771900184384 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0512 04:16:40.332697 139771900184384 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0512 04:16:41.886805 139771900184384 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0512 04:16:41.887598 139771900184384 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0512 04:16:42.732310 139771900184384 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0512 04:16:42.733266 139771900184384 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0512 04:16:43.518005 139771900184384 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    }
   ],
   "source": [
    "### BERT score with human reference\n",
    "from bert_score import BERTScorer\n",
    "scorer = BERTScorer(lang=\"en\",  rescale_with_baseline=True)\n",
    "\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_files = \"/project/work/E2E/predictions/joosung2/testset/f_dev.txt\"\n",
    "\n",
    "human_open = open(human_files, \"r\")\n",
    "human_dataset = human_open.readlines()\n",
    "human_open.close()\n",
    "\n",
    "human_references = []\n",
    "\n",
    "temp_reference = []\n",
    "for i in range(len(human_dataset)):\n",
    "    if human_dataset[i] == '\\n':\n",
    "        human_references.append(temp_reference)\n",
    "        temp_reference = []\n",
    "    else:\n",
    "        temp_reference.append(human_dataset[i].strip())\n",
    "human_references.append(temp_reference)\n",
    "human_compare = []\n",
    "for i in range(len(human_references)):\n",
    "    for k in range(len(human_references[i])):\n",
    "        human_compare.append(human_references[i][k])\n",
    "\n",
    "# output_path = \"/project/work/E2E/predictions/joosung2/testset/base1_sample30/*\"\n",
    "# output_path = \"/project/work/E2E/predictions/final/*\"\n",
    "output_path = \"/data/private/E2E/predictions/fix/try_1/*\"\n",
    "pred_files = glob.glob(output_path)\n",
    "# pred_files = [\"/project/work/E2E/predictions/final/sampling30_1.txt\"]\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(pred_files)):    \n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        repeat_num = len(human_references[k])\n",
    "        for _ in range(repeat_num):\n",
    "            cands.append(out_sen)\n",
    "\n",
    "#     P, R, F1 = score(cands, human_compare, lang='en', verbose=True)\n",
    "    P, R, F1 = scorer.score(cands, human_compare)\n",
    "    \n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    \n",
    "    score_list.append(BERT_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/project/work/E2E/predictions/final/final_model.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling10_3.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling30_1.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling10_1.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling50.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling30_3.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling30_2.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling10_2.txt',\n",
       "  '/project/work/E2E/predictions/final/sampling30_4.txt'],\n",
       " [0.656454939772243,\n",
       "  0.6424815282225609,\n",
       "  0.6472669815854581,\n",
       "  0.6359314698779123,\n",
       "  0.6566227318037554,\n",
       "  0.6406740825420856,\n",
       "  0.650434247401638,\n",
       "  0.6391869411237606,\n",
       "  0.6459375914323566])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_files, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rescaling\n",
    "(['/project/work/E2E/predictions/final/final_model.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling10_3.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling30_1.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling10_1.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling50.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling30_3.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling30_2.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling10_2.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling30_4.txt'],\n",
    " [0.656454939772243,\n",
    "  0.6424815282225609,\n",
    "  0.6472669815854581,\n",
    "  0.6359314698779123,\n",
    "  0.6566227318037554,\n",
    "  0.6406740825420856,\n",
    "  0.650434247401638,\n",
    "  0.6391869411237606,\n",
    "  0.6459375914323566])\n",
    "## original\n",
    "(['/project/work/E2E/predictions/final/final_model.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling10_3.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling30_1.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling10_1.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling50.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling30_3.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling30_2.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling10_2.txt',\n",
    "  '/project/work/E2E/predictions/final/sampling30_4.txt'],\n",
    " [0.9420184408096827,\n",
    "  0.939660089205964,\n",
    "  0.9404677509270877,\n",
    "  0.9385546076889615,\n",
    "  0.9420467598235559,\n",
    "  0.9393550385271541,\n",
    "  0.9410023027687674,\n",
    "  0.9391040466193982,\n",
    "  0.9402433829776976])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0512 04:22:07.024106 139771900184384 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0512 04:22:07.025104 139771900184384 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0512 04:22:08.642861 139771900184384 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0512 04:22:08.643667 139771900184384 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0512 04:22:09.496846 139771900184384 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0512 04:22:09.497787 139771900184384 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0512 04:22:10.284366 139771900184384 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/project/work/E2E/compared_system/system_outputs/primary_txt/adapt.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/forge3.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/tgen.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/chen.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/sheff2.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/tuda.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/forge1.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/dangnt.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw2.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/zhang.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/tr2.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/tnt2.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/slug.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/gong.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw1.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/tr1.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/tnt1.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/nle.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/sheff1.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/slug-alt.txt', '/project/work/E2E/compared_system/system_outputs/primary_txt/harv.txt'] [0.5406375744826766, 0.5717267391700341, 0.6395320737988055, 0.4793327121306572, 0.6073363663566385, 0.6377492239915077, 0.582961198421155, 0.6391421972136748, 0.6023306257891015, 0.5927444820274632, 0.5708038733409662, 0.5867984226116767, 0.6591073991649445, 0.6464014315821711, 0.5987095591319844, 0.49099425277734227, 0.5873226750234843, 0.6418374851457147, 0.6339793985452754, 0.6123424481791828, 0.6387160209458239]\n"
     ]
    }
   ],
   "source": [
    "### Compared System / BERT score with human reference\n",
    "import csv\n",
    "from bert_score import BERTScorer\n",
    "scorer = BERTScorer(lang=\"en\",  rescale_with_baseline=True)\n",
    "\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_files = \"/project/work/E2E/predictions/joosung2/testset/f_dev.txt\"\n",
    "\n",
    "human_open = open(human_files, \"r\")\n",
    "human_dataset = human_open.readlines()\n",
    "human_open.close()\n",
    "\n",
    "human_references = []\n",
    "\n",
    "temp_reference = []\n",
    "for i in range(len(human_dataset)):\n",
    "    if human_dataset[i] == '\\n':\n",
    "        human_references.append(temp_reference)\n",
    "        temp_reference = []\n",
    "    else:\n",
    "        temp_reference.append(human_dataset[i].strip())\n",
    "human_references.append(temp_reference)\n",
    "human_compare = []\n",
    "for i in range(len(human_references)):\n",
    "    for k in range(len(human_references[i])):\n",
    "        human_compare.append(human_references[i][k])\n",
    "\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/primary_txt/*\"\n",
    "pred_files = glob.glob(output_path)\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(pred_files)):    \n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        repeat_num = len(human_references[k])\n",
    "        for _ in range(repeat_num):\n",
    "            cands.append(out_sen)\n",
    "\n",
    "#     P, R, F1 = score(cands, human_compare, lang='en', verbose=True)\n",
    "    P, R, F1 = scorer.score(cands, human_compare)\n",
    "    \n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    \n",
    "    score_list.append(BERT_score)  \n",
    "print(pred_files, score_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/work/E2E/compared_system/system_outputs/primary_txt/adapt.txt 0.5406375744826766\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/forge3.txt 0.5717267391700341\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tgen.txt 0.6395320737988055\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/chen.txt 0.4793327121306572\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/sheff2.txt 0.6073363663566385\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tuda.txt 0.6377492239915077\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/forge1.txt 0.582961198421155\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/dangnt.txt 0.6391421972136748\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw2.txt 0.6023306257891015\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/zhang.txt 0.5927444820274632\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tr2.txt 0.5708038733409662\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tnt2.txt 0.5867984226116767\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/slug.txt 0.6591073991649445\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/gong.txt 0.6464014315821711\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw1.txt 0.5987095591319844\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tr1.txt 0.49099425277734227\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tnt1.txt 0.5873226750234843\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/nle.txt 0.6418374851457147\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/sheff1.txt 0.6339793985452754\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/slug-alt.txt 0.6123424481791828\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/harv.txt 0.6387160209458239\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred_files)):\n",
    "    print(pred_files[i], score_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rescaling\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/adapt.txt 0.5406375744826766\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/forge3.txt 0.5717267391700341\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tgen.txt 0.6395320737988055\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/chen.txt 0.4793327121306572\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/sheff2.txt 0.6073363663566385\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tuda.txt 0.6377492239915077\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/forge1.txt 0.582961198421155\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/dangnt.txt 0.6391421972136748\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw2.txt 0.6023306257891015\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/zhang.txt 0.5927444820274632\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tr2.txt 0.5708038733409662\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tnt2.txt 0.5867984226116767\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/slug.txt 0.6591073991649445\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/gong.txt 0.6464014315821711\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw1.txt 0.5987095591319844\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tr1.txt 0.49099425277734227\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tnt1.txt 0.5873226750234843\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/nle.txt 0.6418374851457147\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/sheff1.txt 0.6339793985452754\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/slug-alt.txt 0.6123424481791828\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/harv.txt 0.6387160209458239\n",
    "\n",
    "## original\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/adapt.txt 0.9224714524527996\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/forge3.txt 0.927718502866061\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tgen.txt 0.9391622970715378\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/chen.txt 0.9121247704618907\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/sheff2.txt 0.9337284908654363\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tuda.txt 0.9388613980156308\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/forge1.txt 0.9296145902852526\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/dangnt.txt 0.9390964960076523\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw2.txt 0.9328836507693603\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/zhang.txt 0.9312657565164231\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tr2.txt 0.9275627467974198\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tnt2.txt 0.9302622148433322\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/slug.txt 0.9424661079272009\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/gong.txt 0.9403216677664085\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw1.txt 0.9322725079498031\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tr1.txt 0.9140929382111049\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/tnt1.txt 0.9303506951379847\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/nle.txt 0.939551391239337\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/sheff1.txt 0.9382251485080423\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/slug-alt.txt 0.9345733885848393\n",
    "/project/work/E2E/compared_system/system_outputs/primary_txt/harv.txt 0.9390245683915118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/slug.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 547362.73 tokens per second.\n",
      "PTBTokenizer tokenized 17228 tokens at 180441.17 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.445\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.677\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.262\n",
      "Creating temp directory  /tmp/e2e-eval-wbdld4sm\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 07:45:40\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-wbdld4sm/mteval_ref.sgm -s /tmp/e2e-eval-wbdld4sm/mteval_src.sgm -t /tmp/e2e-eval-wbdld4sm/mteval_sys.sgm -f /tmp/e2e-eval-wbdld4sm/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.6130  BLEU score = 0.6619 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.6389   1.4164   0.7348   0.4644   0.3587   0.1825   0.1016   0.0638   0.0434  \"tst\"\n",
      "\n",
      " BLEU:  0.9347   0.7661   0.5983   0.4527   0.3233   0.2350   0.1717   0.1239   0.0852  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.6389   7.0552   7.7900   8.2544   8.6130   8.7955   8.8971   8.9609   9.0043  \"tst\"\n",
      "\n",
      " BLEU:  0.9323   0.8440   0.7519   0.6619   0.5732   0.4938   0.4245   0.3638   0.3095  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 07:45:53\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6619\n",
      "NIST: 8.6130\n",
      "METEOR: 0.4454\n",
      "ROUGE_L: 0.6772\n",
      "CIDEr: 2.2615\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/slug-alt.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 545173.68 tokens per second.\n",
      "PTBTokenizer tokenized 17664 tokens at 160850.99 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.437\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.599\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.102\n",
      "Creating temp directory  /tmp/e2e-eval-u73c6a3v\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 07:46:10\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-u73c6a3v/mteval_ref.sgm -s /tmp/e2e-eval-u73c6a3v/mteval_src.sgm -t /tmp/e2e-eval-u73c6a3v/mteval_sys.sgm -f /tmp/e2e-eval-u73c6a3v/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.3954  BLEU score = 0.6035 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.6013   1.3976   0.7122   0.3993   0.2850   0.1340   0.0697   0.0338   0.0194  \"tst\"\n",
      "\n",
      " BLEU:  0.9200   0.7146   0.5312   0.3814   0.2511   0.1666   0.1117   0.0711   0.0393  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.6013   6.9989   7.7111   8.1103   8.3954   8.5293   8.5990   8.6328   8.6522  \"tst\"\n",
      "\n",
      " BLEU:  0.9191   0.8100   0.7035   0.6035   0.5063   0.4206   0.3480   0.2853   0.2289  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 07:46:23\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6035\n",
      "NIST: 8.3954\n",
      "METEOR: 0.4369\n",
      "ROUGE_L: 0.5991\n",
      "CIDEr: 2.1019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_1.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_2.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_3.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_4.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_5.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_6.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_7.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_8.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_9.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base2_sample30/f_pred_10.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base1_sample10_6.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base2_sample10_9.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base4_devtest_4.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/2base1_sample50_7.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/slug.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/slug-alt.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tsv to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/primary/*\"\n",
    "comapred_files = glob.glob(output_path)\n",
    "\n",
    "for i in range(len(comapred_files)):\n",
    "    dataset = pd.read_csv(comapred_files[i], delimiter='\\t', header=None)\n",
    "    \n",
    "    name = comapred_files[i].split('/')[-1].split('.')[0]\n",
    "    txt_files = \"/project/work/E2E/compared_system/system_outputs/primary_txt/\"+name+\".txt\"\n",
    "    f = open(txt_files, \"w\")\n",
    "    gen_sentences = dataset[1]\n",
    "    \n",
    "    for k in range(1, len(gen_sentences)):\n",
    "        f.write(gen_sentences[k]+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gen_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero-shot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0507 00:59:26.720654 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0507 00:59:26.721606 140215530821440 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0507 00:59:26.798727 140215530821440 tokenization_utils.py:663] Adding <START> to the vocabulary\n",
      "I0507 00:59:26.799590 140215530821440 tokenization_utils.py:741] Assigning <START> to the bos_token key of the tokenizer\n",
      "I0507 00:59:26.800053 140215530821440 tokenization_utils.py:663] Adding <name> to the vocabulary\n",
      "I0507 00:59:26.800630 140215530821440 tokenization_utils.py:663] Adding <eatType> to the vocabulary\n",
      "I0507 00:59:26.801137 140215530821440 tokenization_utils.py:663] Adding <priceRange> to the vocabulary\n",
      "I0507 00:59:26.801638 140215530821440 tokenization_utils.py:663] Adding <customer rating> to the vocabulary\n",
      "I0507 00:59:26.802139 140215530821440 tokenization_utils.py:663] Adding <near> to the vocabulary\n",
      "I0507 00:59:26.802641 140215530821440 tokenization_utils.py:663] Adding <food> to the vocabulary\n",
      "I0507 00:59:26.803125 140215530821440 tokenization_utils.py:663] Adding <area> to the vocabulary\n",
      "I0507 00:59:26.803602 140215530821440 tokenization_utils.py:663] Adding <familyFriendly> to the vocabulary\n",
      "I0507 00:59:26.804103 140215530821440 tokenization_utils.py:741] Assigning ['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>'] to the additional_special_tokens key of the tokenizer\n",
      "I0507 00:59:27.610918 140215530821440 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/ds_user1/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5\n",
      "I0507 00:59:27.611916 140215530821440 configuration_utils.py:311] Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0507 00:59:28.384023 140215530821440 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import *\n",
    "\n",
    "max_len = 70\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "model_name = './gen_model/base_devtrain_4/4/model'\n",
    "my_model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<name>', '<eatType>', '<food>', '<customer rating>', '<area>', '<familyFriendly>', '<near>']\n",
      "['The Waterman', 'pub', 'Italian', 'high', 'city centre', 'yes', 'All Bar One']\n",
      "The Waterman is a highly rated, family-friendly pub in the city centre near All Bar One.\n",
      "<NAME> is a <CUSTOMER_RATING>ly rated, family-friendly <EATTYPE> in the <area> near <NEAR>.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "csv_file='dataset/testset_w_refs.csv'\n",
    "dataset = pd.read_csv(csv_file)\n",
    "columns = dataset.columns\n",
    "conditions = dataset[columns[0]]\n",
    "        \n",
    "typ_list = {}\n",
    "for k in range(len(conditions)):\n",
    "    cond_set = conditions[k].split(',')\n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        if cond_set[m][:pos] in typ_list.keys():\n",
    "            typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "        else:            \n",
    "            typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}   \n",
    "\n",
    "def sample_batch(tokenizer, cond_name, cond_set):\n",
    "    condition_string = ''\n",
    "    for m in range(len(cond_set)):\n",
    "        condition_string += cond_name[m] + cond_set[m] + ' '\n",
    "\n",
    "    input_string = condition_string + '<START>'\n",
    "    input_ids = torch.tensor(tokenizer.encode(input_string, add_special_tokens=True))\n",
    "\n",
    "    input_len = len(input_ids)\n",
    "\n",
    "    return input_ids, condition_string\n",
    "\n",
    "# <name> <eatType> <food> <priceRange> <customer rating> <area> <familyFriendly> <near>\n",
    "\n",
    "name = '<NAME>' # None\n",
    "eatType = '<EATTYPE>' # None\n",
    "food = '<FOOD>' # None \n",
    "priceRange = None # '<priceRange>'\n",
    "customer_rating = '<CUSTOMER_RATING>' # None \n",
    "area = '<area>' # None\n",
    "familyFriendly = 'yes' # None\n",
    "near = '<NEAR>' # None\n",
    "\n",
    "cond_list = []\n",
    "conditions = []\n",
    "if name is not None:\n",
    "    placeholder_name = random.choice(list(typ_list['name']))\n",
    "    cond_list.append('<name>')\n",
    "    conditions.append(placeholder_name)\n",
    "if eatType is not None:\n",
    "    placeholder_eatType = random.choice(list(typ_list['eatType']))\n",
    "    cond_list.append('<eatType>')\n",
    "    conditions.append(placeholder_eatType)\n",
    "if food is not None:\n",
    "    placeholder_food = random.choice(list(typ_list['food']))\n",
    "    cond_list.append('<food>')\n",
    "    conditions.append(placeholder_food)\n",
    "if priceRange is not None:\n",
    "    placeholder_priceRange = random.choice(list(typ_list['priceRange']))\n",
    "    cond_list.append('<priceRange>')\n",
    "    conditions.append(placeholder_priceRange)    \n",
    "if customer_rating is not None:\n",
    "    placeholder_customer_rating = random.choice(list(typ_list['customer rating']))\n",
    "    cond_list.append('<customer rating>')\n",
    "    conditions.append(placeholder_customer_rating)        \n",
    "if area is not None:\n",
    "    placeholder_area = random.choice(list(typ_list['area']))\n",
    "    cond_list.append('<area>')\n",
    "    conditions.append(placeholder_area)    \n",
    "if familyFriendly is not None:\n",
    "    cond_list.append('<familyFriendly>')\n",
    "    conditions.append(familyFriendly)            \n",
    "if near is not None:\n",
    "    placeholder_near = random.choice(list(typ_list['near']))\n",
    "    cond_list.append('<near>')\n",
    "    conditions.append(placeholder_near)        \n",
    "\n",
    "\n",
    "# del cond_name[2]\n",
    "# del conditions[2]\n",
    "\n",
    "sample = sample_batch(my_model.tokenizer, cond_list, conditions)\n",
    "\n",
    "input_ids = sample[0].cuda()\n",
    "condition_string = sample[1]  \n",
    "input_len = len(input_ids)\n",
    "\n",
    "max_len = 70\n",
    "for _ in range(max_len):\n",
    "    model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "    pred_idx = model_out.argmax(1)[-1]        \n",
    "    if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "        break            \n",
    "    input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "\n",
    "out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "print(cond_list)\n",
    "print(conditions)\n",
    "print(out_sen)\n",
    "\n",
    "if name is not None:\n",
    "    out_sen = out_sen.replace(placeholder_name, name)\n",
    "if eatType is not None:\n",
    "    out_sen = out_sen.replace(placeholder_eatType, eatType)\n",
    "if food is not None:\n",
    "    out_sen = out_sen.replace(placeholder_food, food)\n",
    "if priceRange is not None:\n",
    "    out_sen = out_sen.replace(placeholder_priceRange, priceRange)\n",
    "if customer_rating is not None:\n",
    "    out_sen = out_sen.replace(placeholder_customer_rating, customer_rating)\n",
    "if area is not None:\n",
    "    out_sen = out_sen.replace(placeholder_area, area)  \n",
    "if near is not None:\n",
    "    out_sen = out_sen.replace(placeholder_near, near)\n",
    "print(out_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delexicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class e2eDataset(Dataset):\n",
    "    def __init__(self, csv_file1, csv_file2, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "        \"\"\"\n",
    "        self.dataset1 = pd.read_csv(csv_file1)\n",
    "        self.dataset2 = pd.read_csv(csv_file2)\n",
    "        \n",
    "        self.columns1 = self.dataset1.columns\n",
    "        self.columns2 = self.dataset2.columns\n",
    "        \n",
    "        self.conditions = list(self.dataset1[self.columns1[0]]) + list(self.dataset2[self.columns2[0]])\n",
    "        self.sentences = list(self.dataset1[self.columns1[1]]) + list(self.dataset2[self.columns2[1]])\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.typ_list = {}\n",
    "        for k in range(len(self.conditions)):\n",
    "            cond_set = self.conditions[k].split(',')\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] in self.typ_list.keys():\n",
    "                    self.typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "                else:            \n",
    "                    self.typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conditions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sen = self.sentences[idx]\n",
    "        \n",
    "        cond = self.conditions[idx]\n",
    "        cond_set = cond.split(',')        \n",
    "        condition_string = ''\n",
    "        \n",
    "        \n",
    "        p = random.random()\n",
    "\n",
    "        if p > 0.3: # 70%\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "\n",
    "                condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        else: # p <= 0.3 / 30%\n",
    "            nochange_list = ['priceRange', 'customer rating', 'familyFriendly']\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                \n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] not in nochange_list:\n",
    "                    placeholder = '<' + cond_set[m][:pos] + '>'\n",
    "                    condition_string += placeholder + ' '\n",
    "                    sen = sen.replace(cond_set[m][pos+1:-1], placeholder)        \n",
    "                else:\n",
    "                    condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        \n",
    "        input_string = condition_string + '<START>' + sen\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(input_string, add_special_tokens=True))\n",
    "        \n",
    "        label_string = sen + ' <|endoftext|>'\n",
    "        label_ids = torch.tensor(self.tokenizer.encode(label_string, add_special_tokens=True))\n",
    "\n",
    "        return input_string, input_ids, label_ids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_dataset = e2eDataset(csv_file1='dataset/trainset.csv', csv_file2='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "print(e2e_dataset[200][0])\n",
    "# print(e2e_dataset.typ_list.keys())\n",
    "# print(e2e_dataset.typ_list)\n",
    "# priceRange, customer rating, familyFriendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dev = open('f_dev.txt', 'r')\n",
    "f_pred = open('f_pred.txt', 'r')\n",
    "f_dev_dataset = f_dev.readlines()\n",
    "f_pred_dataset = f_pred.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f_dev_dataset), len(f_pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[x for x in f_dev_dataset if x != '\\n']\n",
    "data2=[x for x in f_pred_dataset if x != '\\n']\n",
    "len(data), len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4672+546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dev = open('./e2e-metrics/example-inputs/devel-conc.txt', 'r')\n",
    "f_pred = open('./e2e-metrics/example-inputs/baseline-output.txt', 'r')\n",
    "f_dev_dataset = f_dev.readlines()\n",
    "f_pred_dataset = f_pred.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f_dev_dataset), len(f_pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[x for x in f_dev_dataset if x != '\\n']\n",
    "data2=[x for x in f_pred_dataset if x != '\\n']\n",
    "len(data), len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "p=random.random()\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouput 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "csv_file=\"/project/work/E2E/dataset/testset_w_refs.csv\"\n",
    "dataset = pd.read_csv(csv_file)\n",
    "columns = dataset.columns\n",
    "conditions = dataset[columns[0]]\n",
    "refs = dataset[columns[1]]\n",
    "\n",
    "ref_conditions = []\n",
    "ref_sentences = []\n",
    "pre_condition = ''\n",
    "start = 0\n",
    "for k in range(len(refs)):\n",
    "    sen = refs[k]    \n",
    "        \n",
    "    if start == 0 or conditions[k] == pre_condition:      \n",
    "        if start == 0:\n",
    "            ref_sentences.append(sen)\n",
    "            ref_conditions.append(conditions[k])\n",
    "        pre_condition = conditions[k]\n",
    "        start += 1\n",
    "    else:\n",
    "        ref_sentences.append(sen)\n",
    "        ref_conditions.append(conditions[k])\n",
    "        pre_condition = conditions[k]\n",
    "        start += 1        \n",
    "\n",
    "\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/compared/\"\n",
    "pred_files = [output_path+'our_model.txt',output_path+'slug.txt',output_path+'tgen.txt', output_path+'tuda.txt']\n",
    "\n",
    "sentence_list = []\n",
    "file_name = ['our_model', 'slug', 'tgen', 'tuda', 'references']\n",
    "for i in range(len(pred_files)):\n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    sentence_list.append(cands)  \n",
    "sentence_list.append(ref_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(output_path + 'total.txt', 'w')\n",
    "\n",
    "print(file_name)\n",
    "\n",
    "for ind in range(630):\n",
    "#     if sentence_list[0][ind] != sentence_list[-1][ind]:\n",
    "    f.write(str(ind) + '\\t' + ref_conditions[ind] + '\\n')\n",
    "    print(ind, ref_conditions[ind])\n",
    "    for k in range(len(file_name)):\n",
    "        print(sentence_list[k][ind])\n",
    "        f.write(sentence_list[k][ind] + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zizzi is a pub near The Sorrento. Close to The Sorrento you will be able to find a pub called Zizzi\n"
     ]
    }
   ],
   "source": [
    "if sentence_list[0][ind] != sentence_list[-1][ind]:\n",
    "    print(sentence_list[0][ind], sentence_list[-1][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including Slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our_model', 'slug', 'tgen', 'tuda', 'references']\n",
      "[0.9512169312169305, 0.9839606953892663, 0.855699168556305, 0.964172335600906, 0.864580498866211]\n"
     ]
    }
   ],
   "source": [
    "print(file_name)\n",
    "\n",
    "include_percent = [0 for _ in range(len(file_name))]\n",
    "for ind in range(630):\n",
    "    cond_dict = {}    \n",
    "    cond_set = ref_conditions[ind].split(',')\n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        cond_dict[cond_set[m][:pos]] = (cond_set[m][pos+1:-1])    \n",
    "    \n",
    "    for m in range(len(file_name)):\n",
    "        num = len(cond_dict)\n",
    "        sentence_temp = sentence_list[m][ind]\n",
    "        count_temp  = 0\n",
    "        \n",
    "        for k, v in cond_dict.items():\n",
    "            if k != 'familyFriendly':\n",
    "                if v in sentence_temp:\n",
    "                    count_temp += 1\n",
    "            else:\n",
    "                num -= 1\n",
    "                \n",
    "        include_percent[m] += count_temp/num\n",
    "                    \n",
    "include_final = [x/630 for x in include_percent]\n",
    "print(include_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our_model', 'slug', 'tgen', 'tuda', 'references']\n",
      "[0.9668253968253964, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.10952380952380952, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"/project/work/E2E/predictions/incomplete.txt\", 'w')\n",
    "print(file_name)\n",
    "\n",
    "include_percent = [0 for _ in range(len(file_name))]\n",
    "incomplete = [0 for _ in range(len(file_name))]\n",
    "for ind in range(630):\n",
    "    cond_dict = {}\n",
    "    cond_set = ref_conditions[ind].split(',')\n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        cond_dict[cond_set[m][:pos]] = (cond_set[m][pos+1:-1])    \n",
    "    \n",
    "    for m in range(1): # len(file_name)\n",
    "        num = len(cond_dict)\n",
    "        sentence_temp = sentence_list[m][ind]\n",
    "        count_temp  = 0\n",
    "        incomplte_temp = 0\n",
    "        \n",
    "        for k, v in cond_dict.items():\n",
    "            if k != 'familyFriendly':\n",
    "                if v.lower() in sentence_temp.strip().lower():\n",
    "                    count_temp += 1\n",
    "#                 else:\n",
    "#                     print(v, '##', sentence_temp)\n",
    "            else:\n",
    "                num -= 1\n",
    "        include_percent[m] += count_temp/num\n",
    "        \n",
    "        for k, v in cond_dict.items():\n",
    "            if k != 'familyFriendly':\n",
    "                if v.lower() not in sentence_temp.strip().lower():\n",
    "                    incomplete[m] += 1\n",
    "                    f.write(str(cond_dict))\n",
    "                    f.write('\\n' + sentence_temp + '\\n\\n')\n",
    "                    break\n",
    "        \n",
    "f.close()\n",
    "include_final = [x/630 for x in include_percent]\n",
    "incomplete_final = [x/630 for x in incomplete]\n",
    "print(include_final)\n",
    "print(incomplete_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8741976398486118\n",
      "0.4528020455998295\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "csv_file=\"/project/work/E2E/dataset/testset_w_refs.csv\"\n",
    "# csv_file=\"/project/work/E2E/dataset/trainset.csv\"\n",
    "dataset = pd.read_csv(csv_file)\n",
    "columns = dataset.columns\n",
    "conditions = dataset[columns[0]]\n",
    "refs = dataset[columns[1]]\n",
    "\n",
    "include_percent = 0\n",
    "incomplte_num = 0\n",
    "for ind in range(len(conditions)):\n",
    "    cond_dict = {}\n",
    "    cond_set = conditions[ind].split(',')\n",
    "    \n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        cond_dict[cond_set[m][:pos]] = (cond_set[m][pos+1:-1])    \n",
    "        \n",
    "    num = len(cond_dict)\n",
    "    sentence_temp = refs[ind]\n",
    "    count_temp  = 0\n",
    "\n",
    "    for k, v in cond_dict.items():\n",
    "        if k != 'familyFriendly':\n",
    "            if v.lower() in sentence_temp.strip().lower():\n",
    "                count_temp += 1\n",
    "        else:\n",
    "            num -= 1\n",
    "            \n",
    "    for k, v in cond_dict.items():\n",
    "        if k != 'familyFriendly':\n",
    "            if v.lower() not in sentence_temp.strip().lower():\n",
    "                incomplte_num += 1\n",
    "                break\n",
    "\n",
    "    include_percent += count_temp/num  \n",
    "print(include_percent/len(conditions))\n",
    "print(incomplte_num/len(conditions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4693"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include percent / train: 0.8326792945845742, test: 0.8741976398486118\n",
    "incomplte_num / train: 0.474525094505599, test: 0.4528020455998295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34412"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incomplte_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"dict_keys(['name', 'eatType', 'food', 'area', 'familyFriendly', 'near'])\",\n",
       " ['Blue Spice',\n",
       "  'restaurant',\n",
       "  'English',\n",
       "  'riverside',\n",
       "  'yes',\n",
       "  'Rainbow Vegetarian Café'],\n",
       " \"{'name': 'Blue Spice', 'eatType': 'restaurant', 'food': 'English', 'area': 'riverside', 'familyFriendly': 'yes', 'near': 'Rainbow Vegetarian Café'}\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(cond_dict.keys()), list(cond_dict.values()), str(cond_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0512 04:08:15.601484 139771900184384 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0512 04:08:15.602493 139771900184384 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0512 04:08:17.169445 139771900184384 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0512 04:08:17.170119 139771900184384 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0512 04:08:18.017142 139771900184384 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/ds_user1/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "I0512 04:08:18.018072 139771900184384 configuration_utils.py:311] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0512 04:08:18.794093 139771900184384 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from bert_score import BERTScorer\n",
    "scorer = BERTScorer(lang=\"en\",  model_type=\"roberta-large\", rescale_with_baseline=True)\n",
    "# from bert_score import score\n",
    "# P, R, F1 = scorer.score(cate, sen, lang='en', verbose=True, rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate = ['smartphone', 'TV', 'smart watch'] + ['smartphone', 'TV', 'smart watch'] + ['smartphone', 'TV', 'smart watch']\n",
    "sen = ['It shows in fig. 4. And the personal digital assistant of the present invention comprises all personal digital assistants supporting the function of the present invention, the multimedia device and the application about that. For example, the personal digital assistant comprises the mobile communications terminal, operating based on each communications protocol (communication protocols) corresponding to the various communications system the tablet PC (Personal Computer), the Smart phone, the digital camera, the PMP (Portable Multimedia Player), the media player, the carrying gaming terminal, and the device including the PDA (Personal Digital Assistant) etc.' for _ in range(3)]+\\\n",
    "['The invention relates to the video display and image display method, the power supply device and method for supplying power, and the contents method of controlling luminance, and more particularly to the video display and the image display method, converting into the DC voltage having the voltage level performing the feed-forward control based on the video signal provided to the OLED panel about the drive power supplied to the OLED panel for example and it reduces the generation of heat by the voltage deviation in the red, the green, and the drive of the emitting devices of the blue by using the same power supply voltage (VDD) and it confirms R of image frame data, G, and the B-value and produces the peak current value and is the DC voltage corresponded to the peak current value and along with this can supply the power source of the different size according to color or the multiple pixel groups of OLED included in each pixel and provide multiple contentses through one screen. The power supply device and method for supplying power, and the contents method of controlling luminance.' for _ in range(3)] +\\\n",
    "['the invention relates to the atch type portable radio telephone (Watch Type Radiotelephone) can wear in especially, the wrist as the invention relating to the portable rfterminall equipment.' for _ in range(3)]\n",
    "cate\n",
    "\n",
    "from bert_score import score\n",
    "P, R, F1 = score(cate, sen, lang='others', verbose=True)\n",
    "# P, R, F1 = scorer.score(cate, sen)\n",
    "P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(0) tensor(0)\n",
      "tensor(0) tensor(2) tensor(0)\n",
      "tensor(0) tensor(0) tensor(0)\n",
      "tensor(0) tensor(0) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(P[0:3].argmax(0), P[3:6].argmax(0), P[6:9].argmax(0))\n",
    "print(R[0:3].argmax(0), R[3:6].argmax(0), R[6:9].argmax(0))\n",
    "print(F1[0:3].argmax(0), F1[3:6].argmax(0), F1[6:9].argmax(0))\n",
    "X = 2*P+R+F1\n",
    "print(X[0:3].argmax(0), X[3:6].argmax(0), X[6:9].argmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score.score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

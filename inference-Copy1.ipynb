{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class e2eDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file)\n",
    "        self.columns = self.dataset.columns\n",
    "        self.conditions = self.dataset[self.columns[0]]\n",
    "        self.sentences = self.dataset[self.columns[1]]\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.typ_list = {}\n",
    "        for k in range(len(self.conditions)):\n",
    "            cond_set = self.conditions[k].split(',')\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] in self.typ_list.keys():\n",
    "                    self.typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "                else:            \n",
    "                    self.typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conditions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cond = self.conditions[idx]\n",
    "        cond_set = cond.split(',')\n",
    "        condition_string = ''\n",
    "        for m in range(len(cond_set)):\n",
    "            cond_set[m] = cond_set[m].strip()\n",
    "            pos = cond_set[m].index('[')\n",
    "            \n",
    "            condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        \n",
    "        sen = self.sentences[idx]\n",
    "        input_string = condition_string + '<START>'\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(input_string, add_special_tokens=True))\n",
    "        \n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        return input_ids, sen, condition_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0506 00:55:24.555836 139751306041152 file_utils.py:41] PyTorch version 1.2.0 available.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package punkt to /home/ds_user1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "I0506 00:55:28.815614 139751306041152 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0506 00:55:28.816407 139751306041152 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0506 00:55:28.999634 139751306041152 tokenization_utils.py:663] Adding <START> to the vocabulary\n",
      "I0506 00:55:29.001787 139751306041152 tokenization_utils.py:741] Assigning <START> to the bos_token key of the tokenizer\n",
      "I0506 00:55:29.002326 139751306041152 tokenization_utils.py:663] Adding <name> to the vocabulary\n",
      "I0506 00:55:29.002830 139751306041152 tokenization_utils.py:663] Adding <eatType> to the vocabulary\n",
      "I0506 00:55:29.003309 139751306041152 tokenization_utils.py:663] Adding <priceRange> to the vocabulary\n",
      "I0506 00:55:29.003812 139751306041152 tokenization_utils.py:663] Adding <customer rating> to the vocabulary\n",
      "I0506 00:55:29.004278 139751306041152 tokenization_utils.py:663] Adding <near> to the vocabulary\n",
      "I0506 00:55:29.004797 139751306041152 tokenization_utils.py:663] Adding <food> to the vocabulary\n",
      "I0506 00:55:29.005409 139751306041152 tokenization_utils.py:663] Adding <area> to the vocabulary\n",
      "I0506 00:55:29.006875 139751306041152 tokenization_utils.py:663] Adding <familyFriendly> to the vocabulary\n",
      "I0506 00:55:29.007387 139751306041152 tokenization_utils.py:741] Assigning ['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>'] to the additional_special_tokens key of the tokenizer\n",
      "I0506 00:55:29.783185 139751306041152 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/ds_user1/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5\n",
      "I0506 00:55:29.784202 139751306041152 configuration_utils.py:311] Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0506 00:55:30.802666 139751306041152 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4693it [00:04, 1043.14it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th model\n",
      "BLEU score: 64.7182026491176\n",
      "BLEU1 score: 81.66713292807486\n",
      "ok\n",
      "2 th model\n",
      "BLEU score: 72.70623317413948\n",
      "BLEU1 score: 87.90252726244282\n",
      "ok\n",
      "3 th model\n",
      "BLEU score: 67.32506458544132\n",
      "BLEU1 score: 81.56109619474205\n",
      "ok\n",
      "4 th model\n",
      "BLEU score: 73.51276102082251\n",
      "BLEU1 score: 87.92926846373422\n",
      "ok\n",
      "5 th model\n",
      "BLEU score: 72.82402924422641\n",
      "BLEU1 score: 88.03365885644202\n",
      "ok\n",
      "6 th model\n",
      "BLEU score: 74.213070114825\n",
      "BLEU1 score: 89.14764820261996\n",
      "ok\n",
      "7 th model\n",
      "BLEU score: 74.03113321821753\n",
      "BLEU1 score: 88.99630691353684\n",
      "ok\n",
      "8 th model\n",
      "BLEU score: 73.73009345136494\n",
      "BLEU1 score: 89.32915071638948\n",
      "ok\n",
      "9 th model\n",
      "BLEU score: 73.59609822373056\n",
      "BLEU1 score: 88.46212463637237\n",
      "ok\n",
      "10 th model\n",
      "BLEU score: 73.34397462158798\n",
      "BLEU1 score: 88.4945119671014\n"
     ]
    }
   ],
   "source": [
    "# from model_large import *\n",
    "from model import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "max_len = 70\n",
    "\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "\n",
    "for i in range(1, 11):\n",
    "    model_name = './gen_model/base3_sample_30/'+str(i)+'/model'\n",
    "    my_model.load_state_dict(torch.load(model_name))\n",
    "    print('ok') \n",
    "    if i == 1:\n",
    "#         e2e_dataset = e2eDataset(csv_file='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "        e2e_dataset = e2eDataset(csv_file='dataset/testset_w_refs.csv', tokenizer=my_model.tokenizer)\n",
    "        dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        same_condition = []\n",
    "        ref_sentences = []\n",
    "        input_ids_list = []\n",
    "        pre_condition_string = ''\n",
    "        start = 0\n",
    "        for i_batch, sample_batched in tqdm(enumerate(dataloader)):\n",
    "            sen = sample_batched[1][0]\n",
    "            condition_string = sample_batched[2]  \n",
    "            input_ids = sample_batched[0].squeeze(0).cuda()\n",
    "\n",
    "            if start == 0 or condition_string == pre_condition_string:      \n",
    "                if start == 0:\n",
    "                    input_ids_list.append(input_ids)\n",
    "                same_condition.append(sen)        \n",
    "                pre_condition_string = condition_string\n",
    "                start += 1\n",
    "            else:   \n",
    "                input_ids_list.append(input_ids)\n",
    "                ref_sentences.append(same_condition)\n",
    "                pre_condition_string = condition_string\n",
    "                same_condition = [sen]\n",
    "                start += 1\n",
    "        ref_sentences.append(same_condition)    \n",
    "\n",
    "    bleu_score = 0\n",
    "    bleu_1 = 0\n",
    "\n",
    "#     f_dev = open('./predictions/testset/large2/f_dev_'+str(i)+'.txt', 'w')\n",
    "#     f_pred = open('./predictions/devset/base4/f_pred_'+str(i)+'.txt', 'w')\n",
    "    f_pred = open('./predictions/joosung2/testset/base3_sample30/f_pred_'+str(i)+'.txt', 'w')\n",
    "\n",
    "    for k in range(len(ref_sentences)):\n",
    "        input_ids = input_ids_list[k]\n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        ori_tokens = []\n",
    "        for m in range(len(ref_sentences[k])):\n",
    "#             f_dev.write(ref_sentences[k][m]+'\\n')\n",
    "            ori_tokens.append(word_tokenize(ref_sentences[k][m]))\n",
    "#         if k < len(ref_sentences)-1:\n",
    "#             f_dev.write('\\n')\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "            pred_idx = model_out.argmax(1)[-1]        \n",
    "            if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "                break            \n",
    "            input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "\n",
    "        out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "        f_pred.write(out_sen+'\\n')\n",
    "\n",
    "        out_tokens = word_tokenize(out_sen)\n",
    "\n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "        bleu_1 += bleu_1_score\n",
    "\n",
    "        bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))    \n",
    "\n",
    "#     f_dev.close()\n",
    "    f_pred.close()\n",
    "    \n",
    "    print(i, \"th model\")\n",
    "    print(\"BLEU score: {}\".format(bleu_score/len(ref_sentences)*100))\n",
    "    print(\"BLEU1 score: {}\".format(bleu_1/len(ref_sentences)*100))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT score with human reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_files = \"/project/work/E2E/predictions/joosung2/testset/f_dev.txt\"\n",
    "\n",
    "human_open = open(human_files, \"r\")\n",
    "human_dataset = human_open.readlines()\n",
    "human_open.close()\n",
    "\n",
    "human_references = []\n",
    "\n",
    "temp_reference = []\n",
    "for i in range(len(human_dataset)):\n",
    "    if human_dataset[i] == '\\n':\n",
    "        human_references.append(temp_reference)\n",
    "        temp_reference = []\n",
    "    else:\n",
    "        temp_reference.append(human_dataset[i].strip())\n",
    "human_references.append(temp_reference)\n",
    "human_compare = []\n",
    "for i in range(len(human_references)):\n",
    "    for k in range(len(human_references[i])):\n",
    "        human_compare.append(human_references[i][k])\n",
    "\n",
    "output_path = \"/project/work/E2E/predictions/joosung2/testset/base1_sample30/*\"\n",
    "pred_files = glob.glob(output_path)\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(pred_files)):    \n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        repeat_num = len(human_references[k])\n",
    "        for _ in range(repeat_num):\n",
    "            cands.append(out_sen)\n",
    "\n",
    "    P, R, F1 = score(cands, human_compare, lang='en', verbose=True)\n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    \n",
    "    score_list.append(BERT_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_files, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT score with human reference\n",
    "import csv\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_files = \"/project/work/E2E/predictions/testset/f_dev.txt\"\n",
    "\n",
    "human_open = open(human_files, \"r\")\n",
    "human_dataset = human_open.readlines()\n",
    "human_open.close()\n",
    "\n",
    "human_references = []\n",
    "\n",
    "temp_reference = []\n",
    "for i in range(len(human_dataset)):\n",
    "    if human_dataset[i] == '\\n':\n",
    "        human_references.append(temp_reference)\n",
    "        temp_reference = []\n",
    "    else:\n",
    "        temp_reference.append(human_dataset[i].strip())\n",
    "human_references.append(temp_reference)\n",
    "human_compare = []\n",
    "for i in range(len(human_references)):\n",
    "    for k in range(len(human_references[i])):\n",
    "        human_compare.append(human_references[i][k])\n",
    "\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/primary_txt/*\"\n",
    "pred_files = glob.glob(output_path)\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(pred_files)):    \n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        repeat_num = len(human_references[k])\n",
    "        for _ in range(repeat_num):\n",
    "            cands.append(out_sen)\n",
    "\n",
    "    P, R, F1 = score(cands, human_compare, lang='en', verbose=True)\n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    \n",
    "    score_list.append(BERT_score)  \n",
    "print(pred_files, score_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred_files)):\n",
    "    print(pred_files[i], score_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_1.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 470312.13 tokens per second.\n",
      "PTBTokenizer tokenized 16239 tokens at 128875.23 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.371\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.599\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.563\n",
      "Creating temp directory  /tmp/e2e-eval-o2g4n0r1\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:14:29\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-o2g4n0r1/mteval_ref.sgm -s /tmp/e2e-eval-o2g4n0r1/mteval_src.sgm -t /tmp/e2e-eval-o2g4n0r1/mteval_sys.sgm -f /tmp/e2e-eval-o2g4n0r1/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.5005  BLEU score = 0.5699 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.9936   1.2239   0.6178   0.3893   0.2760   0.1357   0.0726   0.0375   0.0207  \"tst\"\n",
      "\n",
      " BLEU:  0.8714   0.6858   0.5177   0.3726   0.2482   0.1586   0.1076   0.0714   0.0470  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.9936   6.2174   6.8352   7.2245   7.5005   7.6362   7.7088   7.7463   7.7670  \"tst\"\n",
      "\n",
      " BLEU:  0.8523   0.7561   0.6615   0.5699   0.4805   0.3980   0.3291   0.2711   0.2226  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:14:42\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.5699\n",
      "NIST: 7.5005\n",
      "METEOR: 0.3709\n",
      "ROUGE_L: 0.5994\n",
      "CIDEr: 1.5630\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_2.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 648115.94 tokens per second.\n",
      "PTBTokenizer tokenized 17269 tokens at 170716.12 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.432\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.676\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.007\n",
      "Creating temp directory  /tmp/e2e-eval-gd27p_7g\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:15:00\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-gd27p_7g/mteval_ref.sgm -s /tmp/e2e-eval-gd27p_7g/mteval_src.sgm -t /tmp/e2e-eval-gd27p_7g/mteval_sys.sgm -f /tmp/e2e-eval-gd27p_7g/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.4382  BLEU score = 0.6579 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4472   1.3790   0.7378   0.4987   0.3755   0.2060   0.1299   0.0775   0.0519  \"tst\"\n",
      "\n",
      " BLEU:  0.9218   0.7588   0.5973   0.4603   0.3406   0.2532   0.1952   0.1494   0.1136  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4472   6.8263   7.5640   8.0627   8.4382   8.6442   8.7741   8.8517   8.9036  \"tst\"\n",
      "\n",
      " BLEU:  0.9158   0.8309   0.7427   0.6579   0.5760   0.5017   0.4380   0.3826   0.3341  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:15:13\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6579\n",
      "NIST: 8.4382\n",
      "METEOR: 0.4322\n",
      "ROUGE_L: 0.6759\n",
      "CIDEr: 2.0066\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_3.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 641353.40 tokens per second.\n",
      "PTBTokenizer tokenized 14065 tokens at 153355.49 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.400\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.660\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.773\n",
      "Creating temp directory  /tmp/e2e-eval-ybrl6tba\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:15:30\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-ybrl6tba/mteval_ref.sgm -s /tmp/e2e-eval-ybrl6tba/mteval_src.sgm -t /tmp/e2e-eval-ybrl6tba/mteval_sys.sgm -f /tmp/e2e-eval-ybrl6tba/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.0658  BLEU score = 0.6049 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.6002   1.1725   0.6096   0.3879   0.2955   0.1503   0.0858   0.0513   0.0347  \"tst\"\n",
      "\n",
      " BLEU:  0.9482   0.7834   0.6189   0.4684   0.3314   0.2340   0.1686   0.1207   0.0830  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.6002   5.7727   6.3823   6.7703   7.0658   7.2161   7.3019   7.3532   7.3878  \"tst\"\n",
      "\n",
      " BLEU:  0.8420   0.7654   0.6854   0.6049   0.5237   0.4489   0.3837   0.3272   0.2773  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:15:42\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6049\n",
      "NIST: 7.0658\n",
      "METEOR: 0.3995\n",
      "ROUGE_L: 0.6601\n",
      "CIDEr: 1.7731\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_4.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 519739.34 tokens per second.\n",
      "PTBTokenizer tokenized 16687 tokens at 181605.68 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.429\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.686\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.120\n",
      "Creating temp directory  /tmp/e2e-eval-34fwg__b\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:16:00\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-34fwg__b/mteval_ref.sgm -s /tmp/e2e-eval-34fwg__b/mteval_src.sgm -t /tmp/e2e-eval-34fwg__b/mteval_sys.sgm -f /tmp/e2e-eval-34fwg__b/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.5222  BLEU score = 0.6623 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5010   1.4071   0.7518   0.4954   0.3671   0.1958   0.1183   0.0713   0.0491  \"tst\"\n",
      "\n",
      " BLEU:  0.9353   0.7783   0.6196   0.4782   0.3481   0.2529   0.1929   0.1465   0.1126  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5010   6.9080   7.6598   8.1552   8.5222   8.7181   8.8364   8.9077   8.9568  \"tst\"\n",
      "\n",
      " BLEU:  0.9090   0.8292   0.7453   0.6623   0.5790   0.5020   0.4361   0.3792   0.3303  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:16:13\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6623\n",
      "NIST: 8.5222\n",
      "METEOR: 0.4292\n",
      "ROUGE_L: 0.6857\n",
      "CIDEr: 2.1199\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_5.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 630477.65 tokens per second.\n",
      "PTBTokenizer tokenized 17490 tokens at 173597.15 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.435\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.677\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.149\n",
      "Creating temp directory  /tmp/e2e-eval-z1ipt35t\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:16:31\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-z1ipt35t/mteval_ref.sgm -s /tmp/e2e-eval-z1ipt35t/mteval_src.sgm -t /tmp/e2e-eval-z1ipt35t/mteval_sys.sgm -f /tmp/e2e-eval-z1ipt35t/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.3457  BLEU score = 0.6480 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4396   1.3736   0.7230   0.4690   0.3405   0.1776   0.1119   0.0711   0.0498  \"tst\"\n",
      "\n",
      " BLEU:  0.9183   0.7536   0.5889   0.4479   0.3258   0.2381   0.1831   0.1412   0.1090  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4396   6.8132   7.5362   8.0052   8.3457   8.5233   8.6352   8.7064   8.7562  \"tst\"\n",
      "\n",
      " BLEU:  0.9104   0.8247   0.7350   0.6480   0.5638   0.4876   0.4235   0.3687   0.3217  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:16:47\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6480\n",
      "NIST: 8.3457\n",
      "METEOR: 0.4354\n",
      "ROUGE_L: 0.6766\n",
      "CIDEr: 2.1493\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_6.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 509761.37 tokens per second.\n",
      "PTBTokenizer tokenized 18000 tokens at 143839.94 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.445\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.689\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.173\n",
      "Creating temp directory  /tmp/e2e-eval-y69dvemb\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:17:09\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-y69dvemb/mteval_ref.sgm -s /tmp/e2e-eval-y69dvemb/mteval_src.sgm -t /tmp/e2e-eval-y69dvemb/mteval_sys.sgm -f /tmp/e2e-eval-y69dvemb/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.4413  BLEU score = 0.6603 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4805   1.3831   0.7360   0.4855   0.3562   0.1915   0.1167   0.0739   0.0479  \"tst\"\n",
      "\n",
      " BLEU:  0.9205   0.7605   0.5968   0.4551   0.3314   0.2429   0.1844   0.1392   0.1049  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4805   6.8636   7.5996   8.0851   8.4413   8.6328   8.7495   8.8234   8.8713  \"tst\"\n",
      "\n",
      " BLEU:  0.9205   0.8367   0.7476   0.6603   0.5753   0.4983   0.4323   0.3752   0.3257  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:17:24\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6603\n",
      "NIST: 8.4413\n",
      "METEOR: 0.4447\n",
      "ROUGE_L: 0.6892\n",
      "CIDEr: 2.1731\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_7.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 633747.29 tokens per second.\n",
      "PTBTokenizer tokenized 17310 tokens at 155672.62 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.440\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.685\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.139\n",
      "Creating temp directory  /tmp/e2e-eval-bjlh72cw\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:17:45\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-bjlh72cw/mteval_ref.sgm -s /tmp/e2e-eval-bjlh72cw/mteval_src.sgm -t /tmp/e2e-eval-bjlh72cw/mteval_sys.sgm -f /tmp/e2e-eval-bjlh72cw/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.5803  BLEU score = 0.6577 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5718   1.4092   0.7510   0.4897   0.3584   0.1866   0.1159   0.0722   0.0486  \"tst\"\n",
      "\n",
      " BLEU:  0.9304   0.7684   0.6039   0.4579   0.3284   0.2366   0.1737   0.1266   0.0915  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5718   6.9811   7.7321   8.2218   8.5803   8.7669   8.8828   8.9550   9.0036  \"tst\"\n",
      "\n",
      " BLEU:  0.9177   0.8340   0.7455   0.6577   0.5708   0.4918   0.4230   0.3632   0.3111  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:17:59\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6577\n",
      "NIST: 8.5803\n",
      "METEOR: 0.4399\n",
      "ROUGE_L: 0.6851\n",
      "CIDEr: 2.1391\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_8.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 496548.09 tokens per second.\n",
      "PTBTokenizer tokenized 18189 tokens at 136671.09 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.448\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.685\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.171\n",
      "Creating temp directory  /tmp/e2e-eval-a64mx0d5\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:18:19\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-a64mx0d5/mteval_ref.sgm -s /tmp/e2e-eval-a64mx0d5/mteval_src.sgm -t /tmp/e2e-eval-a64mx0d5/mteval_sys.sgm -f /tmp/e2e-eval-a64mx0d5/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.4174  BLEU score = 0.6513 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4878   1.3821   0.7348   0.4709   0.3419   0.1757   0.1119   0.0713   0.0477  \"tst\"\n",
      "\n",
      " BLEU:  0.9193   0.7529   0.5876   0.4424   0.3164   0.2281   0.1699   0.1282   0.0985  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4878   6.8699   7.6047   8.0755   8.4174   8.5931   8.7050   8.7764   8.8241  \"tst\"\n",
      "\n",
      " BLEU:  0.9193   0.8320   0.7409   0.6513   0.5637   0.4848   0.4174   0.3601   0.3118  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:18:35\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6513\n",
      "NIST: 8.4174\n",
      "METEOR: 0.4481\n",
      "ROUGE_L: 0.6847\n",
      "CIDEr: 2.1710\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_9.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 532714.42 tokens per second.\n",
      "PTBTokenizer tokenized 17279 tokens at 121838.77 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.439\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.679\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.126\n",
      "Creating temp directory  /tmp/e2e-eval-3os4afzw\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:18:54\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-3os4afzw/mteval_ref.sgm -s /tmp/e2e-eval-3os4afzw/mteval_src.sgm -t /tmp/e2e-eval-3os4afzw/mteval_sys.sgm -f /tmp/e2e-eval-3os4afzw/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.5471  BLEU score = 0.6555 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5512   1.4031   0.7517   0.4863   0.3549   0.1833   0.1131   0.0683   0.0450  \"tst\"\n",
      "\n",
      " BLEU:  0.9278   0.7663   0.6021   0.4562   0.3255   0.2327   0.1703   0.1235   0.0902  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5512   6.9543   7.7060   8.1923   8.5471   8.7304   8.8435   8.9118   8.9568  \"tst\"\n",
      "\n",
      " BLEU:  0.9149   0.8315   0.7432   0.6555   0.5683   0.4886   0.4194   0.3594   0.3077  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:19:10\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6555\n",
      "NIST: 8.5471\n",
      "METEOR: 0.4387\n",
      "ROUGE_L: 0.6791\n",
      "CIDEr: 2.1258\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_10.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 341106.14 tokens per second.\n",
      "PTBTokenizer tokenized 17539 tokens at 158073.39 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.440\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.681\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.131\n",
      "Creating temp directory  /tmp/e2e-eval-hqc7olfw\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 May 6 at 02:19:31\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-hqc7olfw/mteval_ref.sgm -s /tmp/e2e-eval-hqc7olfw/mteval_src.sgm -t /tmp/e2e-eval-hqc7olfw/mteval_sys.sgm -f /tmp/e2e-eval-hqc7olfw/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.4907  BLEU score = 0.6524 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5245   1.3914   0.7445   0.4794   0.3509   0.1804   0.1111   0.0690   0.0447  \"tst\"\n",
      "\n",
      " BLEU:  0.9250   0.7592   0.5951   0.4509   0.3222   0.2314   0.1710   0.1255   0.0924  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5245   6.9159   7.6604   8.1398   8.4907   8.6712   8.7822   8.8512   8.8959  \"tst\"\n",
      "\n",
      " BLEU:  0.9159   0.8298   0.7403   0.6524   0.5655   0.4864   0.4184   0.3595   0.3088  \"tst\"\n",
      "MT evaluation scorer ended on 2020 May 6 at 02:19:46\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6524\n",
      "NIST: 8.4907\n",
      "METEOR: 0.4402\n",
      "ROUGE_L: 0.6812\n",
      "CIDEr: 2.1311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_1.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_2.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_3.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_4.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_5.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_6.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_7.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_8.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_9.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base3_sample30/f_pred_10.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base1_sample10_6.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base2_sample10_9.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base4_devtest_4.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/2base1_sample50_7.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/harv.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tsv to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/primary/*\"\n",
    "comapred_files = glob.glob(output_path)\n",
    "\n",
    "for i in range(len(comapred_files)):\n",
    "    dataset = pd.read_csv(comapred_files[i], delimiter='\\t', header=None)\n",
    "    \n",
    "    name = comapred_files[i].split('/')[-1].split('.')[0]\n",
    "    txt_files = \"/project/work/E2E/compared_system/system_outputs/primary_txt/\"+name+\".txt\"\n",
    "    f = open(txt_files, \"w\")\n",
    "    gen_sentences = dataset[1]\n",
    "    \n",
    "    for k in range(1, len(gen_sentences)):\n",
    "        f.write(gen_sentences[k]+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gen_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero-shot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "max_len = 70\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "model_name = './gen_model/base_devtrain_4/4/model'\n",
    "my_model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "csv_file='dataset/testset_w_refs.csv'\n",
    "dataset = pd.read_csv(csv_file)\n",
    "columns = dataset.columns\n",
    "conditions = dataset[columns[0]]\n",
    "        \n",
    "typ_list = {}\n",
    "for k in range(len(conditions)):\n",
    "    cond_set = conditions[k].split(',')\n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        if cond_set[m][:pos] in typ_list.keys():\n",
    "            typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "        else:            \n",
    "            typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}   \n",
    "\n",
    "def sample_batch(tokenizer, cond_name, cond_set):\n",
    "    condition_string = ''\n",
    "    for m in range(len(cond_set)):\n",
    "        condition_string += cond_name[m] + cond_set[m] + ' '\n",
    "\n",
    "    input_string = condition_string + '<START>'\n",
    "    input_ids = torch.tensor(tokenizer.encode(input_string, add_special_tokens=True))\n",
    "\n",
    "    input_len = len(input_ids)\n",
    "\n",
    "    return input_ids, condition_string\n",
    "\n",
    "# <name> <eatType> <food> <priceRange> <customer rating> <area> <familyFriendly> <near>\n",
    "\n",
    "name = None #'<NAME>'\n",
    "eatType = None #'<EATTYPE>'\n",
    "food = None # '<FOOD>'\n",
    "priceRange = None # '<priceRange>'\n",
    "customer_rating = None # '<CUSTOMER_RATING>'\n",
    "area = '<area>' # None\n",
    "familyFriendly = 'yes' # None\n",
    "near = None #'<NEAR>'\n",
    "\n",
    "cond_list = []\n",
    "conditions = []\n",
    "if name is not None:\n",
    "    placeholder_name = random.choice(list(typ_list['name']))\n",
    "    cond_list.append('<name>')\n",
    "    conditions.append(placeholder_name)\n",
    "if eatType is not None:\n",
    "    placeholder_eatType = random.choice(list(typ_list['eatType']))\n",
    "    cond_list.append('<eatType>')\n",
    "    conditions.append(placeholder_eatType)\n",
    "if food is not None:\n",
    "    placeholder_food = random.choice(list(typ_list['food']))\n",
    "    cond_list.append('<food>')\n",
    "    conditions.append(placeholder_food)\n",
    "if priceRange is not None:\n",
    "    placeholder_priceRange = random.choice(list(typ_list['priceRange']))\n",
    "    cond_list.append('<priceRange>')\n",
    "    conditions.append(placeholder_priceRange)    \n",
    "if customer_rating is not None:\n",
    "    placeholder_customer_rating = random.choice(list(typ_list['customer rating']))\n",
    "    cond_list.append('<customer rating>')\n",
    "    conditions.append(placeholder_customer_rating)        \n",
    "if area is not None:\n",
    "    placeholder_area = random.choice(list(typ_list['area']))\n",
    "    cond_list.append('<area>')\n",
    "    conditions.append(placeholder_area)    \n",
    "if familyFriendly is not None:\n",
    "    cond_list.append('<familyFriendly>')\n",
    "    conditions.append(familyFriendly)            \n",
    "if near is not None:\n",
    "    placeholder_near = random.choice(list(typ_list['near']))\n",
    "    cond_list.append('<near>')\n",
    "    conditions.append(placeholder_near)        \n",
    "\n",
    "\n",
    "# del cond_name[2]\n",
    "# del conditions[2]\n",
    "\n",
    "sample = sample_batch(my_model.tokenizer, cond_list, conditions)\n",
    "\n",
    "input_ids = sample[0].cuda()\n",
    "condition_string = sample[1]  \n",
    "input_len = len(input_ids)\n",
    "\n",
    "max_len = 70\n",
    "for _ in range(max_len):\n",
    "    model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "    pred_idx = model_out.argmax(1)[-1]        \n",
    "    if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "        break            \n",
    "    input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "\n",
    "out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "print(cond_list)\n",
    "print(conditions)\n",
    "print(out_sen)\n",
    "\n",
    "if name is not None:\n",
    "    out_sen = out_sen.replace(placeholder_name, name)\n",
    "if eatType is not None:\n",
    "    out_sen = out_sen.replace(placeholder_eatType, eatType)\n",
    "if food is not None:\n",
    "    out_sen = out_sen.replace(placeholder_food, food)\n",
    "if priceRange is not None:\n",
    "    out_sen = out_sen.replace(placeholder_priceRange, priceRange)\n",
    "if customer_rating is not None:\n",
    "    out_sen = out_sen.replace(placeholder_customer_rating, customer_rating)\n",
    "if area is not None:\n",
    "    out_sen = out_sen.replace(placeholder_area, area)  \n",
    "if near is not None:\n",
    "    out_sen = out_sen.replace(placeholder_near, near)\n",
    "print(out_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delexicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class e2eDataset(Dataset):\n",
    "    def __init__(self, csv_file1, csv_file2, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "        \"\"\"\n",
    "        self.dataset1 = pd.read_csv(csv_file1)\n",
    "        self.dataset2 = pd.read_csv(csv_file2)\n",
    "        \n",
    "        self.columns1 = self.dataset1.columns\n",
    "        self.columns2 = self.dataset2.columns\n",
    "        \n",
    "        self.conditions = list(self.dataset1[self.columns1[0]]) + list(self.dataset2[self.columns2[0]])\n",
    "        self.sentences = list(self.dataset1[self.columns1[1]]) + list(self.dataset2[self.columns2[1]])\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.typ_list = {}\n",
    "        for k in range(len(self.conditions)):\n",
    "            cond_set = self.conditions[k].split(',')\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] in self.typ_list.keys():\n",
    "                    self.typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "                else:            \n",
    "                    self.typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conditions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sen = self.sentences[idx]\n",
    "        \n",
    "        cond = self.conditions[idx]\n",
    "        cond_set = cond.split(',')        \n",
    "        condition_string = ''\n",
    "        \n",
    "        \n",
    "        p = random.random()\n",
    "\n",
    "        if p > 0.3: # 70%\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "\n",
    "                condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        else: # p <= 0.3 / 30%\n",
    "            nochange_list = ['priceRange', 'customer rating', 'familyFriendly']\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                \n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] not in nochange_list:\n",
    "                    placeholder = '<' + cond_set[m][:pos] + '>'\n",
    "                    condition_string += placeholder + ' '\n",
    "                    sen = sen.replace(cond_set[m][pos+1:-1], placeholder)        \n",
    "                else:\n",
    "                    condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        \n",
    "        input_string = condition_string + '<START>' + sen\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(input_string, add_special_tokens=True))\n",
    "        \n",
    "        label_string = sen + ' <|endoftext|>'\n",
    "        label_ids = torch.tensor(self.tokenizer.encode(label_string, add_special_tokens=True))\n",
    "\n",
    "        return input_string, input_ids, label_ids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_dataset = e2eDataset(csv_file1='dataset/trainset.csv', csv_file2='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "print(e2e_dataset[200][0])\n",
    "# print(e2e_dataset.typ_list.keys())\n",
    "# print(e2e_dataset.typ_list)\n",
    "# priceRange, customer rating, familyFriendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dev = open('f_dev.txt', 'r')\n",
    "f_pred = open('f_pred.txt', 'r')\n",
    "f_dev_dataset = f_dev.readlines()\n",
    "f_pred_dataset = f_pred.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f_dev_dataset), len(f_pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[x for x in f_dev_dataset if x != '\\n']\n",
    "data2=[x for x in f_pred_dataset if x != '\\n']\n",
    "len(data), len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4672+546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dev = open('./e2e-metrics/example-inputs/devel-conc.txt', 'r')\n",
    "f_pred = open('./e2e-metrics/example-inputs/baseline-output.txt', 'r')\n",
    "f_dev_dataset = f_dev.readlines()\n",
    "f_pred_dataset = f_pred.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f_dev_dataset), len(f_pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[x for x in f_dev_dataset if x != '\\n']\n",
    "data2=[x for x in f_pred_dataset if x != '\\n']\n",
    "len(data), len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "p=random.random()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

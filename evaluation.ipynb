{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT score with human reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_path = '/DATA/joosung/controllable_english/evaluation/yelp/human/'\n",
    "neg_human_DRG = human_path + 'sentiment.test.0.humanDRG'\n",
    "neg_human_DRG_open = open(neg_human_DRG, \"r\")\n",
    "neg_human_DRG_dataset = neg_human_DRG_open.readlines()\n",
    "neg_human_DRG_open.close()\n",
    "\n",
    "pos_human_DRG = human_path + 'sentiment.test.1.humanDRG'\n",
    "pos_human_DRG_open = open(pos_human_DRG, \"r\")\n",
    "pos_human_DRG_dataset = pos_human_DRG_open.readlines()\n",
    "pos_human_DRG_open.close()\n",
    "\n",
    "neg_human_DUAL = human_path + 'sentiment.test.0.humanDUAL'\n",
    "neg_human_DUAL_open = open(neg_human_DUAL, \"r\")\n",
    "neg_human_DUAL_dataset = neg_human_DUAL_open.readlines()\n",
    "neg_human_DUAL_open.close()\n",
    "\n",
    "pos_human_DUAL = human_path + 'sentiment.test.1.humanDUAL'\n",
    "pos_human_DUAL_open = open(pos_human_DUAL, \"r\")\n",
    "pos_human_DUAL_dataset = pos_human_DUAL_open.readlines()\n",
    "pos_human_DUAL_open.close()\n",
    "\n",
    "DRG_refs = []\n",
    "neg_len = len(neg_human_DRG_dataset)\n",
    "pos_len = len(pos_human_DRG_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_DRG_dataset[k].split('\\t')[1]\n",
    "    DRG_refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_DRG_dataset[k].split('\\t')[1]\n",
    "    DRG_refs.append(ref_sen)\n",
    "\n",
    "DUAL_refs = []\n",
    "neg_len = len(neg_human_DUAL_dataset)\n",
    "pos_len = len(pos_human_DUAL_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_DUAL_dataset[k].split('\\t')[1]\n",
    "    DUAL_refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_DUAL_dataset[k].split('\\t')[1]\n",
    "    DUAL_refs.append(ref_sen)\n",
    "    \n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "model_name=[]\n",
    "score_list=[]\n",
    "for i in range(len(neg_out_files)):    \n",
    "    cands = []\n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "\n",
    "    P, R, DRG_F1 = score(cands, DRG_refs, lang='en', verbose=True)\n",
    "    DRG_F1_list=list(DRG_F1.numpy())\n",
    "    DRG_BERT_scroe = sum(DRG_F1_list)/len(DRG_F1_list)\n",
    "    \n",
    "    P, R, DUAL_F1 = score(cands, DUAL_refs, lang='en', verbose=True)\n",
    "    DUAL_F1_list=list(DUAL_F1.numpy())\n",
    "    DUAL_BERT_scroe = sum(DUAL_F1_list)/len(DUAL_F1_list)\n",
    "    \n",
    "    BERT_score = (DRG_BERT_scroe+DUAL_BERT_scroe)/2    \n",
    "    \n",
    "    model_name.append(neg_out_files[i].split('.')[-1])\n",
    "    score_list.append(BERT_score)\n",
    "for i in range(len(model_name)):\n",
    "    print(\"model: \", model_name[i])\n",
    "    print('BERT score(F1): {}'.format(score_list[i]*100))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PPL\n",
    "import torch, os\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device='cpu'\n",
    "model_class, tokenizer_class = (GPT2LMHeadModel, GPT2Tokenizer)\n",
    "gpt2_lm_tokenizer = tokenizer_class.from_pretrained('gpt2-large')\n",
    "gpt2_lm_model = model_class.from_pretrained('gpt2-large')\n",
    "gpt2_lm_model.to(device)\n",
    "gpt2_lm_model.eval()\n",
    "\n",
    "lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def calculate_ppl_gpt(sentence_batch):\n",
    "    # tokenize the sentences\n",
    "    tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    \n",
    "    for i in range(len(sentence_batch)):\n",
    "        tokenized_ids[i] = gpt2_lm_tokenizer.encode(sentence_batch[i])\n",
    "        \n",
    "    sen_lengths = [len(x) for x in tokenized_ids]\n",
    "    max_sen_lenght = max(sen_lengths)\n",
    "    \n",
    "    n_batch = len(sentence_batch)\n",
    "    input_ids = np.zeros( shape=(n_batch, max_sen_lenght), dtype=np.int64)\n",
    "    lm_labels = np.full(shape=(n_batch, max_sen_lenght), fill_value=-1)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_ids):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "        lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    lm_labels = torch.tensor(lm_labels).to(device)\n",
    "    with torch.no_grad():\n",
    "        lm_pred = gpt2_lm_model(input_ids)\n",
    "    loss_val = lm_loss(lm_pred[0].view(-1, lm_pred[0].size(-1)), lm_labels.view(-1))\n",
    "    normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1) / torch.tensor(sen_lengths, dtype=torch.float32).to(device)\n",
    "    #normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1)\n",
    "    ppl = torch.exp(normalized_loss)\n",
    "    return  ppl.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_PPL = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "            neg_PPL += sample_PPL\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_PPL = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "            pos_PPL += sample_PPL\n",
    "\n",
    "    total_PPL = (neg_PPL+pos_PPL)/(neg_len+pos_len)\n",
    "    print('PPL: {}'.format(total_PPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_1.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 618160.09 tokens per second.\n",
      "PTBTokenizer tokenized 13259 tokens at 132508.62 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.317\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.572\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.092\n",
      "Creating temp directory  /tmp/e2e-eval-0z53x6m8\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 11 at 10:23:28\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-0z53x6m8/mteval_ref.sgm -s /tmp/e2e-eval-0z53x6m8/mteval_src.sgm -t /tmp/e2e-eval-0z53x6m8/mteval_sys.sgm -f /tmp/e2e-eval-0z53x6m8/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 5.5563  BLEU score = 0.4846 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  3.6638   0.9606   0.4708   0.2738   0.1873   0.0807   0.0474   0.0225   0.0127  \"tst\"\n",
      "\n",
      " BLEU:  0.8920   0.6797   0.4902   0.3398   0.2195   0.1450   0.0976   0.0671   0.0488  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  3.6638   4.6244   5.0952   5.3690   5.5563   5.6370   5.6844   5.7068   5.7195  \"tst\"\n",
      "\n",
      " BLEU:  0.7668   0.6693   0.5737   0.4846   0.4013   0.3302   0.2715   0.2237   0.1857  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 11 at 10:23:45\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.4846\n",
      "NIST: 5.5563\n",
      "METEOR: 0.3175\n",
      "ROUGE_L: 0.5716\n",
      "CIDEr: 1.0920\n",
      "\n",
      "/data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_2.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 630072.71 tokens per second.\n",
      "PTBTokenizer tokenized 16302 tokens at 125974.21 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.369\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.607\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.465\n",
      "Creating temp directory  /tmp/e2e-eval-uellgqla\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 11 at 10:24:04\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-uellgqla/mteval_ref.sgm -s /tmp/e2e-eval-uellgqla/mteval_src.sgm -t /tmp/e2e-eval-uellgqla/mteval_sys.sgm -f /tmp/e2e-eval-uellgqla/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.4472  BLEU score = 0.5373 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.9683   1.2545   0.6122   0.3642   0.2480   0.1074   0.0560   0.0297   0.0181  \"tst\"\n",
      "\n",
      " BLEU:  0.8696   0.6669   0.4823   0.3367   0.2176   0.1405   0.0944   0.0664   0.0485  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.9683   6.2228   6.8350   7.1992   7.4472   7.5546   7.6106   7.6403   7.6584  \"tst\"\n",
      "\n",
      " BLEU:  0.8434   0.7386   0.6343   0.5373   0.4457   0.3658   0.3002   0.2476   0.2059  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 11 at 10:24:20\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.5373\n",
      "NIST: 7.4472\n",
      "METEOR: 0.3694\n",
      "ROUGE_L: 0.6069\n",
      "CIDEr: 1.4655\n",
      "\n",
      "/data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_3.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 627558.20 tokens per second.\n",
      "PTBTokenizer tokenized 17407 tokens at 162072.02 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.394\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.631\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.631\n",
      "Creating temp directory  /tmp/e2e-eval-i9hwkrgq\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 11 at 10:24:40\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-i9hwkrgq/mteval_ref.sgm -s /tmp/e2e-eval-i9hwkrgq/mteval_src.sgm -t /tmp/e2e-eval-i9hwkrgq/mteval_sys.sgm -f /tmp/e2e-eval-i9hwkrgq/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.8815  BLEU score = 0.5786 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.2028   1.3364   0.6638   0.4036   0.2749   0.1239   0.0693   0.0361   0.0217  \"tst\"\n",
      "\n",
      " BLEU:  0.8861   0.6942   0.5114   0.3651   0.2408   0.1587   0.1059   0.0700   0.0470  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.2028   6.5392   7.2030   7.6066   7.8815   8.0054   8.0747   8.1108   8.1325  \"tst\"\n",
      "\n",
      " BLEU:  0.8806   0.7794   0.6759   0.5786   0.4849   0.4021   0.3320   0.2731   0.2244  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 11 at 10:24:57\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.5786\n",
      "NIST: 7.8815\n",
      "METEOR: 0.3940\n",
      "ROUGE_L: 0.6308\n",
      "CIDEr: 1.6305\n",
      "\n",
      "/data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_4.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 629226.13 tokens per second.\n",
      "PTBTokenizer tokenized 17758 tokens at 167143.70 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.416\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.645\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.863\n",
      "Creating temp directory  /tmp/e2e-eval-sjgjq3o2\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 11 at 10:25:16\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-sjgjq3o2/mteval_ref.sgm -s /tmp/e2e-eval-sjgjq3o2/mteval_src.sgm -t /tmp/e2e-eval-sjgjq3o2/mteval_sys.sgm -f /tmp/e2e-eval-sjgjq3o2/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.1166  BLEU score = 0.6001 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.3262   1.3702   0.6969   0.4256   0.2978   0.1356   0.0767   0.0448   0.0268  \"tst\"\n",
      "\n",
      " BLEU:  0.8969   0.7116   0.5317   0.3822   0.2592   0.1773   0.1230   0.0875   0.0620  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.3262   6.6963   7.3932   7.8189   8.1166   8.2523   8.3289   8.3737   8.4005  \"tst\"\n",
      "\n",
      " BLEU:  0.8969   0.7989   0.6975   0.6001   0.5074   0.4258   0.3566   0.2992   0.2512  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 11 at 10:25:33\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6001\n",
      "NIST: 8.1166\n",
      "METEOR: 0.4157\n",
      "ROUGE_L: 0.6451\n",
      "CIDEr: 1.8635\n",
      "\n",
      "/data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_5.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 613666.93 tokens per second.\n",
      "PTBTokenizer tokenized 18083 tokens at 166930.03 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.405\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.638\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.769\n",
      "Creating temp directory  /tmp/e2e-eval-rmglp6a7\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 11 at 10:25:53\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-rmglp6a7/mteval_ref.sgm -s /tmp/e2e-eval-rmglp6a7/mteval_src.sgm -t /tmp/e2e-eval-rmglp6a7/mteval_sys.sgm -f /tmp/e2e-eval-rmglp6a7/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.9009  BLEU score = 0.5896 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.2095   1.3359   0.6684   0.4076   0.2795   0.1285   0.0714   0.0365   0.0198  \"tst\"\n",
      "\n",
      " BLEU:  0.8880   0.6991   0.5211   0.3735   0.2482   0.1643   0.1100   0.0748   0.0503  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.2095   6.5454   7.2138   7.6214   7.9009   8.0293   8.1007   8.1372   8.1570  \"tst\"\n",
      "\n",
      " BLEU:  0.8880   0.7879   0.6864   0.5896   0.4959   0.4125   0.3415   0.2825   0.2332  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 11 at 10:26:09\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.5896\n",
      "NIST: 7.9009\n",
      "METEOR: 0.4051\n",
      "ROUGE_L: 0.6384\n",
      "CIDEr: 1.7685\n",
      "\n",
      "/data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_6.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 655905.07 tokens per second.\n",
      "PTBTokenizer tokenized 17655 tokens at 165447.74 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.401\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.637\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.695\n",
      "Creating temp directory  /tmp/e2e-eval-zn3wjw1b\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 11 at 10:26:29\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-zn3wjw1b/mteval_ref.sgm -s /tmp/e2e-eval-zn3wjw1b/mteval_src.sgm -t /tmp/e2e-eval-zn3wjw1b/mteval_sys.sgm -f /tmp/e2e-eval-zn3wjw1b/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.9324  BLEU score = 0.5851 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.1815   1.3481   0.6869   0.4252   0.2907   0.1329   0.0753   0.0449   0.0258  \"tst\"\n",
      "\n",
      " BLEU:  0.8820   0.6920   0.5136   0.3738   0.2569   0.1770   0.1250   0.0926   0.0697  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.1815   6.5296   7.2164   7.6416   7.9324   8.0653   8.1406   8.1855   8.2113  \"tst\"\n",
      "\n",
      " BLEU:  0.8820   0.7812   0.6793   0.5851   0.4963   0.4179   0.3518   0.2977   0.2534  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 11 at 10:26:46\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.5851\n",
      "NIST: 7.9324\n",
      "METEOR: 0.4013\n",
      "ROUGE_L: 0.6371\n",
      "CIDEr: 1.6953\n",
      "\n",
      "/data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_7.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 647991.46 tokens per second.\n",
      "PTBTokenizer tokenized 17122 tokens at 164551.41 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.401\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.640\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.773\n",
      "Creating temp directory  /tmp/e2e-eval-bzqn3l51\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 11 at 10:27:05\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-bzqn3l51/mteval_ref.sgm -s /tmp/e2e-eval-bzqn3l51/mteval_src.sgm -t /tmp/e2e-eval-bzqn3l51/mteval_sys.sgm -f /tmp/e2e-eval-bzqn3l51/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.9833  BLEU score = 0.5884 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.2327   1.3642   0.6810   0.4175   0.2878   0.1287   0.0701   0.0394   0.0226  \"tst\"\n",
      "\n",
      " BLEU:  0.8915   0.6990   0.5199   0.3763   0.2551   0.1734   0.1207   0.0879   0.0654  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.2327   6.5969   7.2779   7.6954   7.9833   8.1120   8.1821   8.2215   8.2441  \"tst\"\n",
      "\n",
      " BLEU:  0.8878   0.7861   0.6839   0.5884   0.4974   0.4170   0.3491   0.2937   0.2484  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 11 at 10:27:22\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.5884\n",
      "NIST: 7.9833\n",
      "METEOR: 0.4013\n",
      "ROUGE_L: 0.6396\n",
      "CIDEr: 1.7731\n",
      "\n",
      "/data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_8.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 628769.51 tokens per second.\n",
      "PTBTokenizer tokenized 17217 tokens at 159426.71 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.408\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.646\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.823\n",
      "Creating temp directory  /tmp/e2e-eval-7is_3o83\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 11 at 10:27:43\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-7is_3o83/mteval_ref.sgm -s /tmp/e2e-eval-7is_3o83/mteval_src.sgm -t /tmp/e2e-eval-7is_3o83/mteval_sys.sgm -f /tmp/e2e-eval-7is_3o83/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.1011  BLEU score = 0.5954 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.2865   1.3840   0.7049   0.4291   0.2966   0.1315   0.0711   0.0418   0.0230  \"tst\"\n",
      "\n",
      " BLEU:  0.8957   0.7053   0.5290   0.3833   0.2593   0.1762   0.1234   0.0909   0.0675  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.2865   6.6705   7.3755   7.8045   8.1011   8.2326   8.3037   8.3455   8.3685  \"tst\"\n",
      "\n",
      " BLEU:  0.8914   0.7910   0.6906   0.5954   0.5037   0.4225   0.3541   0.2986   0.2530  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 11 at 10:28:00\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.5954\n",
      "NIST: 8.1011\n",
      "METEOR: 0.4078\n",
      "ROUGE_L: 0.6460\n",
      "CIDEr: 1.8226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_1.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_2.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_3.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_4.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_5.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_6.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_7.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/predictions/no_pretrained/try_3/f_pred_8.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

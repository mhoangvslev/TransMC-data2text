{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "scorer = BERTScorer(lang=\"en\",  rescale_with_baseline=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "human_files = \"/data/private/WebNLG-models/prediction/challenge/reference.txt\"\n",
    "\n",
    "human_open = open(human_files, \"r\")\n",
    "human_dataset = human_open.readlines()\n",
    "human_open.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/data/private/WebNLG-models/prediction/challenge/my_output/*\"\n",
    "pred_files = glob.glob(output_path)\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(pred_files)):    \n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    P, R, F1 = scorer.score(human_dataset, pred_data_dataset)\n",
    "    \n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    \n",
    "    score_list.append(BERT_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/private/WebNLG-models/prediction/challenge/my_output/prediction_8.txt\n",
      "0.9195736255156622\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred_files)):\n",
    "    print(pred_files[i])\n",
    "    print(score_list[i])\n",
    "\n",
    "    \n",
    "# /data/private/WebNLG-models/prediction/challenge/my_output/prediction_8.txt\n",
    "# 0.9195736255156622\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/ADAPT_nmt.txt\n",
    "# 0.86626166329348\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/BIU_Chimera_v1.txt\n",
    "# 0.8674857248297055\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/BIU_nmt.txt\n",
    "# 0.8811597465892611\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/BIU_Random_0.txt\n",
    "# 0.8632380363444381\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/BIU_Random_1.txt\n",
    "# 0.8628469260947661\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/BIU_Random_2.txt\n",
    "# 0.8632543831263397\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/Melbourne_nmt.txt\n",
    "# 0.8841185178587695\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/Tilburg_pipeline.txt\n",
    "# 0.7477341472910503\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/Tilburg_smt.txt\n",
    "# 0.8760349681054739\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/UIT-VNU_pipeline.txt\n",
    "# 0.8205524895872388\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/UPF_pipeline.txt\n",
    "# 0.8831922012100158\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/PKUWriter_nmt.txt\n",
    "# 0.8698436211310447\n",
    "# /data/private/WebNLG-models/prediction/challenge/compare/Tilburg_nmt.txt\n",
    "# 0.8719970103109177\n",
    "# /data/private/WebNLG-    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlgeval import compute_metrics\n",
    "metrics_dict = compute_metrics(hypothesis='prediction/prediction_1.txt', references=['prediction/reference.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=(blue1*blue2*bleu3*bleu4)**(1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=(0.304804*0.214815*0.155740*0.115418)**(1/4)\n",
    "# x = (0.3191*0.1777*0.1076*0.0688*0.0454*0.0306*0.0215*0.0156*0.0113)**(1/9)\n",
    "x = (0.3191*0.1777*0.1076*0.0688)**(1/4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlgeval import compute_metrics\n",
    "metrics_dict = compute_metrics(hypothesis='ppp.txt', references=['ttt.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_4/prediction_6.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_4/prediction_8.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_4/prediction_10.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_4/prediction_12.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_4/prediction_14.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_4/prediction_16.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_6.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 277884.59 tokens per second.\n",
      "PTBTokenizer tokenized 46159 tokens at 304700.88 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.315\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.489\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.661\n",
      "Creating temp directory  /tmp/e2e-eval-j7t1_61v\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 19 at 09:21:36\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-j7t1_61v/mteval_ref.sgm -s /tmp/e2e-eval-j7t1_61v/mteval_src.sgm -t /tmp/e2e-eval-j7t1_61v/mteval_sys.sgm -f /tmp/e2e-eval-j7t1_61v/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 6.3177  BLEU score = 0.2864 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.8734   1.0347   0.2592   0.1081   0.0422   0.0191   0.0101   0.0058   0.0040  \"tst\"\n",
      "\n",
      " BLEU:  0.5881   0.3394   0.2207   0.1528   0.1090   0.0804   0.0612   0.0472   0.0368  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.8734   5.9081   6.1674   6.2754   6.3177   6.3368   6.3469   6.3527   6.3567  \"tst\"\n",
      "\n",
      " BLEU:  0.5881   0.4468   0.3532   0.2864   0.2361   0.1973   0.1669   0.1426   0.1226  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 19 at 09:21:46\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2864\n",
      "NIST: 6.3177\n",
      "METEOR: 0.3154\n",
      "ROUGE_L: 0.4888\n",
      "CIDEr: 2.6612\n",
      "\n",
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_7.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 308317.98 tokens per second.\n",
      "PTBTokenizer tokenized 46493 tokens at 317228.69 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.314\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.482\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.667\n",
      "Creating temp directory  /tmp/e2e-eval-zjfzt1on\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 19 at 09:22:02\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-zjfzt1on/mteval_ref.sgm -s /tmp/e2e-eval-zjfzt1on/mteval_src.sgm -t /tmp/e2e-eval-zjfzt1on/mteval_sys.sgm -f /tmp/e2e-eval-zjfzt1on/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 6.2339  BLEU score = 0.2808 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.8174   1.0259   0.2477   0.1024   0.0405   0.0187   0.0099   0.0056   0.0037  \"tst\"\n",
      "\n",
      " BLEU:  0.5809   0.3336   0.2154   0.1490   0.1057   0.0774   0.0588   0.0456   0.0358  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.8174   5.8433   6.0910   6.1934   6.2339   6.2526   6.2625   6.2681   6.2718  \"tst\"\n",
      "\n",
      " BLEU:  0.5809   0.4402   0.3469   0.2808   0.2310   0.1925   0.1625   0.1386   0.1193  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 19 at 09:22:12\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2808\n",
      "NIST: 6.2339\n",
      "METEOR: 0.3145\n",
      "ROUGE_L: 0.4815\n",
      "CIDEr: 2.6665\n",
      "\n",
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_8.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 297619.79 tokens per second.\n",
      "PTBTokenizer tokenized 45469 tokens at 265235.90 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.314\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.486\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.678\n",
      "Creating temp directory  /tmp/e2e-eval-oyuroa9p\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 19 at 09:22:27\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-oyuroa9p/mteval_ref.sgm -s /tmp/e2e-eval-oyuroa9p/mteval_src.sgm -t /tmp/e2e-eval-oyuroa9p/mteval_sys.sgm -f /tmp/e2e-eval-oyuroa9p/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 6.3744  BLEU score = 0.2881 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.9222   1.0412   0.2588   0.1094   0.0428   0.0194   0.0099   0.0058   0.0039  \"tst\"\n",
      "\n",
      " BLEU:  0.5935   0.3405   0.2220   0.1536   0.1093   0.0797   0.0594   0.0453   0.0353  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.9222   5.9634   6.2222   6.3316   6.3744   6.3938   6.4037   6.4095   6.4134  \"tst\"\n",
      "\n",
      " BLEU:  0.5935   0.4496   0.3553   0.2881   0.2374   0.1979   0.1666   0.1416   0.1213  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 19 at 09:22:37\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2881\n",
      "NIST: 6.3744\n",
      "METEOR: 0.3140\n",
      "ROUGE_L: 0.4859\n",
      "CIDEr: 2.6784\n",
      "\n",
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_9.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 300945.99 tokens per second.\n",
      "PTBTokenizer tokenized 45445 tokens at 299199.37 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.309\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.477\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.656\n",
      "Creating temp directory  /tmp/e2e-eval-6q47hu27\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 19 at 09:22:53\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-6q47hu27/mteval_ref.sgm -s /tmp/e2e-eval-6q47hu27/mteval_src.sgm -t /tmp/e2e-eval-6q47hu27/mteval_sys.sgm -f /tmp/e2e-eval-6q47hu27/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 6.2476  BLEU score = 0.2812 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.8284   1.0215   0.2524   0.1037   0.0416   0.0185   0.0099   0.0053   0.0037  \"tst\"\n",
      "\n",
      " BLEU:  0.5831   0.3329   0.2155   0.1495   0.1072   0.0784   0.0596   0.0464   0.0368  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.8284   5.8499   6.1023   6.2060   6.2476   6.2660   6.2759   6.2813   6.2849  \"tst\"\n",
      "\n",
      " BLEU:  0.5831   0.4406   0.3471   0.2812   0.2319   0.1935   0.1636   0.1397   0.1205  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 19 at 09:23:03\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2812\n",
      "NIST: 6.2476\n",
      "METEOR: 0.3087\n",
      "ROUGE_L: 0.4769\n",
      "CIDEr: 2.6557\n",
      "\n",
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_10.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 301048.03 tokens per second.\n",
      "PTBTokenizer tokenized 46015 tokens at 302922.48 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.309\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.479\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.635\n",
      "Creating temp directory  /tmp/e2e-eval-zz9uihut\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 19 at 09:23:19\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-zz9uihut/mteval_ref.sgm -s /tmp/e2e-eval-zz9uihut/mteval_src.sgm -t /tmp/e2e-eval-zz9uihut/mteval_sys.sgm -f /tmp/e2e-eval-zz9uihut/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 6.2064  BLEU score = 0.2786 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.7990   1.0190   0.2445   0.1027   0.0413   0.0189   0.0100   0.0058   0.0038  \"tst\"\n",
      "\n",
      " BLEU:  0.5789   0.3312   0.2135   0.1472   0.1052   0.0772   0.0588   0.0458   0.0362  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.7990   5.8180   6.0624   6.1651   6.2064   6.2253   6.2353   6.2411   6.2449  \"tst\"\n",
      "\n",
      " BLEU:  0.5789   0.4379   0.3446   0.2786   0.2293   0.1912   0.1616   0.1380   0.1189  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 19 at 09:23:29\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2786\n",
      "NIST: 6.2064\n",
      "METEOR: 0.3092\n",
      "ROUGE_L: 0.4786\n",
      "CIDEr: 2.6352\n",
      "\n",
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_11.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 262147.02 tokens per second.\n",
      "PTBTokenizer tokenized 46585 tokens at 302964.70 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.309\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.476\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.622\n",
      "Creating temp directory  /tmp/e2e-eval-_u7s9v3y\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 19 at 09:23:45\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-_u7s9v3y/mteval_ref.sgm -s /tmp/e2e-eval-_u7s9v3y/mteval_src.sgm -t /tmp/e2e-eval-_u7s9v3y/mteval_sys.sgm -f /tmp/e2e-eval-_u7s9v3y/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 6.1480  BLEU score = 0.2764 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.7535   1.0075   0.2443   0.1028   0.0399   0.0187   0.0099   0.0054   0.0037  \"tst\"\n",
      "\n",
      " BLEU:  0.5753   0.3275   0.2118   0.1463   0.1050   0.0769   0.0587   0.0461   0.0368  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.7535   5.7610   6.0053   6.1081   6.1480   6.1667   6.1766   6.1820   6.1857  \"tst\"\n",
      "\n",
      " BLEU:  0.5753   0.4341   0.3417   0.2764   0.2278   0.1901   0.1607   0.1375   0.1188  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 19 at 09:23:55\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2764\n",
      "NIST: 6.1480\n",
      "METEOR: 0.3085\n",
      "ROUGE_L: 0.4758\n",
      "CIDEr: 2.6219\n",
      "\n",
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_12.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 300513.88 tokens per second.\n",
      "PTBTokenizer tokenized 46421 tokens at 300943.79 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.309\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.474\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.610\n",
      "Creating temp directory  /tmp/e2e-eval-eusw1blm\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 19 at 09:24:11\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-eusw1blm/mteval_ref.sgm -s /tmp/e2e-eval-eusw1blm/mteval_src.sgm -t /tmp/e2e-eval-eusw1blm/mteval_sys.sgm -f /tmp/e2e-eval-eusw1blm/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 6.1655  BLEU score = 0.2767 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.7699   1.0099   0.2443   0.1024   0.0390   0.0180   0.0096   0.0053   0.0037  \"tst\"\n",
      "\n",
      " BLEU:  0.5753   0.3282   0.2124   0.1461   0.1039   0.0755   0.0574   0.0447   0.0355  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.7699   5.7798   6.0241   6.1265   6.1655   6.1835   6.1931   6.1984   6.2021  \"tst\"\n",
      "\n",
      " BLEU:  0.5753   0.4345   0.3423   0.2767   0.2275   0.1893   0.1596   0.1361   0.1173  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 19 at 09:24:20\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2767\n",
      "NIST: 6.1655\n",
      "METEOR: 0.3089\n",
      "ROUGE_L: 0.4742\n",
      "CIDEr: 2.6102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_6.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_7.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_8.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_9.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_10.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_11.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_5_dev/prediction_12.txt\n",
    "\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_1/prediction_4.txt\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_1/prediction_5.txt\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_1/prediction_6.txt\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_1/prediction_7.txt\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_1/prediction_8.txt\n",
    "\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_2_mat/prediction_6.txt\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_2_mat/prediction_8.txt\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_2_mat/prediction_10.txt\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_2_mat/prediction_12.txt\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_2_mat/prediction_14.txt\n",
    "# !/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/try_2_mat/prediction_16.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_folders[0].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/compare/WebNLG_baseline.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 308995.14 tokens per second.\n",
      "PTBTokenizer tokenized 37173 tokens at 293506.50 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.209\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.359\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.675\n",
      "Creating temp directory  /tmp/e2e-eval-8lmdihg5\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 13 at 14:03:06\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-8lmdihg5/mteval_ref.sgm -s /tmp/e2e-eval-8lmdihg5/mteval_src.sgm -t /tmp/e2e-eval-8lmdihg5/mteval_sys.sgm -f /tmp/e2e-eval-8lmdihg5/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 4.7703  BLEU score = 0.2140 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  3.6672   0.7853   0.1960   0.0862   0.0357   0.0166   0.0101   0.0059   0.0035  \"tst\"\n",
      "\n",
      " BLEU:  0.5482   0.2973   0.1993   0.1416   0.1019   0.0761   0.0587   0.0461   0.0361  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  3.6672   4.4525   4.6485   4.7347   4.7703   4.7870   4.7971   4.8030   4.8065  \"tst\"\n",
      "\n",
      " BLEU:  0.4504   0.3317   0.2622   0.2140   0.1774   0.1491   0.1269   0.1091   0.0944  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 13 at 14:03:15\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2140\n",
      "NIST: 4.7703\n",
      "METEOR: 0.2089\n",
      "ROUGE_L: 0.3585\n",
      "CIDEr: 1.6754\n",
      "\n",
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/compare/UPF_pipeline.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 307588.68 tokens per second.\n",
      "PTBTokenizer tokenized 46958 tokens at 338552.06 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.357\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.502\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.768\n",
      "Creating temp directory  /tmp/e2e-eval-c6cheo43\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 13 at 14:03:30\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-c6cheo43/mteval_ref.sgm -s /tmp/e2e-eval-c6cheo43/mteval_src.sgm -t /tmp/e2e-eval-c6cheo43/mteval_sys.sgm -f /tmp/e2e-eval-c6cheo43/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 6.8557  BLEU score = 0.2641 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5470   0.9791   0.2195   0.0828   0.0273   0.0090   0.0039   0.0010   0.0006  \"tst\"\n",
      "\n",
      " BLEU:  0.6351   0.3487   0.1965   0.1119   0.0655   0.0403   0.0263   0.0178   0.0120  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5470   6.5261   6.7456   6.8284   6.8557   6.8647   6.8686   6.8696   6.8702  \"tst\"\n",
      "\n",
      " BLEU:  0.6351   0.4706   0.3517   0.2641   0.1999   0.1531   0.1190   0.0938   0.0747  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 13 at 14:03:40\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2641\n",
      "NIST: 6.8557\n",
      "METEOR: 0.3567\n",
      "ROUGE_L: 0.5018\n",
      "CIDEr: 2.7679\n",
      "\n",
      "/data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/compare/Melbourne_nmt.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 43303 tokens at 308534.85 tokens per second.\n",
      "PTBTokenizer tokenized 39702 tokens at 241575.87 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.338\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.516\n",
      "computing CIDEr score...\n",
      "CIDEr: 3.025\n",
      "Creating temp directory  /tmp/e2e-eval-oum2wrh6\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 13 at 14:03:55\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-oum2wrh6/mteval_ref.sgm -s /tmp/e2e-eval-oum2wrh6/mteval_src.sgm -t /tmp/e2e-eval-oum2wrh6/mteval_sys.sgm -f /tmp/e2e-eval-oum2wrh6/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 1862 segs)\n",
      "    ref set \"e2e\" (1 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.3450  BLEU score = 0.2972 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.8623   1.0738   0.2628   0.1079   0.0383   0.0178   0.0090   0.0047   0.0027  \"tst\"\n",
      "\n",
      " BLEU:  0.6840   0.3949   0.2429   0.1566   0.1058   0.0745   0.0539   0.0395   0.0297  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.8623   6.9360   7.1988   7.3067   7.3450   7.3628   7.3718   7.3764   7.3791  \"tst\"\n",
      "\n",
      " BLEU:  0.6386   0.4852   0.3766   0.2972   0.2385   0.1942   0.1601   0.1333   0.1119  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 13 at 14:04:04\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.2972\n",
      "NIST: 7.3450\n",
      "METEOR: 0.3383\n",
      "ROUGE_L: 0.5160\n",
      "CIDEr: 3.0245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/compare/WebNLG_baseline.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/compare/UPF_pipeline.txt\n",
    "!/data/private/E2E/e2e-metrics/measure_scores.py /data/private/WebNLG-models/prediction/challenge/reference.txt /data/private/WebNLG-models/prediction/challenge/compare/Melbourne_nmt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력은 10 개의 고유 한 DBpedia 범주, 즉 Astronaut, University, City, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam 및 WrittenWork에 속하는 엔티티를 설명합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train: 6940, dev: 872, test: 1862"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "import json\n",
    "import xmltodict\n",
    "import glob\n",
    "        \n",
    "class webNLG_DATASET(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.category_list = []\n",
    "        self.modifiedtripleset_list = []\n",
    "        self.text_list = []            \n",
    "        \n",
    "        xml_folders = glob.glob(data_path+'*')\n",
    "        xml_folders.sort()\n",
    "        \n",
    "        for xml_folder in xml_folders:\n",
    "            xml_roots = xml_folder+'/*'\n",
    "            xml_files = glob.glob(xml_roots)\n",
    "            xml_files.sort()\n",
    "            \n",
    "            for xml_file in xml_files:        \n",
    "                with open(xml_file,'r') as f:\n",
    "                    xmlString = f.read()\n",
    "                dict_data = xmltodict.parse(xmlString)['benchmark']['entries']['entry']\n",
    "                if not isinstance(dict_data, list):\n",
    "                    dict_data = [dict_data]                \n",
    "\n",
    "                # challenge version\n",
    "                for i in range(len(dict_data)):\n",
    "                    y=dict_data[i]\n",
    "                    self.category_list.append(y['@category'])\n",
    "                    \n",
    "                    self.modifiedtripleset_list.append(y['modifiedtripleset']['mtriple'])\n",
    "                    z = y['lex']\n",
    "                    if isinstance(z, list):\n",
    "                        z = z[0]\n",
    "                    self.text_list.append(z['#text'])\n",
    "\n",
    "                \n",
    "                # version 2.0\n",
    "#                 for i in range(len(dict_data)):\n",
    "#                     y=dict_data[i]\n",
    "\n",
    "#                     self.category_list.append(y['@category'])\n",
    "\n",
    "#                     if 'test' in xml_file.split('/'):\n",
    "#                         self.modifiedtripleset_list.append(y['modifiedtripleset']['otriple'])\n",
    "#                     else:\n",
    "#                         self.modifiedtripleset_list.append(y['modifiedtripleset']['mtriple'])\n",
    "\n",
    "#                     z = y['lex']\n",
    "#                     if isinstance(z, list):\n",
    "#                         z = z[0]\n",
    "#                     self.text_list.append(z['text'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.category_list)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        triple_total = []\n",
    "        if isinstance(self.modifiedtripleset_list[idx], list):\n",
    "            for triple_list in self.modifiedtripleset_list[idx]:\n",
    "                triple_total += triple_list.split('|')\n",
    "        else:\n",
    "            triple_total += self.modifiedtripleset_list[idx].split('|')\n",
    "            \n",
    "        triple = [x.strip() for x in triple_total]\n",
    "        \n",
    "        return self.category_list[idx], triple, self.text_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "class webNLG_DATASET(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.category_list = []\n",
    "        self.modifiedtripleset_list = []\n",
    "        self.text_list = []            \n",
    "        \n",
    "        xml_files = glob.glob(data_path+'*')\n",
    "        for xml_file in xml_files:        \n",
    "            with open(xml_file,'r') as f:\n",
    "                xmlString = f.read()\n",
    "            dict_data = xmltodict.parse(xmlString)['benchmark']['entries']['entry']\n",
    "            if not isinstance(dict_data, list):\n",
    "                dict_data = [dict_data]                \n",
    "\n",
    "            # challenge version\n",
    "            for i in range(len(dict_data)):\n",
    "                y=dict_data[i]\n",
    "                self.category_list.append(y['@category'])\n",
    "\n",
    "                self.modifiedtripleset_list.append(y['modifiedtripleset']['mtriple'])\n",
    "                z = y['lex']\n",
    "                if isinstance(z, list):\n",
    "                    z = z[0]\n",
    "                self.text_list.append(z['#text'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.category_list)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        triple_total = []\n",
    "        if isinstance(self.modifiedtripleset_list[idx], list):\n",
    "            for triple_list in self.modifiedtripleset_list[idx]:\n",
    "                triple_total += triple_list.split('|')\n",
    "        else:\n",
    "            triple_total += self.modifiedtripleset_list[idx].split('|')\n",
    "            \n",
    "        triple = [x.strip() for x in triple_total]\n",
    "        \n",
    "        return self.category_list[idx], triple, self.text_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습버전\n",
    "class webNLG_DATASET(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.category_list = []\n",
    "        self.modifiedtripleset_list = []\n",
    "        self.text_list = []            \n",
    "        \n",
    "        xml_folders = glob.glob(data_path+'*')\n",
    "        xml_folders.sort()\n",
    "        \n",
    "        for xml_folder in xml_folders:\n",
    "            xml_roots = xml_folder+'/*'\n",
    "            xml_files = glob.glob(xml_roots)\n",
    "            xml_files.sort()\n",
    "            \n",
    "            for xml_file in xml_files:        \n",
    "                with open(xml_file,'r') as f:\n",
    "                    xmlString = f.read()\n",
    "                dict_data = xmltodict.parse(xmlString)['benchmark']['entries']['entry']\n",
    "                if not isinstance(dict_data, list):\n",
    "                    dict_data = [dict_data]                \n",
    "\n",
    "                # challenge version\n",
    "                for i in range(len(dict_data)):\n",
    "                    y=dict_data[i]\n",
    "                    self.category_list.append(y['@category'])\n",
    "                    \n",
    "                    self.modifiedtripleset_list.append(y['modifiedtripleset']['mtriple'])\n",
    "                    z = y['lex']\n",
    "                    if isinstance(z, list):\n",
    "                        z = z[0]\n",
    "                    self.text_list.append(z['#text'])\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.category_list)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        \n",
    "        return self.category_list[idx], self.modifiedtripleset_list[idx], self.text_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/data/private/dataset/webnlg/data/v2.0/en/train/'\n",
    "data_path = '/data/private/WebNLG-models/chimera-master/data/WebNLG/raw/train/'\n",
    "webNLG_data = webNLG_DATASET(data_path)\n",
    "dataloader = DataLoader(webNLG_data, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# data_path = '/data/private/WebNLG-models/chimera-master/data/WebNLG/raw/test/'\n",
    "# webNLG_data = webNLG_DATASET(data_path)\n",
    "# dataloader = DataLoader(webNLG_data, batch_size=1, shuffle=False, num_workers=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6940"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(webNLG_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Airport',\n",
       " ['Aarhus | leaderName | Jacob_Bundsgaard',\n",
       "  'Aarhus_Airport | cityServed | Aarhus'],\n",
       " \"Aarhus airport serves the city of Aarhus who's leader is Jacob Bundsgaard.\")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webNLG_data[1788]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " commander \n",
      " commander \n"
     ]
    }
   ],
   "source": [
    "webNLG_data[1788]\n",
    "for x in range(len(webNLG_data)):\n",
    "    data = webNLG_data[x]\n",
    "    \n",
    "    if isinstance(data[1], list):\n",
    "        for triple_set in data[1]:\n",
    "            triples = triple_set.split('|')\n",
    "            for triple in triples:\n",
    "                if '__' in triple:\n",
    "                    print(triple)\n",
    "    else:\n",
    "        triples = data[1].split('|')\n",
    "        for triple in triples:\n",
    "            if 'comma' in triple:\n",
    "                print(triple)        \n",
    "        \n",
    "        \n",
    "# c = 0\n",
    "# for i_batch, data in enumerate(dataloader):    \n",
    "# #     print(data)\n",
    "# #     break\n",
    "#     if c == 1788:\n",
    "#         print(data)\n",
    "#         break\n",
    "#     c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aarhus Airport'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Aarhus Airport'\n",
    "x = my_model.tokenizer.encode(s)\n",
    "my_model.tokenizer.decode(x[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_list = ['<tr1>', '<tr2>', '<tr3>']\n",
    "\n",
    "input_str = '<c> '\n",
    "input_str += cate[0]\n",
    "input_str += ' '\n",
    "\n",
    "\n",
    "triple_total = []\n",
    "if isinstance(tripleset, list):\n",
    "    for tripleset_temp in tripleset:\n",
    "        triple_total = tripleset_temp[0].split('|')\n",
    "        triple_list = [x.strip() for x in triple_total]        \n",
    "\n",
    "        for k in range(len(triple_list)):\n",
    "            triple = triple_list[k]\n",
    "            triple = triple_list[k]\n",
    "            input_str += tr_list[k] + ' '\n",
    "            input_str += triple.replace('_', ' ') # _이 너무 많음\n",
    "            input_str += ' '        \n",
    "else:\n",
    "    triple_total += tripleset[0].split('|')           \n",
    "    triple_list = [x.strip() for x in triple_total]   \n",
    "\n",
    "    for k in range(len(triple_list)):\n",
    "        triple = triple_list[k]\n",
    "        input_str += tr_list[k] + ' '\n",
    "        input_str += triple.replace('_', ' ') # _이 너무 많음\n",
    "        input_str += ' '\n",
    "\n",
    "input_str += '<S>'\n",
    "if text is not '':\n",
    "    input_str += ' '\n",
    "    input_str += text[0]\n",
    "\n",
    "input_str = input_str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<c> Airport <tr1> Aarhus <tr2> leaderName <tr3> Jacob Bundsgaard <tr1> Aarhus Airport <tr2> cityServed <tr3> Aarhus <S> Aarhus airport serves the city of Aarhus who's leader is Jacob Bundsgaard.\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripleset, triple_total\n",
    "input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /data/private/GPT/openai-gpt2/base/ and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "webmodel(\n",
       "  (model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50262, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50262, bias=False)\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import *\n",
    "my_model = webmodel().cuda()\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4, 50262])\n",
      "torch.Size([3, 4, 50262])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [1,2,3,4], [1,2,3,4]]).cuda()\n",
    "print(x.shape)\n",
    "\n",
    "token_type_ids1 = torch.tensor([[0,0,1,1], [0,0,1,1], [0,0,1,1]]).cuda()\n",
    "o1 = my_model.model(x, token_type_ids=token_type_ids1)[0]\n",
    "print(o1.shape)\n",
    "\n",
    "token_type_ids2 = torch.tensor([[0,0,2,2], [0,0,2,2], [0,0,2,2]]).cuda()\n",
    "o2 = my_model.model(x, token_type_ids=token_type_ids2)[0]\n",
    "print(o2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-33.3009, -32.5016, -34.6172,  ...,   2.2522,   3.1099,   0.6586],\n",
       "         [-40.5900, -42.5236, -41.2990,  ...,   4.3247,   4.8959,   0.9258],\n",
       "         [-60.6873, -60.5445, -58.5169,  ...,   5.2210,   6.1823,   1.2264],\n",
       "         [-67.4091, -67.0701, -64.2516,  ...,   5.8832,   6.6753,   1.5033]],\n",
       "        device='cuda:0', grad_fn=<SelectBackward>),\n",
       " tensor([[-33.3009, -32.5016, -34.6172,  ...,   2.2522,   3.1099,   0.6586],\n",
       "         [-40.5900, -42.5236, -41.2990,  ...,   4.3247,   4.8959,   0.9258],\n",
       "         [-47.6841, -50.1135, -46.8205,  ...,   4.6252,   5.5870,   0.9204],\n",
       "         [-64.3876, -67.1301, -62.2161,  ...,   5.2525,   6.8331,   1.2655]],\n",
       "        device='cuda:0', grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1[0], o2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6854245b25dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "wte = my_model.model.get_input_embeddings()\n",
    "a = torch.tensor([0]).cuda()\n",
    "print(wte(a))\n",
    "\n",
    "my_model.tokenizer.decode([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-d680a62d7bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mout_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit_feeding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i_batch, sample_batched in enumerate(dataloader):    \n",
    "    c+= 1\n",
    "\n",
    "    cate, triple, text = sample_batched\n",
    "    input_tensor = my_model.make_tensor(cate, triple, text)\n",
    "\n",
    "    out_logit = my_model.logit_feeding(input_tensor)\n",
    "    target_idx = my_model.tokenizer.encode(text[0])\n",
    "    target_len = len(target_idx)\n",
    "\n",
    "    label_idxs = torch.tensor(target_idx + [my_model.tokenizer.eos_token_id]) # (len)\n",
    "\n",
    "    loss = my_model.LM_loss(out_logit, target_len, label_idxs)      \n",
    "    \n",
    "    print(cate[0], triple, text[0])\n",
    "    if c == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50258, 16170,   634, 50259,    32,   283,  7537,    62, 16170,   634,\n",
       "           1748,    50,  8520,   366,    32,   283,  7537,    11, 16490,     1,\n",
       "          50257,   464,   317,   283,  7537,   318,   262,  9003,   286,   317,\n",
       "            283,  7537,    11, 16490,    13]], device='cuda:0'),\n",
       " tensor([  464,   317,   283,  7537,   318,   262,  9003,   286,   317,   283,\n",
       "          7537,    11, 16490,    13, 50256]),\n",
       " tensor([[50257,   464,   317,   283,  7537,   318,   262,  9003,   286,   317,\n",
       "            283,  7537,    11, 16490,    13]], device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor, label_idxs, input_tensor[:,-target_len-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 35, 50260]), 14, torch.Size([15]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_logit.shape, target_len, label_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 50260])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_logit = out_logit[:,-target_len-1:,:].squeeze(0) # (len, vocab_num)\n",
    "pred_logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aarhus Airport'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple[0][0].replace('_', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xml input (xml_to_json.xml):\n"
     ]
    }
   ],
   "source": [
    "# path = './data/v1.5/en/train/1triples/Airport.xml'\n",
    "# path = '/data/private/dataset/webnlg/data/v2.0/en/test/1triples/City.xml'\n",
    "# path = '/data/private/WebNLG-models/chimera-master/data/WebNLG/raw/train/1triples/1triple_allSolutions_Airport_train_challenge.xml'\n",
    "# path = '/data/private/WebNLG-models/chimera-master/data/WebNLG/raw/test/testdata_with_lex.xml'\n",
    "path = '/data/private/dataset/webnlg/webnlg-dataset/webnlg_challenge_2017/train/1triples/1triple_allSolutions_Airport_train_challenge.xml'\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    " \n",
    "with open(path,'r') as f:\n",
    "    xmlString = f.read()\n",
    "    \n",
    "print(\"xml input (xml_to_json.xml):\")\n",
    "# print(xmlString)\n",
    " \n",
    "jsonString = json.dumps(xmltodict.parse(xmlString), indent=4)\n",
    " \n",
    "# print(\"\\nJSON output(output.json):\")\n",
    "# print(jsonString)\n",
    " \n",
    "# with open(\"xml_to_json.json\", 'w') as f:\n",
    "#     f.write(jsonString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=xmltodict.parse(xmlString)\n",
    "len(x['benchmark']['entries']['entry'])\n",
    "# test: 1862"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category:  Airport\n",
      "modifiedtripleset:  Abilene,_Texas | country | United_States\n",
      "text:  Abilene, Texas is in the United States.\n",
      "\n",
      "category:  Airport\n",
      "modifiedtripleset:  Abilene,_Texas | isPartOf | Jones_County,_Texas\n",
      "text:  Abilene, Texas is part of Jones County, Texas.\n",
      "\n",
      "category:  Airport\n",
      "modifiedtripleset:  Abilene,_Texas | isPartOf | Taylor_County,_Texas\n",
      "text:  Abilene, Texas is part of Taylor County,Texas.\n",
      "\n",
      "category:  Airport\n",
      "modifiedtripleset:  Abilene,_Texas | isPartOf | Texas\n",
      "text:  Abilene, Texas is part of Texas.\n",
      "\n",
      "category:  Airport\n",
      "modifiedtripleset:  Abilene_Regional_Airport | 1st_runway_LengthFeet | 3678\n",
      "text:  The length of the 1st runway at Abilene Regional airport is 3678 feet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# challenge version\n",
    "for i in range(10, 15):\n",
    "    y=x['benchmark']['entries']['entry'][i]\n",
    "    print(\"category: \", y['@category'])\n",
    "#     print('entitymap: ', y['entitymap'])\n",
    "    print(\"modifiedtripleset: \", y['modifiedtripleset']['mtriple'])\n",
    "    z = y['lex']\n",
    "    if isinstance(z, list):\n",
    "        z = z[0]\n",
    "    \n",
    "#     print('reference: ', z['references'])\n",
    "    print('text: ', z['#text'])\n",
    "#     print(\"template: \", z['template'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['@category', '@eid', '@size', 'originaltripleset', 'modifiedtripleset', 'lex']),\n",
       " OrderedDict([('@comment', 'good'),\n",
       "              ('@lid', 'Id1'),\n",
       "              ('#text',\n",
       "               'The length of the 1st runway at Abilene Regional airport is 3678 feet.')]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.keys(),y['lex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "# challenge version\n",
    "max_len = 0\n",
    "for i in range(len(x['benchmark']['entries']['entry'])):\n",
    "    y=x['benchmark']['entries']['entry'][i]\n",
    "    z = y['lex']\n",
    "    if isinstance(z, list):\n",
    "        z = z[0]\n",
    "    text = z['#text']\n",
    "    temp_len = len(my_model.tokenizer.encode(text))\n",
    "    if temp_len > max_len:\n",
    "        max_len = temp_len\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category:  City\n",
      "modifiedtripleset:  Antioch,_California | isPartOf | Contra_Costa_County,_California\n",
      "text:  Antioch, is part of Contra Costa County in California.\n",
      "template:  AGENT-1 is part of PATIENT-1 .\n",
      "\n",
      "category:  City\n",
      "modifiedtripleset:  Arlington,_Texas | populationDensity | 1472.0 (inhabitants per square kilometre)\n",
      "text:  Arlington, Texas, has a population density, of 1472.0 inhabitants per square kilometre.\n",
      "template:  AGENT-1 has a population density ,  of PATIENT-1 .\n",
      "\n",
      "category:  City\n",
      "modifiedtripleset:  Atlanta | leader | Kasim_Reed\n",
      "text:  Kasim Reed is the leader in Atlanta.\n",
      "template:  PATIENT-1 is the leader in AGENT-1 .\n",
      "\n",
      "category:  City\n",
      "modifiedtripleset:  Atlantic_City,_New_Jersey | areaTotal | 44.125 (square kilometres)\n",
      "text:  Atlantic City, New Jersey has a total area of 44.125 (square kilometres).\n",
      "template:  AGENT-1 has a total area of PATIENT-1 .\n",
      "\n",
      "category:  City\n",
      "modifiedtripleset:  Atlantic_City,_New_Jersey | isPartOf | New_Jersey\n",
      "text:  Atlantic City is part of New Jersey.\n",
      "template:  AGENT-1 is part of PATIENT-1 .\n",
      "\n",
      "category:  City\n",
      "modifiedtripleset:  Attica,_Indiana | elevationAboveTheSeaLevel | 166.0\n",
      "text:  Attica, in Indiana is 166.0 above sea level.\n",
      "template:  AGENT-1 is PATIENT-1 above sea level .\n",
      "\n",
      "category:  City\n",
      "modifiedtripleset:  Auburn,_Alabama | isPartOf | Lee_County,_Alabama\n",
      "text:  Auburn is part of Lee County in Alabama.\n",
      "template:  AGENT-1 is part of PATIENT-1 .\n",
      "\n",
      "category:  City\n",
      "modifiedtripleset:  Auburn,_Washington | areaTotal | 77.41 (square kilometres)\n",
      "text:  Auburn (Washington) has a total area of 77.4 square kilometres.\n",
      "template:  AGENT-1 has a total area of PATIENT-1 .\n",
      "\n",
      "category:  City\n",
      "modifiedtripleset:  Austin,_Texas | isPartOf | Texas\n",
      "text:  Austin is located in Texas.\n",
      "template:  AGENT-1 is located in PATIENT-1 .\n",
      "\n",
      "category:  City\n",
      "modifiedtripleset:  California | language | Spanish_language\n",
      "text:  Spanish is spoken in California.\n",
      "template:  PATIENT-1 is spoken in AGENT-1 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# version 2.0\n",
    "for i in range(10, 20):\n",
    "    y=x['benchmark']['entries']['entry'][i]\n",
    "    print(\"category: \", y['@category'])\n",
    "#     print('entitymap: ', y['entitymap'])\n",
    "    if 'train' in path.split('/'):\n",
    "        print(\"modifiedtripleset: \", y['modifiedtripleset']['mtriple'])    \n",
    "    else:\n",
    "        print(\"modifiedtripleset: \", y['modifiedtripleset']['otriple'])\n",
    "    z = y['lex']\n",
    "    if isinstance(z, list):\n",
    "        z = z[0]\n",
    "    \n",
    "#     print('reference: ', z['references'])\n",
    "    print('text: ', z['text'])\n",
    "    print(\"template: \", z['template'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['@category', '@eid', '@size', 'originaltripleset', 'modifiedtripleset', 'lex', 'entitymap']),\n",
       " odict_keys(['@comment', '@lid', 'tree', 'sortedtripleset', 'references', 'text', 'template']),\n",
       " OrderedDict([('otriple',\n",
       "               'Albuquerque,_New_Mexico | leaderTitle | New_Mexico_Senate')]),\n",
       " OrderedDict([('otriple',\n",
       "               'Albuquerque,_New_Mexico | leaderTitle | New_Mexico_Senate')]),\n",
       " OrderedDict([('sentence',\n",
       "               OrderedDict([('@ID', '1'),\n",
       "                            ('striple',\n",
       "                             'Albuquerque,_New_Mexico | leaderTitle | New_Mexico_Senate')]))]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.keys(), z.keys(), y['originaltripleset'], y['modifiedtripleset'], z['sortedtripleset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category:  Airport\n",
      "modifiedtripleset:  Aarhus_Airport | cityServed | \"Aarhus, Denmark\"\n",
      "reference:  The Aarhus\n",
      "reference:  Aarhus , Denmark\n",
      "text:  The Aarhus is the airport of Aarhus, Denmark.\n",
      "template:  AGENT-1 is the airport of PATIENT-1 .\n",
      "lexicalization:  AGENT-1 VP[aspect=simple,tense=present,voice=active,person=3rd,number=singular] be DT[form=defined] the airport of PATIENT-1 .\n",
      "\n",
      "category:  Airport\n",
      "modifiedtripleset:  Aarhus_Airport | cityServed | Aarhus\n",
      "reference:  Aarhus airport\n",
      "reference:  Aarhus\n",
      "text:  Aarhus airport serves the city of Aarhus.\n",
      "template:  AGENT-1 serves the city of PATIENT-1 .\n",
      "lexicalization:  AGENT-1 VP[aspect=simple,tense=present,voice=active,person=3rd,number=null] serve DT[form=defined] the city of PATIENT-1 .\n",
      "\n",
      "category:  Airport\n",
      "modifiedtripleset:  Aarhus_Airport | elevationAboveTheSeaLevel_(in_metres) | 25.0\n",
      "reference:  Aarhus Airport\n",
      "reference:  25 metres\n",
      "text:  Aarhus Airport is 25 metres above sea level.\n",
      "template:  AGENT-1 is PATIENT-1 above sea level .\n",
      "lexicalization:  AGENT-1 VP[aspect=simple,tense=present,voice=active,person=3rd,number=singular] be PATIENT-1 above sea level .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# version 1.5\n",
    "for i in range(3):\n",
    "    y=x['benchmark']['entries']['entry'][i]\n",
    "    print(\"category: \", y['@category'])\n",
    "#     print(\"originaltripleset: \", y['originaltripleset']['otriple'])\n",
    "    print(\"modifiedtripleset: \", y['modifiedtripleset']['mtriple'])\n",
    "#     print(\"entitymap :\", y['entitymap'])    \n",
    "    z = y['lex']\n",
    "    if isinstance(z, list):\n",
    "        z = z[0]\n",
    "#     print(z['@comment'], z['@lid'], z['sortedtripleset']['sentence']['striple'], z['text'], z['template'], z['lexicalization'])\n",
    "#     print(\"striple: \", z['sortedtripleset']['sentence']['striple']) # 학습 데이터만\n",
    "    \n",
    "    if isinstance(z['references']['reference'], list):\n",
    "        for w in z['references']['reference']:\n",
    "            print('reference: ', w['#text'])\n",
    "    else:\n",
    "        print('reference: ', z['references']['reference']['#text'])\n",
    "    print(\"text: \", z['text'])\n",
    "    print(\"template: \", z['template'])\n",
    "    print(\"lexicalization: \", z['lexicalization'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('@comment', 'good'),\n",
       "              ('@lid', 'Id1'),\n",
       "              ('tree',\n",
       "               [None,\n",
       "                '(SENTENCES (S (NP-SUBJ (TAG AGENT-1)) (VP[aspect=simple,tense=present,voice=active,person=3rd,number=singular] (VB be) (NP (NP (NN part)) (PP (IN of) (NP (DT[form=defined] the) (NNP U.S.)))))))']),\n",
       "              ('sortedtripleset',\n",
       "               OrderedDict([('sentence',\n",
       "                             OrderedDict([('@ID', '1'),\n",
       "                                          ('striple',\n",
       "                                           'Albany,_Oregon | isPartOf | United_States')]))])),\n",
       "              ('references', None),\n",
       "              ('text', 'Albany Oregon is part of the U.S.'),\n",
       "              ('template', 'AGENT-1 is part of PATIENT-1 .')]),\n",
       " OrderedDict([('@comment', 'good'),\n",
       "              ('@lid', 'Id2'),\n",
       "              ('tree',\n",
       "               [None,\n",
       "                '(SENTENCES (S (NP-SUBJ (TAG AGENT-1)) (VP[aspect=simple,tense=present,voice=active,person=3rd,number=singular] (VB be) (NP (NP (NN part)) (PP (IN of) (NP (TAG PATIENT-1))))) (. .)))']),\n",
       "              ('sortedtripleset',\n",
       "               OrderedDict([('sentence',\n",
       "                             OrderedDict([('@ID', '1'),\n",
       "                                          ('striple',\n",
       "                                           'Albany,_Oregon | isPartOf | United_States')]))])),\n",
       "              ('references', None),\n",
       "              ('text', 'Albany, Oregon is part of the United States.'),\n",
       "              ('template', 'AGENT-1 is part of PATIENT-1 .')])]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['@category', '@eid', '@size', 'originaltripleset', 'modifiedtripleset', 'lex', 'entitymap'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x['benchmark']['entries']['entry'][1]\n",
    "y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@category Airport\n",
      "@eid Id2\n",
      "@size 1\n",
      "originaltripleset OrderedDict([('otriple', 'Aarhus_Airport | city | Aarhus')])\n",
      "modifiedtripleset OrderedDict([('mtriple', 'Aarhus_Airport | cityServed | Aarhus')])\n",
      "lex OrderedDict([('@comment', 'good'), ('@lid', 'Id1'), ('sortedtripleset', OrderedDict([('sentence', OrderedDict([('@ID', '1'), ('striple', 'Aarhus_Airport | cityServed | Aarhus')]))])), ('references', OrderedDict([('reference', [OrderedDict([('@entity', 'Aarhus_Airport'), ('@number', '1'), ('@tag', 'AGENT-1'), ('@type', 'name'), ('#text', 'Aarhus airport')]), OrderedDict([('@entity', 'Aarhus'), ('@number', '2'), ('@tag', 'PATIENT-1'), ('@type', 'name'), ('#text', 'Aarhus')])])])), ('text', 'Aarhus airport serves the city of Aarhus.'), ('template', 'AGENT-1 serves the city of PATIENT-1 .'), ('lexicalization', 'AGENT-1 VP[aspect=simple,tense=present,voice=active,person=3rd,number=null] serve DT[form=defined] the city of PATIENT-1 .')])\n",
      "entitymap OrderedDict([('entity', ['AGENT-1 | Aarhus_Airport', 'PATIENT-1 | Aarhus'])])\n"
     ]
    }
   ],
   "source": [
    "for k, v in y.items():    \n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aarhus_Airport | city | Aarhus'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['originaltripleset']['otriple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aarhus_Airport | cityServed | Aarhus'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['modifiedtripleset']['mtriple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['@comment', '@lid', 'sortedtripleset', 'references', 'text', 'template', 'lexicalization'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y['lex']\n",
    "z.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('good',\n",
       " 'Id1',\n",
       " 'Aarhus_Airport | cityServed | Aarhus',\n",
       " 'Aarhus airport serves the city of Aarhus.',\n",
       " 'AGENT-1 serves the city of PATIENT-1 .',\n",
       " 'AGENT-1 VP[aspect=simple,tense=present,voice=active,person=3rd,number=null] serve DT[form=defined] the city of PATIENT-1 .')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z['@comment'], z['@lid'], z['sortedtripleset']['sentence']['striple'], z['text'], z['template'], z['lexicalization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('entity', ['AGENT-1 | Aarhus_Airport', 'PATIENT-1 | Aarhus'])])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['entitymap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

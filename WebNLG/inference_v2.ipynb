{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "import json\n",
    "import xmltodict\n",
    "import glob\n",
    "        \n",
    "class webNLG_DATASET(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.category_list = []\n",
    "        self.modifiedtripleset_list = []\n",
    "        self.text_list = []            \n",
    "        \n",
    "        xml_files = glob.glob(data_path+'*')\n",
    "        for xml_file in xml_files:        \n",
    "            with open(xml_file,'r') as f:\n",
    "                xmlString = f.read()\n",
    "            dict_data = xmltodict.parse(xmlString)['benchmark']['entries']['entry']\n",
    "            if not isinstance(dict_data, list):\n",
    "                dict_data = [dict_data]                \n",
    "\n",
    "            # challenge version\n",
    "            for i in range(len(dict_data)):\n",
    "                y=dict_data[i]\n",
    "                self.category_list.append(y['@category'])\n",
    "\n",
    "                self.modifiedtripleset_list.append(y['modifiedtripleset']['mtriple'])\n",
    "                z = y['lex']\n",
    "                if isinstance(z, list):\n",
    "                    z = z[0]\n",
    "                self.text_list.append(z['#text'])\n",
    "\n",
    "                \n",
    "                # version 2.0\n",
    "#                 for i in range(len(dict_data)):\n",
    "#                     y=dict_data[i]\n",
    "\n",
    "#                     self.category_list.append(y['@category'])\n",
    "\n",
    "#                     if 'test' in xml_file.split('/'):\n",
    "#                         self.modifiedtripleset_list.append(y['modifiedtripleset']['otriple'])\n",
    "#                     else:\n",
    "#                         self.modifiedtripleset_list.append(y['modifiedtripleset']['mtriple'])\n",
    "\n",
    "#                     z = y['lex']\n",
    "#                     if isinstance(z, list):\n",
    "#                         z = z[0]\n",
    "#                     self.text_list.append(z['text'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.category_list)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        triple_total = []\n",
    "        if isinstance(self.modifiedtripleset_list[idx], list):\n",
    "            for triple_list in self.modifiedtripleset_list[idx]:\n",
    "                triple_total += triple_list.split('|')\n",
    "        else:\n",
    "            triple_total += self.modifiedtripleset_list[idx].split('|')\n",
    "            \n",
    "        triple = [x.strip() for x in triple_total]\n",
    "        \n",
    "        return self.category_list[idx], triple, self.text_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/data/private/dataset/webnlg/data/v2.0/en/train/'\n",
    "data_path = '/data/private/WebNLG-models/chimera-master/data/WebNLG/raw/test/'\n",
    "webNLG_data = webNLG_DATASET(data_path)\n",
    "dataloader = DataLoader(webNLG_data, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /data/private/GPT/openai-gpt2/base/ and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "my_model = webmodel().cuda()\n",
    "model_path = '/data/private/WebNLG-models/simple_model/pretrained/try_1/1'\n",
    "my_model.load_state_dict(torch.load(model_path + '/model.bin'))\n",
    "my_model.eval()\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([50256, 50259, 50256, 50257, 50258],\n",
       " ['<|endoftext|>', '<tr>', '<|endoftext|>', '<S>', '<c>'],\n",
       " 50257)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.tokenizer.all_special_ids, my_model.tokenizer.all_special_tokens, my_model.tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5a7bf4f46a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwebNLG_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-260f72df8c1c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtriple_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodifiedtripleset_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtriple_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodifiedtripleset_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mtriple_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtriple_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "webNLG_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cate:  Airport\n",
      "triple:  [('Abilene_Regional_Airport',), ('cityServed',), ('Abilene,_Texas',)]\n",
      "text:  Abilene, Texas is served by the Abilene regional airport.\n",
      "#####################################\n",
      "Response text:  Abilene Regional Airport serves the city of Abilene, Texas. The airport is located in the city of Abilene, Texas and is located in the city of Abilene, Texas\n",
      "\n",
      "cate:  Airport\n",
      "triple:  [('Adolfo_Suárez_Madrid–Barajas_Airport',), ('location',), ('\"Madrid, Paracuellos de Jarama, San Sebastián de los Reyes and Alcobendas\"',)]\n",
      "text:  Adolfo Suárez Madrid–Barajas Airport can be found in Madrid, Paracuellos de Jarama, San Sebastián de los Reyes and Alcobendas.\n",
      "#####################################\n",
      "Response text:  Adolfo Suárez Madrid–Barajas Airport is located in the city of Madrid, Paracuellos de Jarama, San Sebastián de los Reyes and Alcobendas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    k += 1\n",
    "    cate, triple, text = sample_batched\n",
    "    print('cate: ', cate[0])\n",
    "    print('triple: ', triple)\n",
    "    print('text: ', text[0])\n",
    "    print('#####################################')       \n",
    "    \n",
    "    input_tensor = my_model.make_tensor(cate, triple, '').squeeze(0)\n",
    "    \n",
    "    response = my_model.generate(input_tensor)\n",
    "    \n",
    "#     print(k)\n",
    "#     print(\"Target text: \", target_sentence)\n",
    "    print(\"Response text: \", response)\n",
    "    print('')    \n",
    "\n",
    "    if k == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/private/dataset/webnlg/data/v2.0/en/test/1triples',\n",
       " '/data/private/dataset/webnlg/data/v2.0/en/test/2triples',\n",
       " '/data/private/dataset/webnlg/data/v2.0/en/test/3triples',\n",
       " '/data/private/dataset/webnlg/data/v2.0/en/test/4triples',\n",
       " '/data/private/dataset/webnlg/data/v2.0/en/test/5triples',\n",
       " '/data/private/dataset/webnlg/data/v2.0/en/test/6triples',\n",
       " '/data/private/dataset/webnlg/data/v2.0/en/test/7triples']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_folders = glob.glob(data_path+'*')\n",
    "xml_folders.sort()\n",
    "xml_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml_folder in xml_folders:\n",
    "    xml_roots = xml_folder+'/*'\n",
    "    xml_files = glob.glob(xml_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/private/WebNLG-models/simple_model/pretrained/try_1/runs\n",
      "/data/private/WebNLG-models/simple_model/pretrained/try_1/1\n",
      "/data/private/WebNLG-models/simple_model/pretrained/try_1/2\n",
      "/data/private/WebNLG-models/simple_model/pretrained/try_1/3\n",
      "/data/private/WebNLG-models/simple_model/pretrained/try_1/4\n",
      "/data/private/WebNLG-models/simple_model/pretrained/try_1/5\n",
      "/data/private/WebNLG-models/simple_model/pretrained/try_1/6\n"
     ]
    }
   ],
   "source": [
    "model_pathes = '/data/private/WebNLG-models/simple_model/pretrained/try_1/*'\n",
    "model_folders = glob.glob(model_pathes)\n",
    "\n",
    "for model_folder in model_folders:\n",
    "    print(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('prediction/prediction_1.txt')\n",
    "f2 = open('prediction/reference.txt')\n",
    "texts = f.readlines()\n",
    "refs = f2.readlines()\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in range(len(refs)):\n",
    "    text = texts[i]\n",
    "    ref = refs[i]\n",
    "    x1 = my_model.tokenizer.encode(text.strip())\n",
    "    x2 = my_model.tokenizer.encode(ref.strip())\n",
    "    if len(x2) > max_len:\n",
    "        max_len = len(x2)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.tokenizer.encode('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50258, 50256, 50257, 50256, 50259]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.END_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50256, '<|endoftext|>')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.tokenizer.eos_token_id,my_model.tokenizer.decode(50256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('prediction/reference.txt')\n",
    "texts = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('prediction/enter_reference.txt', 'w')\n",
    "for text in texts:\n",
    "    f2.write(text+'\\n')\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('prediction/prediction_1.txt')\n",
    "texts = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('prediction/modify_prediction_1.txt', 'w')\n",
    "for text in texts:\n",
    "    f2.write(text.replace('_', ' ').replace('@',''))\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

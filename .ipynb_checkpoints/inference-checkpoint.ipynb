{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class e2eDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file)\n",
    "        self.columns = self.dataset.columns\n",
    "        self.conditions = self.dataset[self.columns[0]]\n",
    "        self.sentences = self.dataset[self.columns[1]]\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.typ_list = {}\n",
    "        for k in range(len(self.conditions)):\n",
    "            cond_set = self.conditions[k].split(',')\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] in self.typ_list.keys():\n",
    "                    self.typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "                else:            \n",
    "                    self.typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conditions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cond = self.conditions[idx]\n",
    "        cond_set = cond.split(',')\n",
    "        condition_string = ''\n",
    "        for m in range(len(cond_set)):\n",
    "            cond_set[m] = cond_set[m].strip()\n",
    "            pos = cond_set[m].index('[')\n",
    "            \n",
    "            condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        \n",
    "        sen = self.sentences[idx]\n",
    "        input_string = condition_string + '<START>'\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(input_string, add_special_tokens=True))\n",
    "        \n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        return input_ids, sen, condition_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0428 05:26:55.763806 140426112489280 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0428 05:26:55.764722 140426112489280 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0428 05:26:55.827636 140426112489280 tokenization_utils.py:663] Adding <START> to the vocabulary\n",
      "I0428 05:26:55.828409 140426112489280 tokenization_utils.py:741] Assigning <START> to the bos_token key of the tokenizer\n",
      "I0428 05:26:55.828986 140426112489280 tokenization_utils.py:663] Adding <name> to the vocabulary\n",
      "I0428 05:26:55.829456 140426112489280 tokenization_utils.py:663] Adding <eatType> to the vocabulary\n",
      "I0428 05:26:55.829906 140426112489280 tokenization_utils.py:663] Adding <priceRange> to the vocabulary\n",
      "I0428 05:26:55.830352 140426112489280 tokenization_utils.py:663] Adding <customer rating> to the vocabulary\n",
      "I0428 05:26:55.830796 140426112489280 tokenization_utils.py:663] Adding <near> to the vocabulary\n",
      "I0428 05:26:55.831223 140426112489280 tokenization_utils.py:663] Adding <food> to the vocabulary\n",
      "I0428 05:26:55.831658 140426112489280 tokenization_utils.py:663] Adding <area> to the vocabulary\n",
      "I0428 05:26:55.832089 140426112489280 tokenization_utils.py:663] Adding <familyFriendly> to the vocabulary\n",
      "I0428 05:26:55.832630 140426112489280 tokenization_utils.py:741] Assigning ['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>'] to the additional_special_tokens key of the tokenizer\n",
      "I0428 05:26:56.612224 140426112489280 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/ds_user1/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5\n",
      "I0428 05:26:56.613286 140426112489280 configuration_utils.py:311] Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0428 05:26:57.916490 140426112489280 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "my_model.load_state_dict(torch.load('./gen_model/base/2/model'))\n",
    "print('ok') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-10a71199b297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# e2e_dataset_test2 = e2eDataset(csv_file='dataset/testset.csv', tokenizer=my_model.tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# e2e_dataset_train = e2eDataset(csv_file='dataset/trainset.csv', tokenizer=my_model.tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0me2e_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me2eDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dataset/devset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2e_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "# e2e_dataset_test = e2eDataset(csv_file='dataset/testset_w_refs.csv', tokenizer=my_model.tokenizer)\n",
    "# e2e_dataset_test2 = e2eDataset(csv_file='dataset/testset.csv', tokenizer=my_model.tokenizer)\n",
    "# e2e_dataset_train = e2eDataset(csv_file='dataset/trainset.csv', tokenizer=my_model.tokenizer)\n",
    "e2e_dataset = e2eDataset(csv_file='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=False, num_workers=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42061, 4672, 4693)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e2e_dataset_train), len(e2e_dataset), len(e2e_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 34\n",
      "eatType 3\n",
      "priceRange 6\n",
      "customer rating 6\n",
      "near 19\n",
      "food 7\n",
      "area 2\n",
      "familyFriendly 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'city centre', 'riverside'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in e2e_dataset_train.typ_list.items():\n",
    "    print(k, len(v))\n",
    "e2e_dataset_train.typ_list['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50258, 50259, 50263, 50260, 50261, 50264, 50265, 50262]\n",
      "['<name>', '<|endoftext|>', '<area>', '<near>', '<START>', '<customer rating>', '<familyFriendly>', '<food>', '<eatType>', '<priceRange>']\n",
      "<name>\n",
      "<eatType>\n",
      "<food>\n",
      "<priceRange>\n",
      "<customer rating>\n",
      "<area>\n",
      "<familyFriendly>\n",
      "<near>\n"
     ]
    }
   ],
   "source": [
    "# x = my_model.tokenizer.additional_special_tokens_ids\n",
    "x = [50258, 50259, 50263, 50260, 50261, 50264, 50265, 50262]\n",
    "print(x)\n",
    "print(my_model.tokenizer.all_special_tokens)\n",
    "for i in range(len(x)):\n",
    "    print(my_model.tokenizer.decode([x[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4672it [00:03, 1311.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "same_condition = []\n",
    "ref_sentences = []\n",
    "input_ids_list = []\n",
    "pre_condition_string = ''\n",
    "start = 0\n",
    "for i_batch, sample_batched in tqdm(enumerate(dataloader)):\n",
    "    sen = sample_batched[1][0]\n",
    "#     print(i_batch, sen)\n",
    "    condition_string = sample_batched[2]  \n",
    "    input_ids = sample_batched[0].squeeze(0).cuda()\n",
    "        \n",
    "    if start == 0 or condition_string == pre_condition_string:      \n",
    "        if start == 0:\n",
    "            input_ids_list.append(input_ids)\n",
    "        same_condition.append(sen)        \n",
    "        pre_condition_string = condition_string\n",
    "        start += 1\n",
    "    else:   \n",
    "        input_ids_list.append(input_ids)\n",
    "        ref_sentences.append(same_condition)\n",
    "        pre_condition_string = condition_string\n",
    "        same_condition = [sen]\n",
    "        start += 1\n",
    "\n",
    "#     if i_batch == 30:\n",
    "#         break            \n",
    "ref_sentences.append(same_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 547)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref_sentences), len(input_ids_list)\n",
    "# input_ids_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ds_user1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 73.41162457057993\n",
      "BLEU1 score: 89.30558590312106\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "max_len = 50\n",
    "start_time = time.time()\n",
    "bleu_score = 0\n",
    "bleu_1 = 0\n",
    "\n",
    "f_dev = open('./predictions/base/f_dev_2.txt', 'w')\n",
    "f_pred = open('./predictions/base/f_pred_2.txt', 'w')\n",
    "\n",
    "for k in range(len(ref_sentences)):\n",
    "    input_ids = input_ids_list[k]\n",
    "    input_len = len(input_ids)\n",
    "\n",
    "    ori_tokens = []\n",
    "    for m in range(len(ref_sentences[k])):\n",
    "        f_dev.write(ref_sentences[k][m]+'\\n')\n",
    "        ori_tokens.append(word_tokenize(ref_sentences[k][m]))\n",
    "    if k < len(ref_sentences)-1:\n",
    "        f_dev.write('\\n')\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "        pred_idx = model_out.argmax(1)[-1]        \n",
    "        if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "            break            \n",
    "        input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "    \n",
    "    out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "    f_pred.write(out_sen+'\\n')\n",
    "    \n",
    "#     print(ref_sentences[k])\n",
    "#     print(out_sen)    \n",
    "    \n",
    "    out_tokens = word_tokenize(out_sen)\n",
    "    \n",
    "    bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "    bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    bleu_1 += bleu_1_score\n",
    "\n",
    "    bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))    \n",
    "    \n",
    "#     print(\"time: {}\".format(time.time()-start_time))\n",
    "#     print('')\n",
    "f_dev.close()\n",
    "f_pred.close()\n",
    "    \n",
    "print(\"BLEU score: {}\".format(bleu_score/len(ref_sentences)*100))\n",
    "print(\"BLEU1 score: {}\".format(bleu_1/len(ref_sentences)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU score: 71.18825887456514\n",
    "BLEU1 score: 88.76109911863252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./measure_scores.py ../predictions/base/f_dev_6.txt ../predictions/base/f_pred_6.txt\n",
    "./measure_scores.py ../predictions/base2/f_dev_2.txt ../predictions/base2/f_pred_2.txt\n",
    "./measure_scores.py ../predictions/large/f_dev_5.txt ../predictions/large/f_pred_5.txt\n",
    "./measure_scores.py ../predictions/f_dev_2.txt ../predictions/large2/f_pred_2.txt\n",
    "\n",
    "### Large\n",
    "#### my_output_1\n",
    "BLEU: 0.6826\n",
    "NIST: 8.4243\n",
    "METEOR: 0.4557\n",
    "ROUGE_L: 0.7032\n",
    "CIDEr: 2.1234\n",
    "\n",
    "#### my_output_2\n",
    "BLEU: 0.7228\n",
    "NIST: 8.5241\n",
    "METEOR: 0.4851\n",
    "ROUGE_L: 0.7461\n",
    "CIDEr: 2.4645\n",
    "\n",
    "#### my_output_3\n",
    "BLEU: 0.7035\n",
    "NIST: 8.5937\n",
    "METEOR: 0.4700\n",
    "ROUGE_L: 0.7252\n",
    "CIDEr: 2.3310\n",
    "\n",
    "#### my_output_4\n",
    "BLEU: 0.6738\n",
    "NIST: 8.4018\n",
    "METEOR: 0.4576\n",
    "ROUGE_L: 0.7075\n",
    "CIDEr: 2.2172\n",
    "\n",
    "#### my_output_5\n",
    "BLEU: 0.6927\n",
    "NIST: 8.4429\n",
    "METEOR: 0.4662\n",
    "ROUGE_L: 0.7180\n",
    "CIDEr: 2.2729\n",
    "\n",
    "### base\n",
    "#### my_output_2\n",
    "BLEU: 0.6812\n",
    "NIST: 8.5491\n",
    "METEOR: 0.4442\n",
    "ROUGE_L: 0.7036\n",
    "CIDEr: 2.1261\n",
    "\n",
    "#### my_output_6\n",
    "BLEU: 0.6655\n",
    "NIST: 8.4830\n",
    "METEOR: 0.4475\n",
    "ROUGE_L: 0.6992\n",
    "CIDEr: 2.1077\n",
    "    \n",
    "#### my_output_final\n",
    "BLEU: 0.6529\n",
    "NIST: 8.3116\n",
    "METEOR: 0.4430\n",
    "ROUGE_L: 0.6842\n",
    "CIDEr: 1.9766\n",
    "\n",
    "### Pragmatically Informative Text Generation\n",
    "BLEU 68.60\n",
    "NIST 8.73\n",
    "METEOR 45.25\n",
    "R-L 70.82\n",
    "CIDEr 2.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 05:03:59.987068 139782770526016 file_utils.py:41] PyTorch version 1.2.0 available.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package punkt to /home/ds_user1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "I0429 05:04:04.541117 139782770526016 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0429 05:04:04.541805 139782770526016 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0429 05:04:04.745631 139782770526016 tokenization_utils.py:663] Adding <START> to the vocabulary\n",
      "I0429 05:04:04.747691 139782770526016 tokenization_utils.py:741] Assigning <START> to the bos_token key of the tokenizer\n",
      "I0429 05:04:04.749249 139782770526016 tokenization_utils.py:663] Adding <name> to the vocabulary\n",
      "I0429 05:04:04.749745 139782770526016 tokenization_utils.py:663] Adding <eatType> to the vocabulary\n",
      "I0429 05:04:04.750217 139782770526016 tokenization_utils.py:663] Adding <priceRange> to the vocabulary\n",
      "I0429 05:04:04.750642 139782770526016 tokenization_utils.py:663] Adding <customer rating> to the vocabulary\n",
      "I0429 05:04:04.751078 139782770526016 tokenization_utils.py:663] Adding <near> to the vocabulary\n",
      "I0429 05:04:04.751498 139782770526016 tokenization_utils.py:663] Adding <food> to the vocabulary\n",
      "I0429 05:04:04.751924 139782770526016 tokenization_utils.py:663] Adding <area> to the vocabulary\n",
      "I0429 05:04:04.752349 139782770526016 tokenization_utils.py:663] Adding <familyFriendly> to the vocabulary\n",
      "I0429 05:04:04.752815 139782770526016 tokenization_utils.py:741] Assigning ['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>'] to the additional_special_tokens key of the tokenizer\n",
      "I0429 05:04:05.517257 139782770526016 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/ds_user1/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5\n",
      "I0429 05:04:05.517954 139782770526016 configuration_utils.py:311] Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0429 05:04:06.289715 139782770526016 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4693it [00:07, 606.39it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th model\n",
      "BLEU score: 64.06577890339878\n",
      "BLEU1 score: 80.76299135672593\n",
      "ok\n",
      "2 th model\n",
      "BLEU score: 66.94586456684863\n",
      "BLEU1 score: 83.85634894016366\n",
      "ok\n",
      "3 th model\n",
      "BLEU score: 74.04349827072004\n",
      "BLEU1 score: 89.77832487132272\n",
      "ok\n",
      "4 th model\n",
      "BLEU score: 71.94864511185955\n",
      "BLEU1 score: 87.53228133602691\n",
      "ok\n",
      "5 th model\n",
      "BLEU score: 72.95271327225126\n",
      "BLEU1 score: 88.69737561484492\n",
      "ok\n",
      "6 th model\n",
      "BLEU score: 74.67766773638043\n",
      "BLEU1 score: 89.90922046049079\n",
      "ok\n",
      "7 th model\n",
      "BLEU score: 73.53966031724963\n",
      "BLEU1 score: 89.32159455457034\n",
      "ok\n",
      "8 th model\n",
      "BLEU score: 72.74866660947484\n",
      "BLEU1 score: 88.73244280078839\n",
      "ok\n",
      "9 th model\n",
      "BLEU score: 73.75337869790299\n",
      "BLEU1 score: 89.44104292271649\n",
      "ok\n",
      "10 th model\n",
      "BLEU score: 73.6591377216643\n",
      "BLEU1 score: 89.25078521016773\n"
     ]
    }
   ],
   "source": [
    "# from model_large import *\n",
    "from model import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "max_len = 70\n",
    "\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "\n",
    "for i in range(1, 11):\n",
    "    model_name = './gen_model/base1_sample_30/'+str(i)+'/model'\n",
    "    my_model.load_state_dict(torch.load(model_name))\n",
    "    print('ok') \n",
    "    if i == 1:\n",
    "#         e2e_dataset = e2eDataset(csv_file='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "        e2e_dataset = e2eDataset(csv_file='dataset/testset_w_refs.csv', tokenizer=my_model.tokenizer)\n",
    "        dataloader = DataLoader(e2e_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        same_condition = []\n",
    "        ref_sentences = []\n",
    "        input_ids_list = []\n",
    "        pre_condition_string = ''\n",
    "        start = 0\n",
    "        for i_batch, sample_batched in tqdm(enumerate(dataloader)):\n",
    "            sen = sample_batched[1][0]\n",
    "            condition_string = sample_batched[2]  \n",
    "            input_ids = sample_batched[0].squeeze(0).cuda()\n",
    "\n",
    "            if start == 0 or condition_string == pre_condition_string:      \n",
    "                if start == 0:\n",
    "                    input_ids_list.append(input_ids)\n",
    "                same_condition.append(sen)        \n",
    "                pre_condition_string = condition_string\n",
    "                start += 1\n",
    "            else:   \n",
    "                input_ids_list.append(input_ids)\n",
    "                ref_sentences.append(same_condition)\n",
    "                pre_condition_string = condition_string\n",
    "                same_condition = [sen]\n",
    "                start += 1\n",
    "        ref_sentences.append(same_condition)    \n",
    "\n",
    "    bleu_score = 0\n",
    "    bleu_1 = 0\n",
    "\n",
    "#     f_dev = open('./predictions/testset/large2/f_dev_'+str(i)+'.txt', 'w')\n",
    "#     f_pred = open('./predictions/devset/base4/f_pred_'+str(i)+'.txt', 'w')\n",
    "    f_pred = open('./predictions/joosung2/testset/base1_sample30/f_pred_'+str(i)+'.txt', 'w')\n",
    "\n",
    "    for k in range(len(ref_sentences)):\n",
    "        input_ids = input_ids_list[k]\n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        ori_tokens = []\n",
    "        for m in range(len(ref_sentences[k])):\n",
    "#             f_dev.write(ref_sentences[k][m]+'\\n')\n",
    "            ori_tokens.append(word_tokenize(ref_sentences[k][m]))\n",
    "#         if k < len(ref_sentences)-1:\n",
    "#             f_dev.write('\\n')\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "            pred_idx = model_out.argmax(1)[-1]        \n",
    "            if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "                break            \n",
    "            input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "\n",
    "        out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "        f_pred.write(out_sen+'\\n')\n",
    "\n",
    "        out_tokens = word_tokenize(out_sen)\n",
    "\n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "        bleu_1 += bleu_1_score\n",
    "\n",
    "        bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))    \n",
    "\n",
    "#     f_dev.close()\n",
    "    f_pred.close()\n",
    "    \n",
    "    print(i, \"th model\")\n",
    "    print(\"BLEU score: {}\".format(bleu_score/len(ref_sentences)*100))\n",
    "    print(\"BLEU1 score: {}\".format(bleu_1/len(ref_sentences)*100))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/project/work/E2E/predictions/testset/f_dev.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-84c505881e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhuman_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/project/work/E2E/predictions/testset/f_dev.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhuman_open\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mhuman_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuman_open\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mhuman_open\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/project/work/E2E/predictions/testset/f_dev.txt'"
     ]
    }
   ],
   "source": [
    "### BERT score with human reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_files = \"/project/work/E2E/predictions/joosung2/testset/f_dev.txt\"\n",
    "\n",
    "human_open = open(human_files, \"r\")\n",
    "human_dataset = human_open.readlines()\n",
    "human_open.close()\n",
    "\n",
    "human_references = []\n",
    "\n",
    "temp_reference = []\n",
    "for i in range(len(human_dataset)):\n",
    "    if human_dataset[i] == '\\n':\n",
    "        human_references.append(temp_reference)\n",
    "        temp_reference = []\n",
    "    else:\n",
    "        temp_reference.append(human_dataset[i].strip())\n",
    "human_references.append(temp_reference)\n",
    "human_compare = []\n",
    "for i in range(len(human_references)):\n",
    "    for k in range(len(human_references[i])):\n",
    "        human_compare.append(human_references[i][k])\n",
    "\n",
    "output_path = \"/project/work/E2E/predictions/joosung2/testset/base1_sample30/*\"\n",
    "pred_files = glob.glob(output_path)\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(pred_files)):    \n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        repeat_num = len(human_references[k])\n",
    "        for _ in range(repeat_num):\n",
    "            cands.append(out_sen)\n",
    "\n",
    "    P, R, F1 = score(cands, human_compare, lang='en', verbose=True)\n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    \n",
    "    score_list.append(BERT_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_files, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT score with human reference\n",
    "import csv\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_files = \"/project/work/E2E/predictions/testset/f_dev.txt\"\n",
    "\n",
    "human_open = open(human_files, \"r\")\n",
    "human_dataset = human_open.readlines()\n",
    "human_open.close()\n",
    "\n",
    "human_references = []\n",
    "\n",
    "temp_reference = []\n",
    "for i in range(len(human_dataset)):\n",
    "    if human_dataset[i] == '\\n':\n",
    "        human_references.append(temp_reference)\n",
    "        temp_reference = []\n",
    "    else:\n",
    "        temp_reference.append(human_dataset[i].strip())\n",
    "human_references.append(temp_reference)\n",
    "human_compare = []\n",
    "for i in range(len(human_references)):\n",
    "    for k in range(len(human_references[i])):\n",
    "        human_compare.append(human_references[i][k])\n",
    "\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/primary_txt/*\"\n",
    "pred_files = glob.glob(output_path)\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(pred_files)):    \n",
    "    cands = []\n",
    "    pred_data_open = open(pred_files[i], \"r\")\n",
    "    pred_data_dataset = pred_data_open.readlines()\n",
    "    pred_len = len(pred_data_dataset)\n",
    "    pred_data_open.close()\n",
    "    \n",
    "    for k in range(len(pred_data_dataset)):\n",
    "        out_sen = pred_data_dataset[k].strip()\n",
    "        repeat_num = len(human_references[k])\n",
    "        for _ in range(repeat_num):\n",
    "            cands.append(out_sen)\n",
    "\n",
    "    P, R, F1 = score(cands, human_compare, lang='en', verbose=True)\n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    \n",
    "    score_list.append(BERT_score)  \n",
    "print(pred_files, score_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/work/E2E/compared_system/system_outputs/primary_txt/adapt.txt 0.9224714525544057\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/forge3.txt 0.9277185035519018\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tgen.txt 0.9391622960808789\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/chen.txt 0.9121247716430608\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/sheff2.txt 0.9337284921863148\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tuda.txt 0.9388613972027826\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/forge1.txt 0.9296145912378093\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/dangnt.txt 0.9390964954615199\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw2.txt 0.9328836475052662\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/zhang.txt 0.931265755944889\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tr2.txt 0.927562745489242\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tnt2.txt 0.9302622135351544\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/slug.txt 0.9424661084606326\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/gong.txt 0.9403216674742911\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/zhaw1.txt 0.9322725058160762\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tr1.txt 0.9140929393287714\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/tnt1.txt 0.9303506949982764\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/nle.txt 0.9395513920775868\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/sheff1.txt 0.938225149765417\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/slug-alt.txt 0.9345733890039641\n",
      "/project/work/E2E/compared_system/system_outputs/primary_txt/harv.txt 0.9390245682645042\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred_files)):\n",
    "    print(pred_files[i], score_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_1.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 513639.39 tokens per second.\n",
      "PTBTokenizer tokenized 15792 tokens at 150837.49 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.366\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.598\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.622\n",
      "Creating temp directory  /tmp/e2e-eval-cma6krnj\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:08:24\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-cma6krnj/mteval_ref.sgm -s /tmp/e2e-eval-cma6krnj/mteval_src.sgm -t /tmp/e2e-eval-cma6krnj/mteval_sys.sgm -f /tmp/e2e-eval-cma6krnj/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.2271  BLEU score = 0.5599 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.8645   1.1786   0.5772   0.3603   0.2465   0.1145   0.0616   0.0397   0.0289  \"tst\"\n",
      "\n",
      " BLEU:  0.8680   0.6879   0.5111   0.3673   0.2441   0.1550   0.1024   0.0683   0.0443  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  4.8645   6.0431   6.6203   6.9806   7.2271   7.3417   7.4032   7.4429   7.4718  \"tst\"\n",
      "\n",
      " BLEU:  0.8399   0.7477   0.6515   0.5599   0.4711   0.3893   0.3202   0.2629   0.2149  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:08:37\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.5599\n",
      "NIST: 7.2271\n",
      "METEOR: 0.3661\n",
      "ROUGE_L: 0.5979\n",
      "CIDEr: 1.6215\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_2.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 566147.84 tokens per second.\n",
      "PTBTokenizer tokenized 18189 tokens at 165774.85 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.398\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.629\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.795\n",
      "Creating temp directory  /tmp/e2e-eval-745hrskh\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:08:57\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-745hrskh/mteval_ref.sgm -s /tmp/e2e-eval-745hrskh/mteval_src.sgm -t /tmp/e2e-eval-745hrskh/mteval_sys.sgm -f /tmp/e2e-eval-745hrskh/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 7.6376  BLEU score = 0.5777 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.1258   1.2315   0.6192   0.3866   0.2745   0.1314   0.0745   0.0441   0.0298  \"tst\"\n",
      "\n",
      " BLEU:  0.8700   0.6808   0.5100   0.3687   0.2514   0.1689   0.1237   0.0909   0.0670  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.1258   6.3573   6.9765   7.3631   7.6376   7.7690   7.8434   7.8876   7.9173  \"tst\"\n",
      "\n",
      " BLEU:  0.8700   0.7696   0.6709   0.5777   0.4891   0.4097   0.3452   0.2922   0.2481  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:09:11\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.5777\n",
      "NIST: 7.6376\n",
      "METEOR: 0.3982\n",
      "ROUGE_L: 0.6289\n",
      "CIDEr: 1.7949\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_3.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 550187.98 tokens per second.\n",
      "PTBTokenizer tokenized 17803 tokens at 167985.50 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.434\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.661\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.169\n",
      "Creating temp directory  /tmp/e2e-eval-ffa29e0l\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:09:32\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-ffa29e0l/mteval_ref.sgm -s /tmp/e2e-eval-ffa29e0l/mteval_src.sgm -t /tmp/e2e-eval-ffa29e0l/mteval_sys.sgm -f /tmp/e2e-eval-ffa29e0l/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.4258  BLEU score = 0.6515 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5015   1.4035   0.7163   0.4629   0.3417   0.1815   0.1078   0.0664   0.0431  \"tst\"\n",
      "\n",
      " BLEU:  0.9248   0.7579   0.5858   0.4387   0.3114   0.2230   0.1641   0.1199   0.0854  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5015   6.9049   7.6212   8.0842   8.4258   8.6073   8.7151   8.7815   8.8246  \"tst\"\n",
      "\n",
      " BLEU:  0.9248   0.8372   0.7433   0.6515   0.5621   0.4818   0.4131   0.3539   0.3022  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:09:45\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6515\n",
      "NIST: 8.4258\n",
      "METEOR: 0.4340\n",
      "ROUGE_L: 0.6613\n",
      "CIDEr: 2.1690\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_4.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 508935.86 tokens per second.\n",
      "PTBTokenizer tokenized 18831 tokens at 177105.13 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.440\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.665\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.070\n",
      "Creating temp directory  /tmp/e2e-eval-vfteomx4\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:10:05\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-vfteomx4/mteval_ref.sgm -s /tmp/e2e-eval-vfteomx4/mteval_src.sgm -t /tmp/e2e-eval-vfteomx4/mteval_sys.sgm -f /tmp/e2e-eval-vfteomx4/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.0912  BLEU score = 0.6279 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.3181   1.3216   0.6965   0.4433   0.3116   0.1626   0.0986   0.0590   0.0371  \"tst\"\n",
      "\n",
      " BLEU:  0.8986   0.7301   0.5631   0.4207   0.3016   0.2151   0.1635   0.1276   0.1024  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.3181   6.6397   7.3362   7.7795   8.0912   8.2538   8.3524   8.4114   8.4485  \"tst\"\n",
      "\n",
      " BLEU:  0.8986   0.8100   0.7175   0.6279   0.5423   0.4648   0.4004   0.3471   0.3030  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:10:19\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6279\n",
      "NIST: 8.0912\n",
      "METEOR: 0.4400\n",
      "ROUGE_L: 0.6646\n",
      "CIDEr: 2.0698\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_5.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 597853.29 tokens per second.\n",
      "PTBTokenizer tokenized 16754 tokens at 150618.81 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.433\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.682\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.065\n",
      "Creating temp directory  /tmp/e2e-eval-l6mr656j\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:10:37\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-l6mr656j/mteval_ref.sgm -s /tmp/e2e-eval-l6mr656j/mteval_src.sgm -t /tmp/e2e-eval-l6mr656j/mteval_sys.sgm -f /tmp/e2e-eval-l6mr656j/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.4245  BLEU score = 0.6459 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5245   1.3839   0.7263   0.4640   0.3258   0.1631   0.1015   0.0601   0.0387  \"tst\"\n",
      "\n",
      " BLEU:  0.9311   0.7598   0.5894   0.4426   0.3125   0.2224   0.1643   0.1248   0.0957  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5245   6.9084   7.6347   8.0987   8.4245   8.5876   8.6891   8.7492   8.7880  \"tst\"\n",
      "\n",
      " BLEU:  0.9177   0.8289   0.7363   0.6459   0.5570   0.4768   0.4087   0.3517   0.3038  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:10:50\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6459\n",
      "NIST: 8.4245\n",
      "METEOR: 0.4334\n",
      "ROUGE_L: 0.6820\n",
      "CIDEr: 2.0651\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_6.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 550400.28 tokens per second.\n",
      "PTBTokenizer tokenized 17931 tokens at 141894.69 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.445\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.687\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.218\n",
      "Creating temp directory  /tmp/e2e-eval-3bb6xa75\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:11:09\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-3bb6xa75/mteval_ref.sgm -s /tmp/e2e-eval-3bb6xa75/mteval_src.sgm -t /tmp/e2e-eval-3bb6xa75/mteval_sys.sgm -f /tmp/e2e-eval-3bb6xa75/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.4851  BLEU score = 0.6628 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4992   1.4032   0.7493   0.4844   0.3490   0.1838   0.1121   0.0684   0.0481  \"tst\"\n",
      "\n",
      " BLEU:  0.9241   0.7609   0.5994   0.4580   0.3373   0.2505   0.1953   0.1558   0.1267  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4992   6.9024   7.6517   8.1361   8.4851   8.6689   8.7810   8.8494   8.8976  \"tst\"\n",
      "\n",
      " BLEU:  0.9241   0.8385   0.7498   0.6628   0.5790   0.5036   0.4399   0.3863   0.3413  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:11:22\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6628\n",
      "NIST: 8.4851\n",
      "METEOR: 0.4452\n",
      "ROUGE_L: 0.6874\n",
      "CIDEr: 2.2183\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_7.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 509229.13 tokens per second.\n",
      "PTBTokenizer tokenized 17578 tokens at 151519.00 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.437\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.674\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.152\n",
      "Creating temp directory  /tmp/e2e-eval-49s0u3v2\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:11:41\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-49s0u3v2/mteval_ref.sgm -s /tmp/e2e-eval-49s0u3v2/mteval_src.sgm -t /tmp/e2e-eval-49s0u3v2/mteval_sys.sgm -f /tmp/e2e-eval-49s0u3v2/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.4257  BLEU score = 0.6492 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4808   1.4083   0.7377   0.4671   0.3319   0.1685   0.0977   0.0627   0.0450  \"tst\"\n",
      "\n",
      " BLEU:  0.9233   0.7545   0.5851   0.4383   0.3149   0.2288   0.1757   0.1379   0.1097  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4808   6.8890   7.6268   8.0939   8.4257   8.5942   8.6919   8.7546   8.7996  \"tst\"\n",
      "\n",
      " BLEU:  0.9220   0.8335   0.7404   0.6492   0.5616   0.4834   0.4183   0.3640   0.3186  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:11:56\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6492\n",
      "NIST: 8.4257\n",
      "METEOR: 0.4373\n",
      "ROUGE_L: 0.6745\n",
      "CIDEr: 2.1523\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_8.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 568451.31 tokens per second.\n",
      "PTBTokenizer tokenized 17302 tokens at 121477.65 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.435\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.675\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.141\n",
      "Creating temp directory  /tmp/e2e-eval-zv5uzr3w\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:12:15\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-zv5uzr3w/mteval_ref.sgm -s /tmp/e2e-eval-zv5uzr3w/mteval_src.sgm -t /tmp/e2e-eval-zv5uzr3w/mteval_sys.sgm -f /tmp/e2e-eval-zv5uzr3w/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.4274  BLEU score = 0.6446 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4827   1.3923   0.7396   0.4711   0.3417   0.1720   0.1032   0.0637   0.0447  \"tst\"\n",
      "\n",
      " BLEU:  0.9223   0.7508   0.5848   0.4394   0.3169   0.2282   0.1720   0.1340   0.1058  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.4827   6.8750   7.6146   8.0857   8.4274   8.5994   8.7027   8.7664   8.8111  \"tst\"\n",
      "\n",
      " BLEU:  0.9154   0.8259   0.7343   0.6446   0.5584   0.4805   0.4144   0.3596   0.3136  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:12:29\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6446\n",
      "NIST: 8.4274\n",
      "METEOR: 0.4346\n",
      "ROUGE_L: 0.6751\n",
      "CIDEr: 2.1406\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_9.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 511085.96 tokens per second.\n",
      "PTBTokenizer tokenized 16982 tokens at 141094.78 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.437\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.680\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.133\n",
      "Creating temp directory  /tmp/e2e-eval-9evbbaw0\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:12:49\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-9evbbaw0/mteval_ref.sgm -s /tmp/e2e-eval-9evbbaw0/mteval_src.sgm -t /tmp/e2e-eval-9evbbaw0/mteval_sys.sgm -f /tmp/e2e-eval-9evbbaw0/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.5537  BLEU score = 0.6525 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5459   1.4204   0.7547   0.4814   0.3513   0.1765   0.1074   0.0632   0.0387  \"tst\"\n",
      "\n",
      " BLEU:  0.9303   0.7612   0.5917   0.4465   0.3210   0.2308   0.1694   0.1265   0.0941  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5459   6.9664   7.7210   8.2024   8.5537   8.7302   8.8376   8.9009   8.9395  \"tst\"\n",
      "\n",
      " BLEU:  0.9230   0.8349   0.7424   0.6525   0.5653   0.4863   0.4178   0.3595   0.3095  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:13:02\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6525\n",
      "NIST: 8.5537\n",
      "METEOR: 0.4372\n",
      "ROUGE_L: 0.6803\n",
      "CIDEr: 2.1328\n",
      "\n",
      "./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_10.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 513422.52 tokens per second.\n",
      "PTBTokenizer tokenized 17236 tokens at 162901.65 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.437\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.677\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.139\n",
      "Creating temp directory  /tmp/e2e-eval-qngtgvnl\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Apr 29 at 06:13:22\n",
      "command line:  /project/work/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-qngtgvnl/mteval_ref.sgm -s /tmp/e2e-eval-qngtgvnl/mteval_src.sgm -t /tmp/e2e-eval-qngtgvnl/mteval_sys.sgm -f /tmp/e2e-eval-qngtgvnl/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.5105  BLEU score = 0.6518 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5150   1.4131   0.7533   0.4800   0.3491   0.1776   0.1067   0.0639   0.0426  \"tst\"\n",
      "\n",
      " BLEU:  0.9255   0.7558   0.5890   0.4443   0.3208   0.2323   0.1735   0.1317   0.1011  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5150   6.9281   7.6813   8.1614   8.5105   8.6881   8.7948   8.8587   8.9013  \"tst\"\n",
      "\n",
      " BLEU:  0.9223   0.8335   0.7415   0.6518   0.5653   0.4871   0.4201   0.3633   0.3150  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Apr 29 at 06:13:35\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6518\n",
      "NIST: 8.5105\n",
      "METEOR: 0.4373\n",
      "ROUGE_L: 0.6767\n",
      "CIDEr: 2.1388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_1.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_2.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_3.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_4.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_5.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_6.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_7.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_8.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_9.txt\n",
    "!./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/joosung2/testset/base1_sample30/f_pred_10.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base1_sample10_6.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base2_sample10_9.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/1base4_devtest_4.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt ./predictions/final/2base1_sample50_7.txt\n",
    "# !./e2e-metrics/measure_scores.py ./predictions/joosung2/testset/f_dev.txt /project/work/E2E/compared_system/system_outputs/primary_txt/harv.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tsv to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_path = \"/project/work/E2E/compared_system/system_outputs/primary/*\"\n",
    "comapred_files = glob.glob(output_path)\n",
    "\n",
    "for i in range(len(comapred_files)):\n",
    "    dataset = pd.read_csv(comapred_files[i], delimiter='\\t', header=None)\n",
    "    \n",
    "    name = comapred_files[i].split('/')[-1].split('.')[0]\n",
    "    txt_files = \"/project/work/E2E/compared_system/system_outputs/primary_txt/\"+name+\".txt\"\n",
    "    f = open(txt_files, \"w\")\n",
    "    gen_sentences = dataset[1]\n",
    "    \n",
    "    for k in range(1, len(gen_sentences)):\n",
    "        f.write(gen_sentences[k]+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "631"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gen_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero-shot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 00:01:39.181757 139917359150912 file_utils.py:41] PyTorch version 1.2.0 available.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I0429 00:01:42.668395 139917359150912 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/ds_user1/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0429 00:01:42.669237 139917359150912 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/ds_user1/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0429 00:01:42.733504 139917359150912 tokenization_utils.py:663] Adding <START> to the vocabulary\n",
      "I0429 00:01:42.734525 139917359150912 tokenization_utils.py:741] Assigning <START> to the bos_token key of the tokenizer\n",
      "I0429 00:01:42.735000 139917359150912 tokenization_utils.py:663] Adding <name> to the vocabulary\n",
      "I0429 00:01:42.735508 139917359150912 tokenization_utils.py:663] Adding <eatType> to the vocabulary\n",
      "I0429 00:01:42.736039 139917359150912 tokenization_utils.py:663] Adding <priceRange> to the vocabulary\n",
      "I0429 00:01:42.736518 139917359150912 tokenization_utils.py:663] Adding <customer rating> to the vocabulary\n",
      "I0429 00:01:42.736990 139917359150912 tokenization_utils.py:663] Adding <near> to the vocabulary\n",
      "I0429 00:01:42.737427 139917359150912 tokenization_utils.py:663] Adding <food> to the vocabulary\n",
      "I0429 00:01:42.737862 139917359150912 tokenization_utils.py:663] Adding <area> to the vocabulary\n",
      "I0429 00:01:42.738298 139917359150912 tokenization_utils.py:663] Adding <familyFriendly> to the vocabulary\n",
      "I0429 00:01:42.738757 139917359150912 tokenization_utils.py:741] Assigning ['<name>', '<eatType>', '<priceRange>', '<customer rating>', '<near>', '<food>', '<area>', '<familyFriendly>'] to the additional_special_tokens key of the tokenizer\n",
      "I0429 00:01:43.507485 139917359150912 configuration_utils.py:275] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/ds_user1/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5\n",
      "I0429 00:01:43.508177 139917359150912 configuration_utils.py:311] Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0429 00:01:44.302492 139917359150912 modeling_utils.py:503] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import *\n",
    "\n",
    "max_len = 70\n",
    "my_model = mymodel().cuda()\n",
    "my_model.eval()\n",
    "model_name = './gen_model/base_devtrain_4/4/model'\n",
    "my_model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<area>', '<familyFriendly>']\n",
      "['city centre', 'yes']\n",
      "The city centre has a family-friendly place called The Phoenix.\n",
      "The <area> has a family-friendly place called The Phoenix.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "csv_file='dataset/testset_w_refs.csv'\n",
    "dataset = pd.read_csv(csv_file)\n",
    "columns = dataset.columns\n",
    "conditions = dataset[columns[0]]\n",
    "        \n",
    "typ_list = {}\n",
    "for k in range(len(conditions)):\n",
    "    cond_set = conditions[k].split(',')\n",
    "    for m in range(len(cond_set)):\n",
    "        cond_set[m] = cond_set[m].strip()\n",
    "        pos = cond_set[m].index('[')\n",
    "        if cond_set[m][:pos] in typ_list.keys():\n",
    "            typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "        else:            \n",
    "            typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}   \n",
    "\n",
    "def sample_batch(tokenizer, cond_name, cond_set):\n",
    "    condition_string = ''\n",
    "    for m in range(len(cond_set)):\n",
    "        condition_string += cond_name[m] + cond_set[m] + ' '\n",
    "\n",
    "    input_string = condition_string + '<START>'\n",
    "    input_ids = torch.tensor(tokenizer.encode(input_string, add_special_tokens=True))\n",
    "\n",
    "    input_len = len(input_ids)\n",
    "\n",
    "    return input_ids, condition_string\n",
    "\n",
    "# <name> <eatType> <food> <priceRange> <customer rating> <area> <familyFriendly> <near>\n",
    "\n",
    "name = None #'<NAME>'\n",
    "eatType = None #'<EATTYPE>'\n",
    "food = None # '<FOOD>'\n",
    "priceRange = None # '<priceRange>'\n",
    "customer_rating = None # '<CUSTOMER_RATING>'\n",
    "area = '<area>' # None\n",
    "familyFriendly = 'yes' # None\n",
    "near = None #'<NEAR>'\n",
    "\n",
    "cond_list = []\n",
    "conditions = []\n",
    "if name is not None:\n",
    "    placeholder_name = random.choice(list(typ_list['name']))\n",
    "    cond_list.append('<name>')\n",
    "    conditions.append(placeholder_name)\n",
    "if eatType is not None:\n",
    "    placeholder_eatType = random.choice(list(typ_list['eatType']))\n",
    "    cond_list.append('<eatType>')\n",
    "    conditions.append(placeholder_eatType)\n",
    "if food is not None:\n",
    "    placeholder_food = random.choice(list(typ_list['food']))\n",
    "    cond_list.append('<food>')\n",
    "    conditions.append(placeholder_food)\n",
    "if priceRange is not None:\n",
    "    placeholder_priceRange = random.choice(list(typ_list['priceRange']))\n",
    "    cond_list.append('<priceRange>')\n",
    "    conditions.append(placeholder_priceRange)    \n",
    "if customer_rating is not None:\n",
    "    placeholder_customer_rating = random.choice(list(typ_list['customer rating']))\n",
    "    cond_list.append('<customer rating>')\n",
    "    conditions.append(placeholder_customer_rating)        \n",
    "if area is not None:\n",
    "    placeholder_area = random.choice(list(typ_list['area']))\n",
    "    cond_list.append('<area>')\n",
    "    conditions.append(placeholder_area)    \n",
    "if familyFriendly is not None:\n",
    "    cond_list.append('<familyFriendly>')\n",
    "    conditions.append(familyFriendly)            \n",
    "if near is not None:\n",
    "    placeholder_near = random.choice(list(typ_list['near']))\n",
    "    cond_list.append('<near>')\n",
    "    conditions.append(placeholder_near)        \n",
    "\n",
    "\n",
    "# del cond_name[2]\n",
    "# del conditions[2]\n",
    "\n",
    "sample = sample_batch(my_model.tokenizer, cond_list, conditions)\n",
    "\n",
    "input_ids = sample[0].cuda()\n",
    "condition_string = sample[1]  \n",
    "input_len = len(input_ids)\n",
    "\n",
    "max_len = 70\n",
    "for _ in range(max_len):\n",
    "    model_out = my_model.model_feeding(input_ids) # (batch, seq_len, emb_dim)\n",
    "    pred_idx = model_out.argmax(1)[-1]        \n",
    "    if pred_idx == my_model.tokenizer.eos_token_id:\n",
    "        break            \n",
    "    input_ids = torch.cat((input_ids, pred_idx.unsqueeze(0)), 0)        \n",
    "\n",
    "out_sen = my_model.tokenizer.decode(input_ids[input_len:])\n",
    "print(cond_list)\n",
    "print(conditions)\n",
    "print(out_sen)\n",
    "\n",
    "if name is not None:\n",
    "    out_sen = out_sen.replace(placeholder_name, name)\n",
    "if eatType is not None:\n",
    "    out_sen = out_sen.replace(placeholder_eatType, eatType)\n",
    "if food is not None:\n",
    "    out_sen = out_sen.replace(placeholder_food, food)\n",
    "if priceRange is not None:\n",
    "    out_sen = out_sen.replace(placeholder_priceRange, priceRange)\n",
    "if customer_rating is not None:\n",
    "    out_sen = out_sen.replace(placeholder_customer_rating, customer_rating)\n",
    "if area is not None:\n",
    "    out_sen = out_sen.replace(placeholder_area, area)  \n",
    "if near is not None:\n",
    "    out_sen = out_sen.replace(placeholder_near, near)\n",
    "print(out_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<name>The Vaults '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delexicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class e2eDataset(Dataset):\n",
    "    def __init__(self, csv_file1, csv_file2, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "        \"\"\"\n",
    "        self.dataset1 = pd.read_csv(csv_file1)\n",
    "        self.dataset2 = pd.read_csv(csv_file2)\n",
    "        \n",
    "        self.columns1 = self.dataset1.columns\n",
    "        self.columns2 = self.dataset2.columns\n",
    "        \n",
    "        self.conditions = list(self.dataset1[self.columns1[0]]) + list(self.dataset2[self.columns2[0]])\n",
    "        self.sentences = list(self.dataset1[self.columns1[1]]) + list(self.dataset2[self.columns2[1]])\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.typ_list = {}\n",
    "        for k in range(len(self.conditions)):\n",
    "            cond_set = self.conditions[k].split(',')\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] in self.typ_list.keys():\n",
    "                    self.typ_list[cond_set[m][:pos]].add(cond_set[m][pos+1:-1])\n",
    "                else:            \n",
    "                    self.typ_list[cond_set[m][:pos]] = {cond_set[m][pos+1:-1]}        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conditions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sen = self.sentences[idx]\n",
    "        \n",
    "        cond = self.conditions[idx]\n",
    "        cond_set = cond.split(',')        \n",
    "        condition_string = ''\n",
    "        \n",
    "        \n",
    "        p = random.random()\n",
    "\n",
    "        if p > 0.3: # 70%\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                pos = cond_set[m].index('[')\n",
    "\n",
    "                condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        else: # p <= 0.3 / 30%\n",
    "            nochange_list = ['priceRange', 'customer rating', 'familyFriendly']\n",
    "            for m in range(len(cond_set)):\n",
    "                cond_set[m] = cond_set[m].strip()\n",
    "                \n",
    "                pos = cond_set[m].index('[')\n",
    "                if cond_set[m][:pos] not in nochange_list:\n",
    "                    placeholder = '<' + cond_set[m][:pos] + '>'\n",
    "                    condition_string += placeholder + ' '\n",
    "                    sen = sen.replace(cond_set[m][pos+1:-1], placeholder)        \n",
    "                else:\n",
    "                    condition_string += '<' + cond_set[m][:pos] + '>' + cond_set[m][pos+1:-1] + ' '\n",
    "        \n",
    "        input_string = condition_string + '<START>' + sen\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(input_string, add_special_tokens=True))\n",
    "        \n",
    "        label_string = sen + ' <|endoftext|>'\n",
    "        label_ids = torch.tensor(self.tokenizer.encode(label_string, add_special_tokens=True))\n",
    "\n",
    "        return input_string, input_ids, label_ids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<name>Fitzbillies <priceRange>moderate <customer rating>1 out of 5 <familyFriendly>yes <near>Express by Holiday Inn <START>Fitzbillies is a kids friendly place located a few steps of the Express by Holiday Inn. Its prices are moderates, and it has  a poor rating  between its clients.\n"
     ]
    }
   ],
   "source": [
    "e2e_dataset = e2eDataset(csv_file1='dataset/trainset.csv', csv_file2='dataset/devset.csv', tokenizer=my_model.tokenizer)\n",
    "print(e2e_dataset[200][0])\n",
    "# print(e2e_dataset.typ_list.keys())\n",
    "# print(e2e_dataset.typ_list)\n",
    "# priceRange, customer rating, familyFriendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dev = open('f_dev.txt', 'r')\n",
    "f_pred = open('f_pred.txt', 'r')\n",
    "f_dev_dataset = f_dev.readlines()\n",
    "f_pred_dataset = f_pred.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5218, 547)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_dev_dataset), len(f_pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4672, 547)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[x for x in f_dev_dataset if x != '\\n']\n",
    "data2=[x for x in f_pred_dataset if x != '\\n']\n",
    "len(data), len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5218"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4672+546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dev = open('./e2e-metrics/example-inputs/devel-conc.txt', 'r')\n",
    "f_pred = open('./e2e-metrics/example-inputs/baseline-output.txt', 'r')\n",
    "f_dev_dataset = f_dev.readlines()\n",
    "f_pred_dataset = f_pred.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_dev_dataset), len(f_pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[x for x in f_dev_dataset if x != '\\n']\n",
    "data2=[x for x in f_pred_dataset if x != '\\n']\n",
    "len(data), len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15699480814634292"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "p=random.random()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

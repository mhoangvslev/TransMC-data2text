{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT score with human reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_path = '/DATA/joosung/controllable_english/evaluation/yelp/human/'\n",
    "neg_human_DRG = human_path + 'sentiment.test.0.humanDRG'\n",
    "neg_human_DRG_open = open(neg_human_DRG, \"r\")\n",
    "neg_human_DRG_dataset = neg_human_DRG_open.readlines()\n",
    "neg_human_DRG_open.close()\n",
    "\n",
    "pos_human_DRG = human_path + 'sentiment.test.1.humanDRG'\n",
    "pos_human_DRG_open = open(pos_human_DRG, \"r\")\n",
    "pos_human_DRG_dataset = pos_human_DRG_open.readlines()\n",
    "pos_human_DRG_open.close()\n",
    "\n",
    "neg_human_DUAL = human_path + 'sentiment.test.0.humanDUAL'\n",
    "neg_human_DUAL_open = open(neg_human_DUAL, \"r\")\n",
    "neg_human_DUAL_dataset = neg_human_DUAL_open.readlines()\n",
    "neg_human_DUAL_open.close()\n",
    "\n",
    "pos_human_DUAL = human_path + 'sentiment.test.1.humanDUAL'\n",
    "pos_human_DUAL_open = open(pos_human_DUAL, \"r\")\n",
    "pos_human_DUAL_dataset = pos_human_DUAL_open.readlines()\n",
    "pos_human_DUAL_open.close()\n",
    "\n",
    "DRG_refs = []\n",
    "neg_len = len(neg_human_DRG_dataset)\n",
    "pos_len = len(pos_human_DRG_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_DRG_dataset[k].split('\\t')[1]\n",
    "    DRG_refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_DRG_dataset[k].split('\\t')[1]\n",
    "    DRG_refs.append(ref_sen)\n",
    "\n",
    "DUAL_refs = []\n",
    "neg_len = len(neg_human_DUAL_dataset)\n",
    "pos_len = len(pos_human_DUAL_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_DUAL_dataset[k].split('\\t')[1]\n",
    "    DUAL_refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_DUAL_dataset[k].split('\\t')[1]\n",
    "    DUAL_refs.append(ref_sen)\n",
    "    \n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "model_name=[]\n",
    "score_list=[]\n",
    "for i in range(len(neg_out_files)):    \n",
    "    cands = []\n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "\n",
    "    P, R, DRG_F1 = score(cands, DRG_refs, lang='en', verbose=True)\n",
    "    DRG_F1_list=list(DRG_F1.numpy())\n",
    "    DRG_BERT_scroe = sum(DRG_F1_list)/len(DRG_F1_list)\n",
    "    \n",
    "    P, R, DUAL_F1 = score(cands, DUAL_refs, lang='en', verbose=True)\n",
    "    DUAL_F1_list=list(DUAL_F1.numpy())\n",
    "    DUAL_BERT_scroe = sum(DUAL_F1_list)/len(DUAL_F1_list)\n",
    "    \n",
    "    BERT_score = (DRG_BERT_scroe+DUAL_BERT_scroe)/2    \n",
    "    \n",
    "    model_name.append(neg_out_files[i].split('.')[-1])\n",
    "    score_list.append(BERT_score)\n",
    "for i in range(len(model_name)):\n",
    "    print(\"model: \", model_name[i])\n",
    "    print('BERT score(F1): {}'.format(score_list[i]*100))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### PPL\n",
    "# import torch, os\n",
    "# from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "# import numpy as np\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device='cpu'\n",
    "# model_class, tokenizer_class = (GPT2LMHeadModel, GPT2Tokenizer)\n",
    "# gpt2_lm_tokenizer = tokenizer_class.from_pretrained('gpt2-large')\n",
    "# gpt2_lm_model = model_class.from_pretrained('gpt2-large')\n",
    "# gpt2_lm_model.to(device)\n",
    "# gpt2_lm_model.eval()\n",
    "\n",
    "# lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "# # import logging\n",
    "# # logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# def calculate_ppl_gpt(sentence_batch):\n",
    "#     # tokenize the sentences\n",
    "#     tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    \n",
    "#     for i in range(len(sentence_batch)):\n",
    "#         tokenized_ids[i] = gpt2_lm_tokenizer.encode(sentence_batch[i])\n",
    "        \n",
    "#     sen_lengths = [len(x) for x in tokenized_ids]\n",
    "#     max_sen_lenght = max(sen_lengths)\n",
    "    \n",
    "#     n_batch = len(sentence_batch)\n",
    "#     input_ids = np.zeros( shape=(n_batch, max_sen_lenght), dtype=np.int64)\n",
    "#     lm_labels = np.full(shape=(n_batch, max_sen_lenght), fill_value=-1)\n",
    "    \n",
    "#     for i, tokens in enumerate(tokenized_ids):\n",
    "#         input_ids[i, :len(tokens)] = tokens\n",
    "#         lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "#     input_ids = torch.tensor(input_ids).to(device)\n",
    "#     lm_labels = torch.tensor(lm_labels).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         lm_pred = gpt2_lm_model(input_ids)\n",
    "#     loss_val = lm_loss(lm_pred[0].view(-1, lm_pred[0].size(-1)), lm_labels.view(-1))\n",
    "#     normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1) / torch.tensor(sen_lengths, dtype=torch.float32).to(device)\n",
    "#     #normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1)\n",
    "#     ppl = torch.exp(normalized_loss)\n",
    "#     return  ppl.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "# pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "# for i in range(len(neg_out_files)):\n",
    "#     print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "#     neg_data_open = open(neg_out_files[i], \"r\")\n",
    "#     neg_data_dataset = neg_data_open.readlines()\n",
    "#     neg_len = len(neg_data_dataset)\n",
    "#     neg_data_open.close()\n",
    "    \n",
    "#     neg_PPL = 0\n",
    "#     for k in range(neg_len):\n",
    "#         out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "#         if len(out_sen) != 0:\n",
    "#             sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "#             neg_PPL += sample_PPL\n",
    "    \n",
    "#     pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "#     pos_data_open = open(pos_out_name, \"r\")\n",
    "#     pos_data_dataset = pos_data_open.readlines()\n",
    "#     pos_len = len(neg_data_dataset)\n",
    "#     pos_data_open.close()\n",
    "    \n",
    "#     pos_PPL = 0\n",
    "#     for k in range(pos_len):\n",
    "#         out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "#         if len(out_sen) != 0:\n",
    "#             sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "#             pos_PPL += sample_PPL\n",
    "\n",
    "#     total_PPL = (neg_PPL+pos_PPL)/(neg_len+pos_len)\n",
    "#     print('PPL: {}'.format(total_PPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/private/E2E/dataset/f_test.txt /data/private/E2E/compared_system/tgen.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 625919.02 tokens per second.\n",
      "PTBTokenizer tokenized 17387 tokens at 124107.13 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.448\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.685\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.234\n",
      "Creating temp directory  /tmp/e2e-eval-ez2s7jv0\n",
      "Running MTEval to compute BLEU & NIST...\n",
      "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl line 993.\n",
      "MT evaluation scorer began on 2020 Aug 25 at 15:50:11\n",
      "command line:  /data/private/E2E/e2e-metrics/mteval/mteval-v13a-sig.pl -r /tmp/e2e-eval-ez2s7jv0/mteval_ref.sgm -s /tmp/e2e-eval-ez2s7jv0/mteval_src.sgm -t /tmp/e2e-eval-ez2s7jv0/mteval_sys.sgm -f /tmp/e2e-eval-ez2s7jv0/mteval_log.txt\n",
      "  Evaluation of any-to-en translation using:\n",
      "    src set \"e2e\" (1 docs, 630 segs)\n",
      "    ref set \"e2e\" (45 refs)\n",
      "    tst set \"e2e\" (1 systems)\n",
      "\n",
      "NIST score = 8.6094  BLEU score = 0.6593 for system \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "\n",
      "Individual N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5919   1.4352   0.7536   0.4745   0.3540   0.1723   0.1190   0.0724   0.0461  \"tst\"\n",
      "\n",
      " BLEU:  0.9292   0.7602   0.5914   0.4522   0.3305   0.2483   0.1897   0.1457   0.1109  \"tst\"\n",
      "\n",
      "# ------------------------------------------------------------------------\n",
      "Cumulative N-gram scoring\n",
      "        1-gram   2-gram   3-gram   4-gram   5-gram   6-gram   7-gram   8-gram   9-gram\n",
      "        ------   ------   ------   ------   ------   ------   ------   ------   ------\n",
      " NIST:  5.5919   7.0272   7.7808   8.2553   8.6094   8.7817   8.9007   8.9731   9.0192  \"tst\"\n",
      "\n",
      " BLEU:  0.9292   0.8405   0.7475   0.6593   0.5742   0.4994   0.4349   0.3793   0.3309  \"tst\"\n",
      "MT evaluation scorer ended on 2020 Aug 25 at 15:50:27\n",
      "\n",
      "Removing temp directory\n",
      "SCORES:\n",
      "==============\n",
      "BLEU: 0.6593\n",
      "NIST: 8.6094\n",
      "METEOR: 0.4483\n",
      "ROUGE_L: 0.6850\n",
      "CIDEr: 2.2338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/compared_system/slug.txt\n",
    "# !./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/compared_system/slug_alt.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/E2E/dataset/f_test.txt /data/private/E2E/compared_system/tgen.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/private/01_E2E/dataset/f_test.txt /data/private/01_E2E/predictions/no_pretrained/try_3/f_pred_1.txt None\n",
      "Running MS-COCO evaluator...\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 129948 tokens at 547023.85 tokens per second.\n",
      "PTBTokenizer tokenized 13259 tokens at 124620.31 tokens per second.\n",
      "setting up scorers...\n",
      "computing METEOR score...\n",
      "METEOR: 0.317\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.572\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.092\n",
      "Creating temp directory  /tmp/e2e-eval-1om7q5cm\n",
      "Running MTEval to compute BLEU & NIST...\n"
     ]
    }
   ],
   "source": [
    "!./e2e-metrics/measure_scores.py /data/private/01_E2E/dataset/f_test.txt /data/private/01_E2E/predictions/no_pretrained/try_3/f_pred_1.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/01_E2E/dataset/f_test.txt /data/private/01_E2E/predictions/no_pretrained/try_3/f_pred_2.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/01_E2E/dataset/f_test.txt /data/private/01_E2E/predictions/no_pretrained/try_3/f_pred_3.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/01_E2E/dataset/f_test.txt /data/private/01_E2E/predictions/no_pretrained/try_3/f_pred_4.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/01_E2E/dataset/f_test.txt /data/private/01_E2E/predictions/no_pretrained/try_3/f_pred_5.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/01_E2E/dataset/f_test.txt /data/private/01_E2E/predictions/no_pretrained/try_3/f_pred_6.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/01_E2E/dataset/f_test.txt /data/private/01_E2E/predictions/no_pretrained/try_3/f_pred_7.txt\n",
    "!./e2e-metrics/measure_scores.py /data/private/01_E2E/dataset/f_test.txt /data/private/01_E2E/predictions/no_pretrained/try_3/f_pred_8.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
